@ARTICLE{10345557,
  author={Aoujil, Zakaria and Hanine, Mohamed and Flores, Emmanuel Soriano and Samad, Md. Abdus and Ashraf, Imran},
  journal={IEEE Access},
  title={Artificial Intelligence and Behavioral Economics: A Bibliographic Analysis of Research Field},
  year={2023},
  volume={11},
  number={},
  pages={139367-139394},
  abstract={Behavioral economics and artificial intelligence (AI) have been two rapidly growing fields of research over the past few years. While behavioral economics aims to combine concepts from psychology, sociology, and neuroscience with classical economic thoughts to understand human decision-making processes in the complex economic environment, AI on the other hand, focuses on creating intelligent machines that can mimic human cognitive abilities such as learning, problem-solving, decision-making, and language understanding. The intersection of these two fields has led to thrilling research theories and practical applications. This study provides a bibliometric analysis of the literature on AI and behavioral economics to gain insight into research trends in this field. We conducted this bibliometric analysis using the Web of Science database on articles published between 2012 and 2022 that were related to AI and behavioral economics. VOSviewer and Bibliometrix R package were utilized to identify influential authors, journals, institutions, and countries in the field. Network analysis was also performed to identify the main research themes and their interrelationships. The analysis revealed that the number of publications on AI and behavioral economics has been increasing steadily over the past decade. We found that most studies focused on customer and consumer behavior, including topics such as decision-making under uncertainty, neuroeconomics, and behavioral game theory, combined mainly with machine learning and deep learning techniques. We also identified several emerging themes, including the use of AI in nudging and prospect theory in behavioral finance, as well as undeveloped themes such as AI-driven behavioral macroeconomics. The findings suggests that there is a need for more interdisciplinary collaboration between researchers in behavioral economics and AI. We also suggest that future research on AI and behavioral economics further consider the ethical implications of using AI and behavioral insights in decision-making. This study can serve as a valuable resource for researchers interested in AI and behavioral economics.},
  keywords={Economics;Behavioral sciences;Artificial intelligence;Psychology;Codes;Biological system modeling;Uncertainty;Finance;Investment;Neural engineering;Machine learning;Bibliometrics;Artificial intelligence;behavioral economics;behavioral finance;consumer behavior;investor behavior;decision making;neuroeconomics;machine learning;bibliometric analysis},
  doi={10.1109/ACCESS.2023.3339778},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8783783,
  author={Osorio-Zuluaga, Germán A. and Duque-Mendez, Nestor Dario},
  booktitle={2018 XIII Latin American Conference on Learning Technologies (LACLO)},
  title={Search and Selection of Learning Objects in Repositories: A Review},
  year={2018},
  volume={},
  number={},
  pages={513-520},
  abstract={This article aims to present a review of the academic literature on the search for learning objects in repositories of the last ten years. To carry it out, first, it defined a methodology for the search and selection of the papers to be included in this study. Then, the bibliographic databases Web of Science, Scopus, and Google Scholar for the implementation of the methodology were used. Next, based on the selected documents, three strategies for the search of learning objects were characterized, such as classic by keywords, full-text search, and hybrids. In this sense, hybrid solutions that use multiple integrated strategies have been gaining importance. Finally, it was verified that the methodology used to determine the papers of the review was adequate and allowed an approach to the state of the art in this topic.},
  keywords={Search problems;Metadata;Information services;Search engines;Google;Tools;Information Retrieval;learning object search;full text search;hybrid search;keywords search;eLearning},
  doi={10.1109/LACLO.2018.00090},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8964236,
  author={Ming, Jing and Zhang, Li},
  booktitle={2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
  title={Colorwall: An Embedded Temporal Display of Bibliographic Data},
  year={2019},
  volume={},
  number={},
  pages={482-491},
  abstract={A bibliographical data set is often visualized as a network to depict relationships among authors. However, static networks only display minimal information when a dataset accommodates temporal features. This paper proposes an embedded network visualization to present concealed temporal patterns in a data set and leverage multiple intelligent filters to reduce occlusion. We compare different graphing styles, such as feature representation and time direction, then determine the best approach for displaying temporal features. We demonstrate the usability of our approach with case studies and an evaluation of the IEEE InfoVis and VAST conference dataset.},
  keywords={Bibliographic data;Dynamic network;information visualization},
  doi={10.1109/DSAA.2019.00063},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{6642469,
  author={Kaczynski, David and Hu, Gongzhu},
  booktitle={2013 IEEE 14th International Conference on Information Reuse & Integration (IRI)},
  title={Geographic clustering of research universities in specialized fields using Microsoft Academic Search},
  year={2013},
  volume={},
  number={},
  pages={170-174},
  abstract={Search is no doubt the most common activities on the Web. One of the most frequent search activities people in the educational and academic communities do is to search for research topics and papers and their author/affiliation information. For example, undergraduate students searching for graduate schools and master-level students searching for institutions for doctoral studies may go online to find universities that are conducting research in a highly specific field of their interest. A common way of doing this is to search journals and conferences for pertinent articles, and then accumulating the affiliations of the authors. In this paper, we present an interactive web-based academic search interface for discovering geographic clusters of academic institutions given a specific field by the user. The underlying system interacts with Microsoft Academic Search API and Google Places API, as well as Weka for clustering analysis. The system can be easily extended to accommodate other types of queries.},
  keywords={Educational institutions;Google;Organizations;Data mining;Search engines;Internet;Encyclopedias;Microsoft Academic Search;Google Places;system integration;clustering;Weka;Scala},
  doi={10.1109/IRI.2013.6642469},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{5616719,
  author={Muhlenbach, Fabrice and Lallich, Stephane},
  booktitle={2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
  title={Discovering Research Communities by Clustering Bibliographical Data},
  year={2010},
  volume={1},
  number={},
  pages={500-507},
  abstract={Today's world is characterized by the multiplicity of interconnections through many types of links between the people, that is why mining social networks appears to be an important topic. Extracting information from social networks becomes a challenging problem, particularly in the case of the discovery of community structures. Mining bibliographical data can be useful to find communities of researchers. In this paper we propose a formal definition to consider the similarity and dissimilarity between individuals of a social network and how a graph-based clustering method can extract research communities from the DBLP database.},
  keywords={bibliographical data;graph-based clustering;community mining},
  doi={10.1109/WI-IAT.2010.117},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9994867,
  author={Huang, Pei-Chi and Shakya, Ejan and Song, Myoungkyu and Subramaniam, Mahadevan},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  title={BioMDSE: A Multimodal Deep Learning-Based Search Engine Framework for Biofilm Documents Classifications},
  year={2022},
  volume={},
  number={},
  pages={3608-3612},
  abstract={As biofilms research grows rapidly, a corpus of bibliographic literature (i.e., documents) is increasing at an incredible rate. Many researchers often need to inspect these large document collections, including (1) text, (2) images, and (3) captions, to understand underlying biological mechanisms and make a critical decision. However, researchers have great difficulty in exploring such ever-growing large datasets in labor-intensive processes. Thus, automation of such tasks is urgently required for the automatic identification or classification of a large volume of document collections. To address this problem, we present a multimodal deep learning-based approach to automatically classify documents for a specialized information retrieval technique based on biofilm images, captions, and texts, which is a major source of information for the classification of documents. Images, captions, and texts from biofilm documents are represented in a large vector space. Then, they are fed into convolutional neural networks (CNNs), to improve similarity matching and relevance. Our extensive experiments and analysis will take captions, texts, or images as unimodal models as inputs and concatenate them all into multimodal models. The trained models for this classification approach in turn help a search engine to precisely identify relevant and domain-specific documents from a large volume of document collections for further research direction in biofilm development.},
  keywords={Analytical models;Solid modeling;Automation;Biological system modeling;Search engines;Convolutional neural networks;Task analysis;Deep Learning;Computer Vision;Document Classification;Multimodal Learning;Data Fusion},
  doi={10.1109/BIBM55620.2022.9994867},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8406672,
  author={ElAbdi, Mariem and Smine, Boutheina and Yahia, Sadok Ben},
  booktitle={2018 12th International Conference on Research Challenges in Information Science (RCIS)},
  title={DFBICA: A new distributed approach for sentiment analysis of bibliographic citations},
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Sentiment analysis of citations in scientific papers is a new and interesting research area. In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity of citations in scientific papers. In this work, we conducted empirical research to investigate the classification of positive and negative citations. It is based on word vectors as a feature space, to which the examined citation context was mapped to. In order to handle with the huge amount of data, we have implemented our proposed approach in a distributed manner according to MapReduce paradigm through the Hadoop framework.},
  keywords={Sentiment analysis;Task analysis;Data mining;Feature extraction;Neural networks;Machine learning;Reliability;Sentiment analysis;Scientific paper;Word2vec;MapReduce},
  doi={10.1109/RCIS.2018.8406672},
  ISSN={2151-1357},
  month={May},}@INPROCEEDINGS{9401964,
  author={Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  title={Asset Management in Machine Learning: A Survey},
  year={2021},
  volume={},
  number={},
  pages={51-60},
  abstract={Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
  keywords={Machine learning;Tools;Software systems;Control systems;Asset management;Monitoring;Software engineering;machine learning;se4ml;asset management;experiment management},
  doi={10.1109/ICSE-SEIP52600.2021.00014},
  ISSN={},
  month={May},}@INPROCEEDINGS{9073669,
  author={Jafery, Wan Ain Zubaidah Wan Chek and Omar, Mohd Shahir Shamsir and Ahmad, Noor Azurati and Ithnin, Hafizah},
  booktitle={2019 6th International Conference on Research and Innovation in Information Systems (ICRIIS)},
  title={Classification of Patents according to Industry 4.0 Pillars using Machine Learning Algorithms},
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Industry 4.0 is on the horizon. Therefore, it is crucial to analyze the patterns and trends of intellectual property (IP) information to determine the readiness of stakeholders to adapt to the changing industrial evolution. Patent bibliography documents consist of structured and unstructured data, so text mining or machine learning must be employed for the data analysis. This paper established a patent trend by analyzing the patent data of Intellectual Property Corporations of Malaysia (MyIPO) to identify the institution's readiness to face the fourth industrial revolution. To achieve this aim, a patent classification method was used to classify MyIPO patent data based on the pillars of Industry 4.0. Furthermore, the patents data were drawn from MyIPO Online Search and Filing System was used as the datasets in this study. However, the dataset consists of the title of the patent and the publication year only. Since short text data in the title has fewer semantic information and high sparseness, this issue was a challenge for this study. In this paper, five common classifiers were used for text classification. Support Vector Machine (SVM) was proven to be the machine learning classifier with the highest accuracy in classifying the training and testing datasets. The findings of this paper present the patent trend for each pillar of Industry 4.0 including the patents related to Industry 4.0 where Autonomous Robot is the pillar with the highest innovation.},
  keywords={Patents;Industries;Market research;Technological innovation;Intellectual property;Machine learning;Support vector machines;patent analytics;management map;patent classification;machine learning;intellectual property},
  doi={10.1109/ICRIIS48246.2019.9073669},
  ISSN={2324-8157},
  month={Dec},}@ARTICLE{9770797,
  author={Abbas, Abdallah M. H. and Ghauth, Khairil Imran and Ting, Choo-Yee},
  journal={IEEE Access},
  title={User Experience Design Using Machine Learning: A Systematic Review},
  year={2022},
  volume={10},
  number={},
  pages={51501-51514},
  abstract={User experience (UX) is the key to increased productivity by enhancing the usability and interactivity of the product. Machine learning (ML) solutions have raised user and academic awareness of technical innovation. As a result, ML is becoming increasingly popular to improve the quality of UX. Several investigations have highlighted a potential lack of studies on the overall challenges and recommendations for UX using ML. Therefore, more attention should be paid to ML’s existence and potential applications across various applications to get the most out of ML techniques to improve the UX design process. To this objective, a systematic review of the literature was performed as to determine the challenges faced by UX designers when incorporating ML in their design process. Recommendations that help UX designers incorporate ML into UX design will be highlighted. Furthermore, the PRISMA approach is used (a process that has been established in the literature), to restrict the chance of bias at the selection stage. Relevant articles in the following four databases were searched: IEEE Xplore, Scopus, Web of Science, and ACM. The findings revealed that the number of publications on issues linked to UX with ML had advanced exponentially. This review highlights the challenges, recommendations, tools, algorithms, techniques and datasets used in different studies. In addition, suggestions are given for future investigations.},
  keywords={Machine learning;Databases;User experience;Libraries;Systematics;Metadata;Graphical user interfaces;User experience;experience design;UX;ED;machine learning;ML;HCI;UX design;user interaction;user behavior},
  doi={10.1109/ACCESS.2022.3173289},
  ISSN={2169-3536},
  month={},}@ARTICLE{9661321,
  author={Sterling, Julia A. and Montemore, Matthew M.},
  journal={IEEE Access},
  title={Combining Citation Network Information and Text Similarity for Research Article Recommender Systems},
  year={2022},
  volume={10},
  number={},
  pages={16-23},
  abstract={Researchers often need to gather a comprehensive set of papers relevant to a focused topic, but this is often difficult and time-consuming using existing search methods. For example, keyword searching suffers from difficulties with synonyms and multiple meanings. While some automated research-paper recommender systems exist, these typically depend on either a researcher’s entire library or just a single paper, resulting in either a quite broad or a quite narrow search. With these issues in mind, we built a new research-paper recommender system that utilizes both citation information and textual similarity of abstracts to provide a highly focused set of relevant results. The input to this system is a set of one or more related papers, and our system searches for papers that are closely related to the entire set. This framework helps researchers gather a set of papers that are closely related to a particular topic of interest, and allows control over which cross-section of the literature is located. We show the effectiveness of this recommender system by using it to recreate the references of review papers. We also show its utility as a general similarity metric between scientific articles by performing unsupervised clustering on sets of scientific articles. We release an implementation, ExCiteSearch (bitbucket.org/mmmontemore/excitesearch), to allow researchers to apply this framework to locate relevant scientific articles.},
  keywords={Recommender systems;Filtering;Machine learning;Search engines;Motion pictures;Search methods;Science - general;Publishing;Scientific literature;recommender systems;search engines;search methods},
  doi={10.1109/ACCESS.2021.3137960},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10044125,
  author={Nicesio, Otavio Kiyatake and Leal, Adriano Galindo and Gava, Vagner Luiz},
  booktitle={2023 IEEE 2nd International Conference on AI in Cybersecurity (ICAIC)},
  title={Quantum Machine Learning for Network Intrusion Detection Systems, a Systematic Literature Review},
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Quantum computing presents potential advantages over classical computing in terms of computational complexity. Therefore, it is expected for quantum machine learning applications to have improvements in capacity and learning efficiency over classical machine learning methods. This paper aims to present a Systematic Literature Review of articles published between 2017 and 2022, identifying, analyzing, and comparing different proposals of quantum machine learning applications for network intrusion detection systems (IDS). This study focused on identifying papers that implemented quantum machine learning algorithms in the context of intrusion detection systems. The main algorithms found were variational hybrid quantum-classical, with models based on quantum support vector machines and quantum neural networks. Benefits compared to classical models were observed and described, such as reduced training time and improved classification accuracy for attacking traffic.},
  keywords={Training;Support vector machines;Machine learning algorithms;Systematics;Computational modeling;Bibliographies;Network intrusion detection;Cybersecurity;Intrusion Detection System;Quantum Machine Learning;Quantum Computing;Machine Learning},
  doi={10.1109/ICAIC57335.2023.10044125},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10707620,
  author={Napoleão, Bianca Minetto and Sarkar, Ritika and Hallé, Sylvain and Petrillo, Fabio and Kalinowski, Marcos},
  booktitle={2024 IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE)},
  title={Emerging Results on Automated Support for Searching and Selecting Evidence for Systematic Literature Review Updates},
  year={2024},
  volume={},
  number={},
  pages={34-41},
  abstract={Context: The constant growth of primary evidence and Systematic Literature Reviews (SLRs) publications in the Software Engineering (SE) field leads to the need for SLR Updates. However, searching and selecting evidence for SLR updates demands significant effort from SE researchers. Objective: We present emerging results on an automated approach to support searching and selecting studies for SLR updates in SE. Method: We developed an automated tool prototype to perform the snowballing search technique and to support the selection of relevant studies for SLR updates using Machine Learning (ML) algorithms. We evaluated our automation proposition through a small-scale evaluation with a reliable dataset from an SLR replication and its update. Results: Effectively automating snowballing-based search strategies showed feasibility with minor losses, specifically related to papers without Digital Object Identifier (DOI). The ML algorithm giving the highest performance to select studies for SLR updates was Linear Support Vector Machine with approximately 74% recall and 15% precision. The use of such algorithms with conservative thresholds to minimize the risk of missing papers can already significantly reduce evidence selection efforts. Conclusion: The preliminary results of our evaluation point in promising directions, indicating the potential of automating snowballing search efforts and of reducing the number of papers to be manually analyzed by about 2.5 times when selecting evidence for updating SLRs in SE.},
  keywords={Support vector machines;Analytical models;Automation;Machine learning algorithms;Prototypes;Search problems;Approximation algorithms;Reliability;Proposals;Software engineering;Systematic Review Update;SLR Update;Searching for evidence;Selecting evidence},
  doi={10.1145/3643664.3648202},
  ISSN={},
  month={April},}@ARTICLE{9359733,
  author={Rathore, M. Mazhar and Shah, Syed Attique and Shukla, Dhirendra and Bentafat, Elmahdi and Bakiras, Spiridon},
  journal={IEEE Access},
  title={The Role of AI, Machine Learning, and Big Data in Digital Twinning: A Systematic Literature Review, Challenges, and Opportunities},
  year={2021},
  volume={9},
  number={},
  pages={32030-32052},
  abstract={Digital twinning is one of the top ten technology trends in the last couple of years, due to its high applicability in the industrial sector. The integration of big data analytics and artificial intelligence/machine learning (AI-ML) techniques with digital twinning, further enriches its significance and research potential with new opportunities and unique challenges. To date, a number of scientific models have been designed and implemented related to this evolving topic. However, there is no systematic review of digital twinning, particularly focusing on the role of AI-ML and big data, to guide the academia and industry towards future developments. Therefore, this article emphasizes the role of big data and AI-ML in the creation of digital twins (DTs) or DT-based systems for various industrial applications, by highlighting the current state-of-the-art deployments. We performed a systematic review on top of multidisciplinary electronic bibliographic databases, in addition to existing patents in the field. Also, we identified development-tools that can facilitate various levels of the digital twinning. Further, we designed a big data driven and AI-enriched reference architecture that leads developers to a complete DT-enabled system. Finally, we highlighted the research potential of AI-ML for digital twinning by unveiling challenges and current opportunities.},
  keywords={Big Data;Digital twin;Patents;Industries;Systematics;Tools;Libraries;Digital twin;artificial intelligence;machine learning;big data;industry 40},
  doi={10.1109/ACCESS.2021.3060863},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{6831001,
  author={Tkaczyk, Dominika and Szostek, Pawel and Dendek, Piotr Jan and Fedoryszak, Mateusz and Bolikowski, Lukasz},
  booktitle={2014 11th IAPR International Workshop on Document Analysis Systems},
  title={CERMINE -- Automatic Extraction of Metadata and References from Scientific Literature},
  year={2014},
  volume={},
  number={},
  pages={217-221},
  abstract={CERMINE is a comprehensive open source system for extracting metadata and parsed bibliographic references from scientific articles in born-digital form. The system is based on a modular workflow, whose architecture allows for single step training and evaluation, enables effortless modifications and replacements of individual components and simplifies further architecture expanding. The implementations of most steps are based on supervised and unsupervised machine-learning techniques, which simplifies the process of adjusting the system to new document layouts. The paper describes the overall workflow architecture, provides details about individual implementations and reports evaluation methodology and results. CERMINE service is available at http://cermine.ceon.pl.},
  keywords={Libraries;Portable document format;Data mining;Feature extraction;Computer architecture;Abstracts;Support vector machines;document analysis;metadata extraction;bibliographic references extraction;PDF processing;zone classification},
  doi={10.1109/DAS.2014.63},
  ISSN={},
  month={April},}@ARTICLE{9206554,
  author={Khan, Md. Al-Masrur and Khan, Md Rashed Jaowad and Tooshil, Abul and Sikder, Niloy and Mahmud, M. A. Parvez and Kouzani, Abbas Z. and Nahid, Abdullah-Al},
  journal={IEEE Access},
  title={A Systematic Review on Reinforcement Learning-Based Robotics Within the Last Decade},
  year={2020},
  volume={8},
  number={},
  pages={176598-176623},
  abstract={Robotics is one of the many tools that is making a substantial difference as the world is experiencing the fourth industrial revolution. To ease control over this engineering marvel substantially, Reinforcement Learning (RL) has paved its way in recent years quite remarkably. RL enables robots to become self-aware towards carrying out a specific task followed by user operations. For decades of rigorous endeavor, this research field has gone through numerous groundbreaking developments and it will be the same for the coming days. Therefore, this paper steps in to enlighten the scientific community with a systemic review of the published research papers within the past decade. The bibliographic data that is extracted from the papers are analyzed using an automated tool named Vosviewer with respect to some parameters. Substantial excerpts from the most influential papers are highlighted in this work. Furthermore, this paper points out the global research practice in this field. The paper also generates some intriguing questions and answers them in regards to the research topic. After reading this paper, future researchers will have a firm idea in the RL-based robotics and will be able to incorporate in their own research.},
  keywords={Robot kinematics;Systematics;Reinforcement learning;Task analysis;Automation;Service robots;Bibliometric analysis;reinforcement learning;robotics;systematic review},
  doi={10.1109/ACCESS.2020.3027152},
  ISSN={2169-3536},
  month={},}@ARTICLE{8580549,
  author={Kong, Xiangjie and Ma, Kai and Hou, Shen and Shang, Di and Xia, Feng},
  journal={IEEE Access},
  title={Human Interactive Behavior: A Bibliographic Review},
  year={2019},
  volume={7},
  number={},
  pages={4611-4628},
  abstract={As a common human behavior, interaction is everywhere in human life. With the rise of big data and human–computer interaction in the 21st century, more and more researchers from different industries and disciplines pay great attention to the human interactive behavior research. From the perspective of computer science, scholars try to use computer technology to make research more meaningful. To the best of our knowledge, this paper is the first study to investigate the potential rules of human interactive behavior in the view of computer science, based on 16 top-tier journals of human interactive behavior from Microsoft Academic Graph dataset. We put forward a topic extraction and clustering model based on word2vec to infer key topics, which can be widely used in different fields of research. We find that the growth of human interactive behavior is in an uptrend on the whole. Besides, the cooperative relationship between authors and countries/regions is closer over time. We also make the mensurable evolution analysis of topics by a statistical method. Some topics are hot all the time, while some are unpopular as time goes by. Finally, we do rankings in the field of human behavior research from a new perspective. All these findings help researchers observe potential patterns and the topic evolution in half a century, which may shed dazzling light on the exploration of human interactive behavior.},
  keywords={Computer science;Statistical analysis;Collaboration;Market research;Computational modeling;Tools;Transportation;Human interactive behavior;computer science;topic modeling;bibliographic review and topic trend},
  doi={10.1109/ACCESS.2018.2887341},
  ISSN={2169-3536},
  month={},}@ARTICLE{10551808,
  author={Garcia-Buendia, Noelia and Muñoz-Montoro, Antonio J. and Cortina, Raquel and Maqueira-Marín, Juan M. and Moyano-Fuentes, José},
  journal={IEEE Access},
  title={Mapping the Landscape of Quantum Computing and High Performance Computing Research Over the Last Decade},
  year={2024},
  volume={12},
  number={},
  pages={106107-106120},
  abstract={Quantum Computing (QC) is a rapidly evolving research field that has garnered significant attention due to its potential to revolutionize various domains such as cryptography, optimization, and machine learning. In this article, we conduct an extensive analysis of the evolution of QC research within the realm of High Performance Computing (HPC) over a span of ten years, up to 2023. Through bibliometric analysis and advanced science mapping techniques, we uncover key thematic areas that have emerged in the field, including quantum algorithms, simulation, parallel-computing, deep learning, machine learning, and encryption. This analysis highlights the interdisciplinary nature of QC, which intersects with disciplines such as physics, mathematics, computer science, and materials science. Furthermore, our study elucidates the close relationship between HPC and QC, showcasing how advancements in one field can significantly impact the other. The findings of this study not only provide valuable insights into the past trends and research landscape but also serve as a guide for future research directions, enabling the advancement of knowledge and fostering innovation in computer science. Additionally, our analysis sheds light on the global distribution of research contributions, identifying countries and regions that have made significant strides in QC research, thus presenting potential collaboration opportunities. Overall, this comprehensive study contributes to a deeper understanding of the development of QC within the realm of HPC, offering valuable insights and paving the way for future advancements in this exciting field.},
  keywords={Quantum computing;High performance computing;Bibliometrics;Performance analysis;Collaboration;Big Data;Task analysis;Machine learning;Quantum computing;high performance computing;bibliometrics;science mapping;co-occurrence analysis;machine learning},
  doi={10.1109/ACCESS.2024.3411307},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9103378,
  author={Joseph, Charles and Lekamge, Sugeeswari},
  booktitle={2019 International Conference on Advancements in Computing (ICAC)},
  title={Machine Learning Approaches for Emotion Classification of Music: A Systematic Literature Review},
  year={2019},
  volume={},
  number={},
  pages={334-339},
  abstract={Music is ubiquitous in every corner of the world. Fundamental mechanisms for how a listener perceives emotions in music as well as the way in which the emotional goal of a composer is expressed through music have been widely explored. Not only humans but also machines can recognize emotions in music once they are trained. As a result, various machine learning approaches are tested by previous researchers for their capability in music emotion recognition. To identify the current status of research and the research gaps in the above domain, we conducted a systematic literature review, through which we investigated several key aspects including types of music, acoustic features, feature extraction mechanisms, classification algorithms, and their performance. Six electronic databases were searched for studies published until 2018. Initial set of studies comprised of 198 studies from which 14 were selected for detailed analysis. Findings revealed that a considerable number of studies have used western music whereas other cultural-specific music still remains to be explored. Acoustic features pertaining to pitch, intensity, timbre, tempo, rhythm, melody, and harmony have been commonly used. While Support Vector Machine, Naive Bayes, and k-Nearest Neighbor are among the frequently used standard classifiers, Fuzzy classifiers and ensemble learning also have been attempted. The discrepancies among the classifier performances reported in previous studies could be partially attributed to the differences in the key aspects we have considered. The study makes a significant contribution by providing a comprehensive and up-to-date review of the previous attempts made in the selected domain.},
  keywords={Music;Databases;Machine learning;Bibliographies;Systematics;Emotion recognition;Feature extraction;music emotion;music emotion classification;machine learning;systematic literature review},
  doi={10.1109/ICAC49085.2019.9103378},
  ISSN={},
  month={Dec},}@ARTICLE{10534234,
  author={},
  journal={IEEE P3187/D0.8, May 2024},
  title={IEEE Approved Draft Guide for Framework for Trustworthy Federated Machine Learning},
  year={2024},
  volume={},
  number={},
  pages={1-45},
  abstract={The development and application of federated machine learning are facing the critical challenges about how to balance the tradeoff among privacy, security, performance, and efficiency, how to realize supervision covering the whole life cycle and how to get the explainable results. Then trustworthy federated machine learning is proposed to solve the above problem. In this standard, a general view on framework for trustworthy federated machine learning is provided in four parts: a principle in trustworthy federated machine learning, requirements from the perspective of different principles and different federated machine learning participants, and methods to realize trustworthy federated machine learning. It also provides some guidance on how trustworthy federated machine learning is used in various scenarios.},
  keywords={IEEE Standards;Federated learning;Machine learning;Homomorphic encryption;Cryptography;Privacy;Computer applications;Trusted computing;federated machine learning;framework;IEEE 3187™;machine learning;principle;trustworthy federated machine learning},
  doi={},
  ISSN={},
  month={Oct},}@ARTICLE{10476376,
  author={},
  journal={IEEE P3187/D0.7, December 2023},
  title={IEEE Draft Guide for Framework for Trustworthy Federated Machine Learning},
  year={2024},
  volume={},
  number={},
  pages={1-45},
  abstract={The development and application of federated machine learning are facing the critical challenges about how to balance the tradeoff among privacy, security, performance, and efficiency, how to realize supervision covering the whole life cycle and how to get the explainable results. Then trustworthy federated machine learning is proposed to solve the above problem. In this standard, a general view on framework for trustworthy federated machine learning is provided in four parts: a principle in trustworthy federated machine learning, requirements from the perspective of different principles and different federated machine learning participants, and methods to realize trustworthy federated machine learning. It also provides some guidance on how trustworthy federated machine learning is used in various scenarios.},
  keywords={IEEE Standards;Federated learning;Machine learning;Trusted computing;Control systems;federated machine learning;framework;IEEE 3187™;machine learning;principle;trustworthy federated machine learning},
  doi={},
  ISSN={},
  month={March},}@ARTICLE{10068497,
  author={Gorment, Nor Zakiah and Selamat, Ali and Cheng, Lim Kok and Krejcar, Ondrej},
  journal={IEEE Access},
  title={Machine Learning Algorithm for Malware Detection: Taxonomy, Current Challenges, and Future Directions},
  year={2023},
  volume={11},
  number={},
  pages={141045-141089},
  abstract={Malware has emerged as a cyber security threat that continuously changes to target computer systems, smart devices, and extensive networks with the development of information technologies. As a result, malware detection has always been a major worry and a difficult issue, owing to shortcomings in performance accuracy, analysis type, and malware detection approaches that fail to identify unexpected malware attacks. This paper seeks to conduct a thorough systematic literature review (SLR) and offer a taxonomy of machine learning methods for malware detection that considers these problems by analyzing 77 chosen research works related to malware detection using machine learning algorithm. The research investigates malware and machine learning in the context of cybersecurity, including malware detection taxonomy and machine learning algorithm classification into numerous categories. Furthermore, the taxonomy was used to evaluate the most recent machine learning algorithm and analysis. The paper also examines the obstacles and associated concerns encountered in malware detection and potential remedies. Finally, to address the related issues that would motivate researchers in their future work, an empirical study was utilized to assess the performance of several machine learning algorithms.},
  keywords={Malware;Machine learning;Support vector machines;Taxonomy;Data mining;Classification tree analysis;Malware detection;machine learning algorithms;state-of-the-art},
  doi={10.1109/ACCESS.2023.3256979},
  ISSN={2169-3536},
  month={},}@ARTICLE{9102282,
  author={Ahmed, Muhammad Waqas and Afzal, Muhammad Tanvir},
  journal={IEEE Access},
  title={FLAG-PDFe: Features Oriented Metadata Extraction Framework for Scientific Publications},
  year={2020},
  volume={8},
  number={},
  pages={99458-99469},
  abstract={The unprecedented growth of the research publications in diversified domains has overwhelmed the research community. It requires a cumbersome process to extract this enormous information by manually analyzing these research documents. To automatically extract content of a document in a structured way, metadata and content must be annotated. Scientific community has been focusing on automatic extraction of content by forming different heuristics and applying different machine learning techniques. One of the renowned conference organizers, ESWC organizes state-of-the-art challenge to extract metadata like authors, affiliations, countries in affiliations, supplementary material, sections, table, figures, funding agencies, and EU funded projects from PDF files of research articles. We have proposed a feature centric technique that can be used to extract logical layout structure of articles from publishers with diversified composition styles. To extract unique metadata from a research article placed in logical layout structure, we have developed a four-staged novel approach “FLAG-PDFe”. The approach is built upon distinct and generic features based on the textual and the geometric information from the raw content of research documents. At the first stage, the distinct features are used to identify different physical layout components of an individual article. Since research journals follow their unique publishing styles and layout formats, therefore, we develop generic features to handle these diversified publishing patterns. We employ support vector classification (SVC) in the third stage to extract the logical layout structure (LLS)/ sections of an article, after performing comprehensive evaluation of generic features and machine learning models. Finally, we further apply heuristics on LLS to extract the desired metadata of an article. The outcomes of the study are obtained using the gold standard data set. The results yields 0.877 recall, precision 0.928 and 0.897 F-measure. Our approach has achieved a 16% gain on f-measure when compared to the best approach of the ESWC challenge.},
  keywords={Feature extraction;Metadata;Layout;Portable document format;Data mining;Machine learning;Support vector machines;Machine learning;research article;metadata extraction;text patterns;document structure analysis},
  doi={10.1109/ACCESS.2020.2997907},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{7289016,
  author={Knyazeva, Anna and Kolobov, Oleg and Tatarsky, Fjodor and Turchanovsky, Igor},
  booktitle={2015 5th International Conference on Information Science and Technology (ICIST)},
  title={A merging approach for authority records},
  year={2015},
  volume={},
  number={},
  pages={461-465},
  abstract={The process of merging two or more authority databases is considered in this paper. We study the problem of duplicate detection and merging authority records, created in accordance with Russian Cataloguing Rules and format RUSMARC/RUSMARC Authorities. The toolbox Cflib has been developed by us for solving this problem. It's based on standard principles of record linkage and has quite simple architecture.},
  keywords={Databases;record linkage;duplicate detection;authority records;bibliographic records},
  doi={10.1109/ICIST.2015.7289016},
  ISSN={2164-4357},
  month={April},}@ARTICLE{10156853,
  author={Pham, Hoang-Son and Poelmans, Hanne and Ali-Eldin, Amr},
  journal={IEEE Access},
  title={A Metadata-Based Approach for Research Discipline Prediction Using Machine Learning Techniques and Distance Metrics},
  year={2023},
  volume={11},
  number={},
  pages={61995-62012},
  abstract={Forecasting research disciplines associated with research projects is a significant challenge in research information systems. It can reduce the administrative effort involved in entering research project-related metadata, eliminate human errors, and enhance the quality of research project metadata. It also enables the calculation of the degree of interdisciplinarity of these projects. However, predicting scientific research disciplines and measuring interdisciplinarity in a research endeavor remain difficult. In this paper, we propose a framework for predicting the research disciplines associated with a research project and measuring the degree of interdisciplinarity based on associated metadata to address these issues. The proposed framework consists of several components to improve the performance of research disciplines prediction and interdisciplinarity measurement systems. These include a feature extraction component that utilizes a topic model to extract the most appropriate features. Further, the framework proposes a discipline encoding component that applies a data mapping strategy to lower the dimensionality of the output variables. Furthermore, a distance matrix creation component is proposed to recommend the most appropriate research disciplines and compute interdisciplinarity associated with research projects. We implemented the suggested framework on two separate research information systems databases for research projects, Dimensions and the Flemish Research Information Space. Experimental results demonstrate that the proposed framework predicts the research disciplines associated with research projects more accurately than related work.},
  keywords={Deep learning;Databases;Feature extraction;Metadata;Information systems;Predictive models;Task analysis;Metadata;research information systems (RIS);research disciplines prediction;interdisciplinarity;machine learning;distance metrics},
  doi={10.1109/ACCESS.2023.3287935},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{5474861,
  author={Shin, Dongwook and Kim, Taehwan and Jung, Hana and Choi, Joongmin},
  booktitle={2010 24th IEEE International Conference on Advanced Information Networking and Applications},
  title={Automatic Method for Author Name Disambiguation Using Social Networks},
  year={2010},
  volume={},
  number={},
  pages={1263-1270},
  abstract={A name is a key feature for distinguishing people, but we often fail to distinguish people because an author may have multiple names or multiple authors may share the same name. Such name ambiguity problems affect the performance of the document retrieval, web search and database integration. Especially, in bibliographic information, a number of errors may be included since there are different authors with the same name or an author name may be misspelled or represented with an abbreviation. For solving these problems, it is necessary to disambiguate the names inputted into the database. In this paper, we propose a method to solve the name ambiguity by using social networks constructed based on the relations among authors. We evaluated the effectiveness of the proposed system based on the DBLP data that offer computer science bibliographic information.},
  keywords={Social network services;Uncertainty;Computer science;Databases;Information retrieval;Search engines;Software libraries;Graph theory;Artificial intelligence;Support vector machines;networks;name disambiguation;identity uncertainty;DBLP;cycle detection},
  doi={10.1109/AINA.2010.66},
  ISSN={2332-5658},
  month={April},}@INPROCEEDINGS{10456101,
  author={Jangde, Priyanka and Sethi, Kamal Kumar and Singh, Narendra Pal and Bhanodia, Praveen and Mishra, Durgesh},
  booktitle={2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG)},
  title={A Comprehensive Machine Learning Based Ensemble Model for Prediction of Possible Diseases and Recommending Medicines},
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Many instances remain untreated globally due to the lack and unavailability of medical services. In this regard, efficient monitoring and analysis of medical records will be helpful in timely identification and prediction of diseases. To enable a quicker and more precise diagnosis, a platform that keeps track of a patient's medical history and makes illness predictions based on the patient's present symptoms is needed. Early disease prediction allows users to determine the severity of a condition and take appropriate action timely. Thus, anticipation of illness in its early stage becomes essential but it's difficult for doctors to anticipate outcomes precisely and accurately based only on symptoms. Availability of medical data and its analysis with the help of machine learning techniques can help the healthcare system for appropriate predictions and recommendations of diseases. In this paper, based on the patient's symptoms and medical records, a general illness prediction system is proposed. Using the input image for disease prediction, we applied an ensemble model exploiting the KNN, SVM, Naïve bayes and deep learning techniques to accurately predict diseases. The proposed model superseded the existing techniques with 98% accuracy on experimental study performed using benchmark datasets.},
  keywords={Deep learning;Machine learning algorithms;Medical services;Predictive models;Benchmark testing;Prediction algorithms;Ensemble learning;Recommender System;Machine Learning;SVM;KNN;Naïve Bayes;Transfer Learning},
  doi={10.1109/ICTBIG59752.2023.10456101},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{4119140,
  author={Sivasubramaniam, Anand and Debnath, Sandip and Li, Huajing and Lee, Wang Chien and Bolelli, Levent and Giles, C. Lee and Zhuang, Ziming and Councill, Isaac G.},
  booktitle={Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)},
  title={Learning metadata from the evidence in an on-line citation matching scheme},
  year={2006},
  volume={},
  number={},
  pages={276-285},
  abstract={Citation matching, or the automatic grouping of bibliographic references that refer to the same document, is a data management problem faced by automatic digital libraries for scientific literature such as CiteSeer and Google Scholar. Although several solutions have been offered for citation matching in large bibliographic databases, these solutions typically require expensive batch clustering operations that must be run offline. Large digital libraries containing citation information can reduce maintenance costs and provide new services through efficient online processing of citation data, resolving document citation relationships as new records become available. Additionally, information found in citations can be used to supplement document metadata, requiring the generation of a canonical citation record from merging variant citation subfields into a unified "best guess" from which to draw information. Citation information must be merged with other information sources in order to provide a complete document record. This paper outlines a system and algorithms for online citation matching and canonical metadata generation. A Bayesian framework is employed to build the ideal citation record for a document that carries the added advantages of fusing information from disparate sources and increasing system resilience to erroneous data},
  keywords={Software libraries;Bayesian methods;Information retrieval;Citation analysis;Data mining;Indexing;Information systems;Permission;Paper technology;Computer science;CiteSeer;bayesian inference;citation matching},
  doi={10.1145/1141753.1141817},
  ISSN={},
  month={June},}@ARTICLE{10107622,
  author={Neto, Helio N. Cunha and Hribar, Jernej and Dusparic, Ivana and Mattos, Diogo Menezes Ferrazani and Fernandes, Natalia C.},
  journal={IEEE Access},
  title={A Survey on Securing Federated Learning: Analysis of Applications, Attacks, Challenges, and Trends},
  year={2023},
  volume={11},
  number={},
  pages={41928-41953},
  abstract={The growth of data generation capabilities, facilitated by advancements in communication and computation technologies, as well as the rise of the Internet of Things (IoT), results in vast amounts of data that significantly enhance the performance of machine learning models. However, collecting all necessary data to train accurate models is often unfeasible due to privacy laws. Federated Learning (FL) evolved as a collaborative machine learning approach for training models without sharing private data. Unfortunately, several in-design vulnerabilities have been exposed, allowing attackers to infer private data of participants and negatively impacting the performance of the federated model. In light of these challenges and to encourage the development of FL solutions, this paper provides a comprehensive analysis of secure FL proposals that both protect user privacy and enhance the performance of the model. We performed a systematic review using predefined criteria to screen and extract data from multiple electronic databases, resulting in a final set of studies for analysis. Through the systematic review methodology, the paper groups the security vulnerabilities of FL into model performance and data privacy attacks. It also presents an analysis and comparison of potential mitigation strategies against these attacks. Additionally, the paper conducts a security analysis of state-of-the-art FL applications and proposals based on the vulnerabilities addressed. Finally, the paper outlines the main applications of secure FL and lists future research challenges. The survey highlights the crucial role of security strategies in ensuring the protection of user privacy and model performance in the context of future FL applications.},
  keywords={Data models;Computational modeling;Security;Cloud computing;Collaboration;Federated learning;Machine learning;Information security;Edge computing;Federated learning;machine learning;collaborative learning;information security;multiaccess edge computing},
  doi={10.1109/ACCESS.2023.3269980},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10245165,
  author={Vijay, Margret and Jayan, J. P.},
  booktitle={2023 International Conference on Circuit Power and Computing Technologies (ICCPCT)},
  title={Evolution of Research on Adaptive Gamification: A Bibliometric Study},
  year={2023},
  volume={},
  number={},
  pages={1062-1070},
  abstract={This study explores the evolving landscape of adaptive gamification-related research from 2014 to 2022. A comprehensive analysis of 1110 documents from journals and books was conducted to investigate publication trends, prominent countries, authors, and the development of knowledge in the field. The findings reveal a consistent growth in publications, with an annual growth rate of 26.89%. Gamification emerges as a dominant cluster, consistently exhibiting high centrality and density scores, indicating its strong interconnectedness and influence within the research network. The country-wise analysis demonstrates a diverse global research landscape, with the United States contributing the highest number of publications, followed by Spain and Germany. Prominent authors from universities and institutions worldwide have made significant contributions to the field. The thematic evolution demonstrates a shift from collaborative learning to gamification and online learning, followed by a focus on technology-driven themes and the emergence of machine learning as a prominent area of research. The increasing relevance of machine learning indicates its growing significance in the domain of adaptive gamification. These findings contribute to the knowledge base of adaptive gamification and offer insights for researchers in shaping the future of gamified learning environments.},
  keywords={Computers;Adaptive learning;Text analysis;Federated learning;Knowledge based systems;Bibliometrics;Integrated circuit interconnections;adaptive gamification;bibliometric analysis;machine learning;thematic evolution;personalized learning;game-based learning},
  doi={10.1109/ICCPCT58313.2023.10245165},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8869283,
  author={Sapna, R. and Monikarani, H.G. and Mishra, Shakti},
  booktitle={2019 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)},
  title={Linked data through the lens of machine learning: an Enterprise view},
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Enterprises have adopted semantic Web technologies for data reuse and sharing purposes and machine learning techniques for data analysis. Semantic Web is a generative technology which aims to build a giant global dataspace on the Web by interlinking diverse data sources available on the Web. Machine learning is a field of computational science that includes several generative algorithms that adaptively learn and make data-driven predictions. However, their confluence presents significant challenges from an enterprise context - that has not been effectively addressed. To solve the challenges, linked data can be chosen as the best practice for data representation while machine learning as a paradigm for data analysis. The paper proposes a conceptual model that confluences machine learning and linked data approaches, specifically to cater to enterprise needs. The conceptual model is applicable across varied enterprises viz., education, banking, government and medical, to name a few.},
  keywords={Ontologies;Machine learning;Resource description framework;Linked data;OWL;Semantics;Linked data;Ontology;Semantic Web;Machine Learning},
  doi={10.1109/ICECCT.2019.8869283},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{9411525,
  author={Almaghrabi, Maram and Chetty, Girija},
  booktitle={2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)},
  title={Deep Machine Learning Digital library recommendation system based on Metadata for Arabic and English Languages},
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={During the last three decades, information technologies are adopted by many libraries. It provides public access to their material in digital form to improve service. Metadata are the key aspect that must be considered to achieve a proper integration of digital library. It is data about data and has many purposes: data description, data browsing and data transfer. The advanced search engine for text documents allowed retrieving text information in an efficient way. For the organization structured digital collections on internet scale, metadata is an approach for retrieval improvement, preservation, and interoperability. However, such engines experienced low accuracy when documents had unique properties that need specialized and deeper semantic extraction. By Combining the strengths of the deep learning models with that of word embedding is the key to high-performance metadata classification in digital library recommendation system. Throughout this article, we aim at providing a proposed method on the utilization of the deep machine learning approaches to build digital library recommendation system based on Metadata for Arabic and English languages.},
  keywords={Support vector machines;Semantics;Organizations;Metadata;Search engines;Libraries;Information technology;Metadata;Digital libraries;Deep Learning;SVM;Similarity;Recommendation System},
  doi={10.1109/CSDE50874.2020.9411525},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{5687205,
  author={Diaz-Valenzuela, Irene and Martín-Bautista, Maria J. and Vila, M. Amparo},
  booktitle={2010 10th International Conference on Intelligent Systems Design and Applications},
  title={An automatic data mining authority control system: A first approach},
  year={2010},
  volume={},
  number={},
  pages={569-574},
  abstract={In this paper we present an automatic authority control system for raw noisy web data based on Data Mining. We use a hierarchical clustering approach with a special distance measure combination of three parameters: author name similarity, token similarity and co-authors similarity, each one defined in a specific way. A preliminary experimental study has been performed with real data obtained from CiteSeerX.},
  keywords={Libraries;Data mining;Databases;Clustering algorithms;Intelligent systems;Process control;Weight measurement;authority control;data mining;clustering;raw web data},
  doi={10.1109/ISDA.2010.5687205},
  ISSN={2164-7151},
  month={Nov},}@ARTICLE{10506811,
  author={Mutemi, Abed and Bacao, Fernando},
  journal={Big Data Mining and Analytics},
  title={E-Commerce Fraud Detection Based on Machine Learning Techniques: Systematic Literature Review},
  year={2024},
  volume={7},
  number={2},
  pages={419-444},
  abstract={The e-commerce industry's rapid growth, accelerated by the COVID-19 pandemic, has led to an alarming increase in digital fraud and associated losses. To establish a healthy e-commerce ecosystem, robust cyber security and anti-fraud measures are crucial. However, research on fraud detection systems has struggled to keep pace due to limited real-world datasets. Advances in artificial intelligence, Machine Learning (ML), and cloud computing have revitalized research and applications in this domain. While ML and data mining techniques are popular in fraud detection, specific reviews focusing on their application in e-commerce platforms like eBay and Facebook are lacking depth. Existing reviews provide broad overviews but fail to grasp the intricacies of ML algorithms in the e-commerce context. To bridge this gap, our study conducts a systematic literature review using the Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA) methodology. We aim to explore the effectiveness of these techniques in fraud detection within digital marketplaces and the broader e-commerce landscape. Understanding the current state of the literature and emerging trends is crucial given the rising fraud incidents and associated costs. Through our investigation, we identify research opportunities and provide insights to industry stakeholders on key ML and data mining techniques for combating e-commerce fraud. Our paper examines the research on these techniques as published in the past decade. Employing the PRISMA approach, we conducted a content analysis of 101 publications, identifying research gaps, recent techniques, and highlighting the increasing utilization of artificial neural networks in fraud detection within the industry.},
  keywords={Surveys;Industries;Systematics;Reviews;Bibliographies;Focusing;Machine learning;E-commerce;fraud detection;Machine Learning (ML);systematic review;organized retail fraud},
  doi={10.26599/BDMA.2023.9020023},
  ISSN={2097-406X},
  month={June},}@ARTICLE{9560087,
  author={Ramos, Ilmara Monteverde Martins and Ramos, David Brito and Gadelha, Bruno Freitas and de Oliveira, Elaine Harada Teixeira},
  journal={IEEE Transactions on Learning Technologies},
  title={An Approach to Group Formation in Collaborative Learning Using Learning Paths in Learning Management Systems},
  year={2021},
  volume={14},
  number={5},
  pages={555-567},
  abstract={Forming groups in distance education is challenging for teachers because, with this modality, only 20&#x0025; of the classes are held in person with the students. Thus, it is essential to achieve satisfactory results with automated approaches that can help teachers. In this article, an automated approach is proposed to assist teachers in recommending groups of students to learning management systems. We developed and validated a conceptual framework for group recommendation for collaborative activities using the characterization of learners based on learning paths (LPs). The approach emphasizes the formation of groups by applying the k-means algorithm, associated with three distance metrics of similarity (i.e., Euclidean, Manhattan, and cosine) in conjunction with the attributes derived from LPs. The framework was validated through the implementation of an M-Cluster tool. The M-Cluster presents three solution options, which can be visualized in a descriptive manner or via a bubble graph; it is the teacher who chooses the most acceptable solution for each case. The results of the case study indicate that the tool shows promise for improving the performance of students to up to 75&#x0025;.},
  keywords={Tools;Collaboration;Education;Collaborative work;Euclidean distance;Clustering algorithms;Learning management systems;Collaborative learning;group formation;learning management systems (LMSs);learning paths (LPs).},
  doi={10.1109/TLT.2021.3117916},
  ISSN={1939-1382},
  month={Oct},}@BOOK{8275574,
  author={Jain, Prateek and Kar, Purushottam},
  title={Non-convex Optimization for Machine Learning},
  year={2017},
  volume={},
  number={},
  pages={},
  abstract={Non-convex Optimization for Machine Learning takes an in-depth look at the basics of non-convex optimization with applications to machine learning. It introduces the rich literature in this area, as well as equips the reader with the tools and techniques needed to apply and analyze simple but powerful procedures for non-convex problems. Non-convex Optimization for Machine Learning is as self-contained as possible while not losing focus of the main topic of non-convex optimization techniques. The monograph initiates the discussion with entire chapters devoted to presenting a tutorial-like treatment of basic concepts in convex analysis and optimization, as well as their non-convex counterparts. The monograph concludes with a look at four interesting applications in the areas of machine learning and signal processing, and exploring how the non-convex optimization techniques introduced earlier can be used to solve these problems. The monograph also contains, for each of the topics discussed, exercises and figures designed to engage the reader, as well as extensive bibliographic notes pointing towards classical works and recent advances. Non-convex Optimization for Machine Learning can be used for a semester-length course on the basics of non-convex optimization with applications to machine learning. On the other hand, it is also possible to cherry pick individual portions, such the chapter on sparse recovery, or the EM algorithm, for inclusion in a broader course. Several courses such as those in machine learning, optimization, and signal processing may benefit from the inclusion of such topics.},
  keywords={Kernel methods;Nonparametric methods;Statistical learning theory;Statistical/machine learning;Pattern recognition and learning;Information theory and statistics;Learning and statistical methods;Filtering;Estimation;Identification},
  doi={10.1561/2200000058},
  ISSN={},
  publisher={now},
  isbn={9781680833690},
  url={https://ieeexplore.ieee.org/document/8275574},}@ARTICLE{10049452,
  author={Zieni, Rasha and Massari, Luisa and Calzarossa, Maria Carla},
  journal={IEEE Access},
  title={Phishing or Not Phishing? A Survey on the Detection of Phishing Websites},
  year={2023},
  volume={11},
  number={},
  pages={18499-18519},
  abstract={Phishing is a security threat with serious effects on individuals as well as on the targeted brands. Although this threat has been around for quite a long time, it is still very active and successful. In fact, the tactics used by attackers have been evolving continuously in the years to make the attacks more convincing and effective. In this context, phishing detection is of primary importance. The literature offers many diverse solutions that cope with this issue and in particular with the detection of phishing websites. This paper provides a broad and comprehensive review of the state of the art in this field by discussing the main challenges and findings. More specifically, the discussion is centered around three important categories of detection approaches, namely, list-based, similarity-based and machine learning-based. For each category we describe the detection methods proposed in the literature together with the datasets considered for their assessment and we discuss some research gaps that need to be filled.},
  keywords={Phishing;Uniform resource locators;Machine learning;Security;Blocklists;Visualization;Market research;Social engineering (security);Phishing;security threat;phishing website;phishing detection;URL;blacklists;machine learning;page similarity;datasets;social engineering},
  doi={10.1109/ACCESS.2023.3247135},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10099343,
  author={Ali, Subhan and Imran, Ali Shariq and Kastrati, Zenun and Daudpota, Sher Muhammad},
  booktitle={2023 4th International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)},
  title={Visualizing Research on Explainable Artificial Intelligence for Medical and Healthcare},
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Understanding complex machine learning and artificial intelligence models have always been challenging because these models are black-box, and often we don't know what information models rely upon to infer. Explainable Artificial Intelligence (XAI) has emerged as a new exciting field to explain and understand these machine learning models as humans can understand and improve them. In the past few years, there have been numerous research articles on explainable artificial intelligence for medical and healthcare. 1687 documents are being studied and analysed using bibliometric methods in this work. There are certain systematic reviews on the same topic, but this study is the first of its kind to use a quantitative method to analyze a large number of publications. The results of this study show that the research in this field took pace in 2011, and there have been quite many publications in the following years. We have also identified top-cited journals and articles. Through thematic analysis, we have found some important thematic areas of research in the field of XAI for medical and healthcare. The findings showed that the USA is the global leader in XAI research, followed by China and Canada at second and third place, respectively.},
  keywords={Visualization;Image segmentation;Systematics;Bibliometrics;Medical services;Machine learning;Market research;xai;explainable;interpretability;artificial intelligence;machine learning;deep learning;medical;healthcare;bibliometric analysis},
  doi={10.1109/iCoMET57998.2023.10099343},
  ISSN={},
  month={March},}@INPROCEEDINGS{8269991,
  author={Audeh, Bissan and Beigbeder, Michel and Largeron, Christine},
  booktitle={2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
  title={A Machine Learning System for Assisting Neophyte Researchers in Digital Libraries},
  year={2017},
  volume={01},
  number={},
  pages={317-323},
  abstract={Although existing digital libraries such as Google Scholar and CiteSeerX propose advanced search functionalities, they do not take into consideration whether the user is new or specialized in the research domain of his query. As a result, neophytes can spend a lot of time checking documents that are not adapted to their initial information need. In this paper, we propose NeoTex, a machine learning based approach that combines content-based retrieval and citation graph measures to propose documents adapted to new researchers. The contributions of our work are: designing a model for scientific retrieval suited to neophytes, defining an evaluation protocol with realistic ground truths, and testing the model on a large real collection from a national digital library.},
  keywords={Libraries;Information retrieval;Round robin;Context modeling;Supervised learning;Electronic mail;Google;Machine Learning;Digital Libraries;Citation Graph},
  doi={10.1109/ICDAR.2017.60},
  ISSN={2379-2140},
  month={Nov},}@ARTICLE{10278119,
  author={},
  journal={IEEE P2986/D1.1, September 2023},
  title={IEEE Draft Recommended Practice for Privacy and Security for Federated Machine Learning},
  year={2023},
  volume={},
  number={},
  pages={1-61},
  abstract={Privacy and security issues pose great challenges to the federated machine leaning (FML) community. A general view on privacy and security risks while meeting applicable privacy and security requirements in FML is provided. This recommended practice is provided in four parts: malicious failure and non-malicious failure in FML, privacy and security requirements from the perspective of system and FML participants, defensive methods and fault recovery methods, and the privacy and security risks evaluation. It also provides some guidance for typical FML scenarios in different industry areas, which can facilitate practitioners to use FML in a better way.},
  keywords={IEEE Standards;Machine learning;Federated learning;Privacy;Security;federated machine learning;FML;IEEE 2986™;machine learning;privacy;security},
  doi={},
  ISSN={},
  month={Oct},}@ARTICLE{10507779,
  author={},
  journal={IEEE Std 2986-2023},
  title={IEEE Recommended Practice for Privacy and Security for Federated Machine Learning},
  year={2024},
  volume={},
  number={},
  pages={1-57},
  abstract={Privacy and security issues pose great challenges to the federated machine leaning (FML) community. A general view on privacy and security risks while meeting applicable privacy and security requirements in FML is provided. This recommended practice is provided in four parts: malicious failure and non-malicious failure in FML, privacy and security requirements from the perspective of system and FML participants, defensive methods and fault recovery methods, and the privacy and security risks evaluation. It also provides some guidance for typical FML scenarios in different industry areas, which can facilitate practitioners to use FML in a better way.},
  keywords={IEEE Standards;Federated learning;Machine learning;Privacy;Security;federated machine learning;FML;IEEE 2986™;machine learning;privacy;security},
  doi={10.1109/IEEESTD.2024.10507779},
  ISSN={},
  month={April},}@INPROCEEDINGS{6118965,
  author={Guo, Zhixin and Jin, Hai},
  booktitle={2011 12th International Conference on Parallel and Distributed Computing, Applications and Technologies},
  title={Reference metadata extraction from scientific papers},
  year={2011},
  volume={},
  number={},
  pages={45-49},
  abstract={Bibliographical information of scientific papers is of great value since the Science Citation Index is introduced to measure research impact. Most scientific documents available on the web are unstructured or semi-structured, and the automatic reference metadata extraction process becomes an important task. This paper describes a framework for automatic reference metadata extraction from scientific papers. Our system can extract title, author, journal, volume, year, and page from scientific papers in PDF. We utilize a document metadata knowledge base to guide the reference metadata extraction process. The experiment results show that our system achieves a high accuracy.},
  keywords={Data mining;Portable document format;Knowledge based systems;Accuracy;Libraries;Semantics;Hidden Markov models;metadata extraction;rule-based approach;reference},
  doi={10.1109/PDCAT.2011.72},
  ISSN={2379-5352},
  month={Oct},}@ARTICLE{9693903,
  author={Sobreiro, Pedro and Martinho, Domingos Dos Santos and Alonso, José G. and Berrocal, Javier},
  journal={IEEE Access},
  title={A SLR on Customer Dropout Prediction},
  year={2022},
  volume={10},
  number={},
  pages={14529-14547},
  abstract={Dropout prediction is a problem that is being addressed with machine learning algorithms; thus, appropriate approaches to address the dropout rate are needed. The selection of an algorithm to predict the dropout rate is only one problem to be addressed. Other aspects should also be considered, such as which features should be selected and how to measure accuracy while considering whether the features are appropriate according to the business context in which they are employed. To solve these questions, the goal of this paper is to develop a systematic literature review to evaluate the development of existing studies and to predict the dropout rate in contractual settings using machine learning to identify current trends and research opportunities. The results of this study identify trends in the use of machine learning algorithms in different business areas and in the adoption of machine learning algorithms, including which metrics are being adopted and what features are being applied. Finally, some research opportunities and gaps that could be explored in future research are presented.},
  keywords={Machine learning algorithms;Machine learning;Companies;Organizations;Data mining;Predictive models;Prediction algorithms;Customers;dropout prediction;machine learning;systematic review},
  doi={10.1109/ACCESS.2022.3146397},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{1313420,
  author={Kawano, H.},
  booktitle={International Conference on Informatics Research for Development of Knowledge Society Infrastructure, 2004. ICKS 2004.},
  title={Applications of Web mining - from Web search engine to P2P filtering $},
  year={2004},
  volume={},
  number={},
  pages={150-157},
  abstract={We have developed Japanese Web search engine "Mondou (RCAAU)", which was based on the emerging technologies of data mining. Our search engine provides associative keywords which are tightly related to focusing Web pages. We also implemented the visual interface based on the technology of information visualization. In order to improve the performance of various search strategies by using characteristics of Web systems, we try to implement the advanced Web information systems with data mining and information technologies. Firstly, we introduce various Web mining algorithm, which efficiently reduces the computing cost of Web search. We pay attention to a part of useful pages effectively and improve the performance of Web search by using our proposed algorithms. Secondly, for preserving huge volume of born-digital information in the Internet, we are focusing on technologies of Web archiving system like WARP. In order to handle monotonously increasing digital information, we have to resolve many difficult problems of long life data preservation by improving Web searching techniques. Our experiences of our Mondou Web search engine and cooperative distributed Web robots are very useful and effective. Finally, the technologies of P2P (Peer-to-Peer) distributed search systems are becoming important rapidly. For example, it is very hard to discover appropriate information resources by simple queries of Gnutella, Freenet and so on. Therefore, in order to realize the topic-driven search, we propose more intelligent search systems, which are based on the technologies of data mining.},
  keywords={Web mining;Web search;Search engines;Information filtering;Information filters;Data mining;Web pages;Data visualization;Information systems;Information technology},
  doi={10.1109/ICKS.2004.1313420},
  ISSN={},
  month={March},}@INPROCEEDINGS{10339029,
  author={Simeonidis, Dimitrios and Petrov, Pavel and Jordanov, Jordan},
  booktitle={2023 International Conference Automatics and Informatics (ICAI)},
  title={Network Intrusion Detection Through Classification Methods and Machine Learning Techniques},
  year={2023},
  volume={},
  number={},
  pages={409-413},
  abstract={The field of network attacks is rapidly evolving, which makes it particularly difficult to grasp a universal picture of the problem. Intrusion detection is one of the most important tools for securing and protecting computer systems from malicious attackers by enabling detection of attacks and reducing their impact. In this research, we study the problem of cyberattacks and approach the possibility of securing computer and network communications by proposing intrusion detection approaches based on classification and machine learning mechanisms. Considering that intrusion detection is not simply a matter of predicting the most likely classification (attack-“normal” behavior), since different types of errors incur different costs, we propose the implementation of a cost-sensitive approach to intrusion detection and apply it to some well-known and efficient classification algorithms for both wired and wireless networks. The research method used in the study employs a research design focused on evaluating the effectiveness of intrusion detection systems using classification algorithms. Statistical analysis could be applied as necessary to analyze the data, and the research is designed to be reproducible for future studies.},
  keywords={Training;Computer hacking;Wireless networks;Intrusion detection;Telecommunication traffic;Machine learning;Classification algorithms;computer network;intrusion detection;machine learning;classification},
  doi={10.1109/ICAI58806.2023.10339029},
  ISSN={},
  month={Oct},}@ARTICLE{9999444,
  author={Miltiadous, Andreas and Tzimourta, Katerina D. and Giannakeas, Nikolaos and Tsipouras, Markos G. and Glavas, Euripidis and Kalafatakis, Konstantinos and Tzallas, Alexandros T.},
  journal={IEEE Access},
  title={Machine Learning Algorithms for Epilepsy Detection Based on Published EEG Databases: A Systematic Review},
  year={2023},
  volume={11},
  number={},
  pages={564-594},
  abstract={Epilepsy is the only neurological condition for which electroencephalography (EEG) is the primary diagnostic and important prognostic clinical tool. However, the manual inspection of EEG signals is a time-consuming procedure for neurologists. Thus, intense research has been made on creating machine learning methodologies for automated epilepsy detection. Also, many research or medical facilities have published databases of epileptic EEG signals to accommodate this research effort. The vast number of studies concerning epilepsy detection with EEG makes this systematic review necessary. It presents a detailed evaluation of the signal processing and classification methodologies employed on the different databases and provides valuable insights for future work. 190 studies were included in this systematic review according to the PRISMA guidelines, acquired from a systematic literature search in PubMed, Scopus, ScienceDirect and IEEE Xplore on 1st May 2021. Studies were examined based on the Signal Transformation technique, classification methodology and database for evaluation. Along with other findings, the increasing tendency to employ Convolutional Neural Networks that use a combination of Time-Frequency decomposition methodology images is noticed.},
  keywords={Electroencephalography;Epilepsy;Databases;Systematics;Recording;Transforms;Machine learning;Database;detection;EEG;epilepsy;machine learning;signal transformation;systematic review},
  doi={10.1109/ACCESS.2022.3232563},
  ISSN={2169-3536},
  month={},}@ARTICLE{10192385,
  author={Akrami, Nouhaila El and Hanine, Mohamed and Flores, Emmanuel Soriano and Aray, Daniel Gavilanes and Ashraf, Imran},
  journal={IEEE Access},
  title={Unleashing the Potential of Blockchain and Machine Learning: Insights and Emerging Trends From Bibliometric Analysis},
  year={2023},
  volume={11},
  number={},
  pages={78879-78903},
  abstract={Blockchain and machine learning (ML) has garnered growing interest as cutting-edge technologies that have witnessed tremendous strides in their respective domains. Blockchain technology provides a decentralized and immutable ledger, enabling secure and transparent transactions without intermediaries. Alternatively, ML is a sub-field of artificial intelligence (AI) that empowers systems to enhance their performance by learning from data. The integration of these data-driven paradigms holds the potential to reinforce data privacy and security, improve data analysis accuracy, and automate complex processes. The confluence of blockchain and ML has sparked increasing interest among scholars and researchers. Therefore, a bibliometric analysis is carried out to investigate the key focus areas, hotspots, potential prospects, and dynamical aspects of the field. This paper evaluates 700 manuscripts drawn from the Web of Science (WoS) core collection database, spanning from 2017 to 2022. The analysis is conducted using advanced bibliometric tools (e.g., Bibliometrix R, VOSviewer, and CiteSpace) to assess various aspects of the research area regarding publication productivity, influential articles, prolific authors, the productivity of academic countries and institutions, as well as the intellectual structure in terms of hot topics and emerging trends. The findings suggest that upcoming research should focus on blockchain technology, AI-powered 5G networks, industrial cyber-physical systems, IoT environments, and autonomous vehicles. This paper provides a valuable foundation for both academic scholars and practitioners as they contemplate future projects on the integration of blockchain and ML.},
  keywords={Blockchains;Bibliometrics;Market research;Machine learning;Business;Bibliographies;Systematics;Visualization;Blockchain;machine learning;bibliometric analysis;network visualization},
  doi={10.1109/ACCESS.2023.3298371},
  ISSN={2169-3536},
  month={},}@ARTICLE{10723263,
  author={},
  journal={IEEE P3198/D3, October 2024},
  title={IEEE Draft Standard for Evaluation Method of Machine Learning Fairness},
  year={2024},
  volume={},
  number={},
  pages={1-34},
  abstract={This document specifies a method for evaluating the fairness of machine learning. Multiple causes contribute to the unfairness of machine learning. In this document, these causes of machine learning unfairness are categorized. The widely recognized and used definitions of machine learning fairness are presented. This document also specifies various metrics corresponding to the definitions, and how to calculate the metrics. Test cases in this document give detailed conditions and procedures to set up the tests for evaluating machine learning fairness.},
  keywords={IEEE Standards;Machine learning;Performance evaluation;Measurement;Quality assessment;Artificial intelligence;machine learning;fairness;evaluation method;evaluation metric;bias},
  doi={},
  ISSN={},
  month={Oct},}@ARTICLE{9878042,
  author={Zurita, Gustavo and Mulet-Forteza, Carles and Merigó, José M. and Lobos-Ossandón, Valeria and Ogata, Hiroaki},
  journal={IEEE Transactions on Learning Technologies},
  title={A Bibliometric Overview of the IEEE Transactions on Learning Technologies},
  year={2022},
  volume={15},
  number={6},
  pages={656-672},
  abstract={IEEE Transactions on Learning Technologies (IEEE-TLT) is a leading journal in the fields of computer science and educational research with a focus on learning technologies. It published its first issue in 2008 and commemorated its 15th anniversary in 2022. Inspired by this event, this article provides a general lifetime overview of the journal using bibliometric indicators and science mapping analysis. The main objective is to provide a complete overview of the main components that have affected the journal. This analysis includes key factors such as the most cited articles and the leading authors, institutions, and countries for the journal, along with an insight into the publication and the citation structure. We use the Web of Science Core Collection database to analyze the bibliometric data and VOSviewer software to graphically map the bibliographic material using a bibliographic coupling, cocitation, and co-occurrence of author keywords. With this analysis, we gain a deeper understanding of how IEEE-TLT is connected to other journals and researchers across the globe and how it contributes to scientific communities. Results indicate that IEEE-TLT is a high-impact journal in computer Science and education and has been referenced by a wide range of authors, institutions, countries, and the main topics related to learning technologies from all over the world.},
  keywords={Bibliometrics;Education;Databases;Computer science;Software;Productivity;Economics;Bibliometrics;general literary works;journal impact study;learning technologies},
  doi={10.1109/TLT.2022.3204457},
  ISSN={1939-1382},
  month={Dec},}@ARTICLE{10041115,
  author={Barua, Arnab and Ahmed, Mobyen Uddin and Begum, Shahina},
  journal={IEEE Access},
  title={A Systematic Literature Review on Multimodal Machine Learning: Applications, Challenges, Gaps and Future Directions},
  year={2023},
  volume={11},
  number={},
  pages={14804-14831},
  abstract={Multimodal machine learning (MML) is a tempting multidisciplinary research area where heterogeneous data from multiple modalities and machine learning (ML) are combined to solve critical problems. Usually, research works use data from a single modality, such as images, audio, text, and signals. However, real-world issues have become critical now, and handling them using multiple modalities of data instead of a single modality can significantly impact finding solutions. ML algorithms play an essential role in tuning parameters in developing MML models. This paper reviews recent advancements in the challenges of MML, namely: representation, translation, alignment, fusion and co-learning, and presents the gaps and challenges. A systematic literature review (SLR) was applied to define the progress and trends on those challenges in the MML domain. In total, 1032 articles were examined in this review to extract features like source, domain, application, modality, etc. This research article will help researchers understand the constant state of MML and navigate the selection of future research directions.},
  keywords={Market research;Visualization;Systematics;Representation learning;Medical services;Biomedical imaging;Bibliographies;Multimodal machine learning;systematic literature review;representation;translation;alignment;fusion;co-learning},
  doi={10.1109/ACCESS.2023.3243854},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9724962,
  author={Ying, Changtian and Li, Qi and Liu, Jianhua},
  booktitle={2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM)},
  title={A Brief Investigation for Techniques of Deep Learning Model in Smart Grid},
  year={2021},
  volume={},
  number={},
  pages={173-181},
  abstract={Over the last few years, smart grid has gained a tremendous attention from the research community. To ensure the effective acquisition, transmission, protection and control of various data in the smart grid, so that the smart grid can realize the dynamic exchange and interaction of information and meet the sustainable requirements of power development, which has gradually become a realistic choice for the construction and development of smart grid. Various soft and hardware techniques have been proposed for efficient smart grid. Deep learning techniques are one of the soft computing approaches which can be applied to automate and further improve the performance of the smart grid. Although, this research domain of deep learning model in smart grid is gaining some attention, there is a strong need for a motivation to encourage researchers to explore more in this area. In this paper, we have investigated on recent development in the field of deep learning techniques in smart grid. In the study, various performance metrics including total papers, total citations, and citation per paper are calculated. Further, top 10 or 20 of most productive and highly cited authors, discipline, source journals, countries, institutions, and highly influential papers are also evaluated. Later, a comparative analysis is performed on the deep learning techniques in smart grid after analyzing the most influential works in this field.},
  keywords={Deep learning;Training;Learning systems;Analytical models;Solid modeling;Computational modeling;Neurons;smart grid;deep learning;literature analysis;investigation},
  doi={10.1109/AIAM54119.2021.00045},
  ISSN={},
  month={Oct},}@ARTICLE{8063898,
  author={Salawu, Semiu and He, Yulan and Lumsden, Joanna},
  journal={IEEE Transactions on Affective Computing},
  title={Approaches to Automated Detection of Cyberbullying: A Survey},
  year={2020},
  volume={11},
  number={1},
  pages={3-24},
  abstract={Research into cyberbullying detection has increased in recent years, due in part to the proliferation of cyberbullying across social media and its detrimental effect on young people. A growing body of work is emerging on automated approaches to cyberbullying detection. These approaches utilise machine learning and natural language processing techniques to identify the characteristics of a cyberbullying exchange and automatically detect cyberbullying by matching textual data to the identified traits. In this paper, we present a systematic review of published research (as identified via Scopus, ACM and IEEE Xplore bibliographic databases) on cyberbullying detection approaches. On the basis of our extensive literature review, we categorise existing approaches into 4 main classes, namely supervised learning, lexicon-based, rule-based, and mixed-initiative approaches. Supervised learning-based approaches typically use classifiers such as SVM and Naıve Bayes to develop predictive models for cyberbullying detection. Lexicon-based systems utilise word lists and use the presence of words within the lists to detect cyberbullying. Rule-based approaches match text to predefined rules to identify bullying, and mixed-initiatives approaches combine human-based reasoning with one or more of the aforementioned approaches. We found lack of labelled datasets and non-holistic consideration of cyberbullying by researchers when developing detection systems are two key challenges facing cyberbullying detection research. This paper essentially maps out the state-of-the-art in cyberbullying detection research and serves as a resource for researchers to determine where to best direct their future research efforts in this field.},
  keywords={Sentiment analysis;Social networking (online);Natural language processing;Supervised learning;Behavioral sciences;Machine learning;Abuse and crime involving computers;data mining;machine learning;natural language processing;sentiment analysis;social networking},
  doi={10.1109/TAFFC.2017.2761757},
  ISSN={1949-3045},
  month={Jan},}@ARTICLE{10217126,
  author={},
  journal={IEEE P2986/D1, August 2023},
  title={IEEE Draft Recommended Practice for Privacy and Security for Federated Machine Learning},
  year={2023},
  volume={},
  number={},
  pages={1-61},
  abstract={Privacy and security issues pose great challenges to the federated machine leaning (FML) community. A general view on privacy and security risks while meeting applicable privacy and security requirements in FML is provided. This recommended practice is provided in four parts: malicious failure and non-malicious failure in FML, privacy and security requirements from the perspective of system and FML participants, defensive methods and fault recovery methods, and the privacy and security risks evaluation. It also provides some guidance for typical FML scenarios in different industry areas, which can facilitate practitioners to use FML in a better way.},
  keywords={IEEE Standards;Security;Privacy;Machine learning;federated machine learning;FML;IEEE 2986™;machine learning;privacy;security},
  doi={},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9582563,
  author={Napoleão, Bianca Minetto and Petrillo, Fabio and Hallé, Sylvain},
  booktitle={2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)},
  title={Automated Support for Searching and Selecting Evidence in Software Engineering: A Cross-domain Systematic Mapping},
  year={2021},
  volume={},
  number={},
  pages={45-53},
  abstract={Context: Searching and selecting relevant evidence is crucial to answer research questions from secondary studies in Software Engineering (SE). The activities of search and selection of studies are labour-intensive, time-consuming and demand automation support. Objective: Our goal is to identify and summarize the state-of-the-art on automation support for searching and selecting evidence for secondary studies in SE. Method: We performed a systematic mapping on existing automating support to search and select evidence for secondary studies in SE, expanding our investigation in a cross-domain study addressing advancements from the medical field. Results: Our results show that the SE field has a variety of tools and Text Classification (TC) approaches to automate the search and selection activities. However, medicine has more well-established tools with a larger adoption than SE. Cross-validation and experiment are the most adopted methods to assess TC approaches. Furthermore, recall and precision are the most adopted assessment metrics. Conclusion: Automated approaches for searching and selecting studies in SE have not been applied in practice by SE researchers. Integrated and easy-to-use automated approaches addressing consolidated TC techniques can bring relevant advantages on workload and time saving for SE researchers who conduct secondary studies.},
  keywords={Measurement;Automation;Systematics;Text categorization;Tools;Search problems;Object recognition;Secondary Studies;Systematic Review Automation;Search of Studies;Selection of Studies},
  doi={10.1109/SEAA53835.2021.00015},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{994694,
  author={Elfeky, M.G. and Verykios, V.S. and Elmagarmid, A.K.},
  booktitle={Proceedings 18th International Conference on Data Engineering},
  title={TAILOR: a record linkage toolbox},
  year={2002},
  volume={},
  number={},
  pages={17-28},
  abstract={Data cleaning is a vital process that ensures the quality of data stored in real-world databases. Data cleaning problems are frequently encountered in many research areas, such as knowledge discovery in databases, data warehousing, system integration and e-services. The process of identifying the record pairs that represent the same entity (duplicate records), commonly known as record linkage, is one of the essential elements of data cleaning. In this paper, we address the record linkage problem by adopting a machine learning approach. Three models are proposed and are analyzed empirically. Since no existing model, including those proposed in this paper, has been proved to be superior, we have developed an interactive record linkage toolbox named TAILOR (backwards acronym for "RecOrd LInkAge Toolbox"). Users of TAILOR can build their own record linkage models by tuning system parameters and by plugging in in-house-developed and public-domain tools. The proposed toolbox serves as a framework for the record linkage process, and is designed in an extensible way to interface with existing and future record linkage models. We have conducted an extensive experimental study to evaluate our proposed models using not only synthetic but also real data. The results show that the proposed machine-learning record linkage models outperform the existing ones both in accuracy and in performance.},
  keywords={Couplings;Cleaning;Databases;Machine learning;Pattern classification;Decision trees;Educational institutions;Information science;Warehousing;Process design},
  doi={10.1109/ICDE.2002.994694},
  ISSN={1063-6382},
  month={Feb},}@INPROCEEDINGS{4031712,
  author={Ning, Xiaomin and Jin, Hai and Wu, Hao},
  booktitle={2006 IEEE International Conference on e-Business Engineering (ICEBE'06)},
  title={SemreX: Towards Large-Scale Literature Information Retrieval and Browsing with Semantic Association},
  year={2006},
  volume={},
  number={},
  pages={602-609},
  abstract={Access to scientific literature information is a very important, as well as time-consuming daily work for scientific researchers. Current methods of retrieval are usually limited to keyword-based searching using information retrieval techniques. In this paper, we present SemreX which implements efficient large-scale literature retrieval and browsing with a single access point based on semantic Web technologies. The concept of semantic association is proposed to reveal explicit or implicit relationships between semantic entities, combining with the ontology-based information visualization technique so as to facilitate researchers retrieving semantically relevant information, as well as context relationships which can capture user's current search intentions while preserving an overall picture of scientific knowledge},
  keywords={Large-scale systems;Information retrieval;Semantic Web;Ontologies;Visualization;Citation analysis;Search engines;Image retrieval;Grid computing;Standards development},
  doi={10.1109/ICEBE.2006.87},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10406842,
  author={Zuefle, M. and Rennpferdt, C. and Batora, M. and Bursac, N. and Krause, D.},
  booktitle={2023 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
  title={Complexity Coping by Methodical Agile and Modular Product Development - A Bibliometric Review},
  year={2023},
  volume={},
  number={},
  pages={1394-1400},
  abstract={This bibliometric analysis identifies the extent to which the methodical development of modular product families (product side) and agile project development (process side) can be beneficial for coping with complex product development tasks. The accomplished co-occurrence analysis reveals an indirect relationship between the development of modular product families and agile product development. This indirect connection is because the different methods are incorporated in the field of Design Methodology. However, the analysis leaves a gap in the direct interaction of both subjects, a subsequently accomplished bibliographic coupling points out. A consecutive in-depth abstract and paper analysis reveals further research on their integration is necessary.},
  keywords={Couplings;Design methodology;Bibliometrics;Industrial engineering;Product development;Complexity theory;Task analysis;Bibliometric Analysis;Agile Development;Modular Development;Modular Design;System architecture},
  doi={10.1109/IEEM58616.2023.10406842},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{6195410,
  author={Dendek, Piotr Jan and Bolikowski, Lukasz and Lukasik, Michal},
  booktitle={2012 10th IAPR International Workshop on Document Analysis Systems},
  title={Evaluation of Features for Author Name Disambiguation Using Linear Support Vector Machines},
  year={2012},
  volume={},
  number={},
  pages={440-444},
  abstract={Author name disambiguation allows to distinguish between two or more authors sharing the same name. In a previous paper, we have proposed a name disambiguation framework in which for each author name in each article we build a context consisting of classification codes, bibliographic references, co-authors, etc. Then, by pair wise comparison of contexts, we have been grouping contributions likely referring to the same people. In this paper we examine which elements of the context are most effective in author name disambiguation. We employ linear Support Vector Machines (SVM) to find the most influential features.},
  keywords={Support vector machines;Electronic mail;Null value;Libraries;Vectors;Context;Conferences;Name Disambiguation;Support Vector Machines;classification algorithms;supervised learning;context;bibliographies;linearity;information retrieval;computer science},
  doi={10.1109/DAS.2012.36},
  ISSN={},
  month={March},}@ARTICLE{9815588,
  author={Shafiq, Dalia Abdulkareem and Marjani, Mohsen and Habeeb, Riyaz Ahamed Ariyaluran and Asirvatham, David},
  journal={IEEE Access},
  title={Student Retention Using Educational Data Mining and Predictive Analytics: A Systematic Literature Review},
  year={2022},
  volume={10},
  number={},
  pages={72480-72503},
  abstract={Student retention is an essential measurement metric in education, indicated by retention rates, which are accumulated as students re-enroll from one academic year to the next. High retention rates can be obtained if institutions aim to provide appropriate support and teaching methods among the various practices to prevent students from deferring their studies. To address this pressing challenge faced by educational institutions, the underlying factors and the methodological aspects of building robust predictive models are reviewed and scrutinized. Educational Data Mining (EDM) and Learning Analytics (LA) have been widely adopted for knowledge discovery from educational data sources, improving the teaching practice, and identifying at-risk students. Various predictive techniques are applied in LA, such as Machine Learning (ML), Statistical Analysis, and Deep Learning (DL). To gain an in-depth review of these techniques, academic publications have been reviewed to highlight their potential to resolve Student Retention issues in education. Additionally, the paper presents a taxonomy of ML approaches and a comprehensive review of the success factors and the features that are not indicative of student performance in three different learning environments: Traditional Learning, Blended Learning, and Online Learning. The survey reveals that supervised ML and DL techniques are broadly applied in Student Retention. However, the application of ensemble and unsupervised learning clustering techniques supporting the heterogenous and homogenous groups of students is generally lacking. Moreover, static and traditional features are commonly used in student performance, ignoring vital factors such as educators-related, cognitive, and personal data. Furthermore, the paper highlights open challenges for future research directions.},
  keywords={Education;Data mining;Systematics;Predictive models;Prediction algorithms;Bibliographies;Soft sensors;Educational data mining;learning analytics;machine learning;predictive models;student retention},
  doi={10.1109/ACCESS.2022.3188767},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9671612,
  author={Wu, Jian and Rohatgi, Shaurya and Reddy Keesara, Sai Raghav and Chhay, Jason and Kuo, Kevin and Menon, Arjun Manoj and Parsons, Sean and Urgaonkar, Bhuvan and Giles, C. Lee},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)},
  title={Building an Accessible, Usable, Scalable, and Sustainable Service for Scholarly Big Data},
  year={2021},
  volume={},
  number={},
  pages={141-152},
  abstract={Since the emergence of scholarly big data, there have been several efforts for web-based services such as digital library search engines (DLSEs). However, much of the design and specifications of an accessible, usable, scalable, and sustainable DLSE have not been well represented and discussed in the literature. We argue that these four characteristics are essential to providing a high-quality service for scholarly big data from both the user and developer’s perspectives. This paper reviews the design, implementation, and operation experiences, and lessons of CiteSeerX, a real-world digital library search engine. We analyze the strengths and weaknesses of the current design, and proposed a new design with a revised architecture, enhanced hardware, and software infrastructure. The Alpha version of the new design has been implemented and tested. The new system replaces MySQL and Apache Solr with a single instance of Elasticsearch, which plays a dual role of data storage and search. Another major improvement is the integration of extraction and ingestion, which significantly boosts document ingestion speed. The web application is re-engineered to enhance the user experience by applying a learning-to-rank model and offering more refined search tools. The system is also improved in many other aspects. We believe the design considerations and experience can benefit researchers and engineers who plan, design, and upgrade future systems with comparable scales and functionalities.},
  keywords={Buildings;Semantics;Computer architecture;Big Data;Search engines;Rendering (computer graphics);Libraries;scholarly big data;big data infrastructure;big data search;big data service},
  doi={10.1109/BigData52589.2021.9671612},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10151069,
  author={Sarwar, Saba and Jabin, Suraiya},
  booktitle={2023 International Conference on Recent Advances in Electrical, Electronics & Digital Healthcare Technologies (REEDCON)},
  title={AI Techniques for Cone Beam Computed Tomography in Dentistry: Trends and Practices},
  year={2023},
  volume={},
  number={},
  pages={226-231},
  abstract={Cone-beam computed tomography (CBCT) is a popular imaging modality in dentistry for diagnosing and planning treatment for a variety of oral diseases with the ability to produce detailed, three-dimensional images of the teeth, jawbones, and surrounding structures. CBCT imaging has emerged as an essential diagnostic tool in dentistry. CBCT imaging has seen significant improvements in terms of its diagnostic value, as well as its accuracy and efficiency, with the most recent development of artificial intelligence (AI) techniques. This paper reviews recent AI trends and practices in dental CBCT imaging. AI has been used for lesion detection, malocclusion classification, measurement of buccal bone thickness, and classification and segmentation of teeth, alveolar bones, mandibles, landmarks, contours, and pharyngeal airways using CBCT images. Mainly machine learning algorithms, deep learning algorithms, and super-resolution techniques are used for these tasks. This review focuses on the potential of AI techniques to transform CBCT imaging in dentistry, which would improve both diagnosis and treatment planning. Finally, we discuss the challenges and limitations of artificial intelligence in dentistry and CBCT imaging.},
  keywords={Machine learning algorithms;Computed tomography;Imaging;Teeth;Transforms;Market research;Bones;Artificial Intelligence;CBCT;Deep Learning;Image Analysis;Dentistry},
  doi={10.1109/REEDCON57544.2023.10151069},
  ISSN={},
  month={May},}@INPROCEEDINGS{6137491,
  author={Goncalves, Celia Talma and Camacho, Rui and Oliveira, Eugenio},
  booktitle={2011 IEEE 11th International Conference on Data Mining Workshops},
  title={From Sequences to Papers: An Information Retrieval Exercise},
  year={2011},
  volume={},
  number={},
  pages={1010-1017},
  abstract={Whenever new sequences of DNA or proteins have been decoded it is almost compulsory to look at similar sequences and papers describing those sequences in order to both collect relevant information concerning the function and activity of the new sequences and/or know what is known already about similar sequences that might be useful in the explanation of the function or activity of the newly discovered ones. In current web sites and data bases of sequences there are, usually, a set of paper references linked to each sequence. Those links are very useful because the papers describe useful information concerning the sequences. They are, therefore, a good starting point to look for relevant information related to a set of sequences. One way is to implement such approach is to do a blast with the new decoded sequences, and collect similar sequences. Then one looks at the papers linked with the similar sequences. Most often the number of retrieved papers is small and one has to search large data bases for relevant papers. In this paper we propose a process of generating a classifier based on the initially set of relevant papers that are directly linked to the similar sequences retrieved and use that classifier to automatically enlarge the set of relevant papers by searching the MEDLINE using the automatically constructed classifier. We have empirically evaluated our proposal and report very promising results.},
  keywords={Databases;Abstracts;Dictionaries;Machine learning;Proteins;Machine learning algorithms;MEDLINE;classification;information retrieval system},
  doi={10.1109/ICDMW.2011.184},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{10293405,
  author={Alsaadi, Husam Ibrahiem Husain and Al-Anni, Maad Kamal and Al-Khuzaie, Fanar Emad Khazaal},
  booktitle={2023 3rd International Conference on Emerging Smart Technologies and Applications (eSmarTA)},
  title={Deep Learning to Mitigate Economic Denial of Sustainability (EDoS) Attacks: Cloud Computing},
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Cloud computing, despite being a groundbreaking technology, has increased the risk of attacks that exploit cloud services, leading to the exhaustion of resource allocations. One such attack is Economic Denial of Sustainability (EDoS), which uses pay-per-use services to gradually increase resource demand, potentially leading to financial disaster for service provider. The Cyber Range Lab at the University of New South Wales (UNSW), located in Canberra, conducted nine injection attacks to test machine learning and deep learning algorithms. These experiments involved multi-classification, incorporating nine different types of attack data. Among the presented algorithms, the RNN-LSTM method achieved an accuracy of 84%. Various statistical analyses, such as mean square error (MSE), Pearson correlation coefficient (R), and root mean square error (RMSE), were used to evaluate prediction errors between input data and values generated by different machine learning and deep learning algorithms. Despite its relatively low prediction level (MSE=0.560), the Recurrent Neural Network Long Short Term Memory (RNNLSTM) algorithm achieved an R2 level of 82.09%when applied to the multi-classification dataset. The suggested system's performance was compared to existing EDoS attack detection systems, and the RNN-LSTM-based attack mitigation algorithms demonstrated superior performance. This study aims to contribute to the identification and effective protection of cloud-based resources that could be exploited by Distributed Denial-of-Service (DDoS) attacks, thereby preventing financial losses for potential victims.},
  keywords={Deep learning;Training;Economics;Cloud computing;Machine learning algorithms;Statistical analysis;System performance;Deep Learning;Cloud Computing;Economic denial of sustainability (EDoS);Artificial Intelligent;Intrusion Detection Sytem (IDS);Denial of Service (DoS)},
  doi={10.1109/eSmarTA59349.2023.10293405},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10295716,
  author={Swandari, Yulia and Ferdiana, Ridi and Permanasari, Adhistya Erna},
  booktitle={2023 10th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)},
  title={Research Trends in Software Development Effort Estimation},
  year={2023},
  volume={},
  number={},
  pages={625-630},
  abstract={Developing a software project without the appropriate amount of effort would significantly impede and even fail the project, putting the software developer's quality at risk. Therefore, software development effort estimation (SDEE) is the most critical activity in software engineering. SDEE has seen extensive research, resulting in a massive rise in the literature in a relatively short period. In this regard, it is crucial to identify the significant study topics in software development effort estimation that will assist researchers in understanding and recognizing research trends. This research applied a systematic literature review (SLR) to compile all journals from the predefined search directory about software development effort estimation thoroughly and unbiasedly from 2018 to 2022. This review was a prelude to further research activities in software development effort estimation. Five research topics out of 71 papers have been revealed, including the machine learning approach, algorithmic technique, expert judgement, dataset analysis, and evaluation metric. With 27 journals, deploying a machine learning approach for SDEE is the most discussed research topic. The potential research described in this study can be investigated further in software development effort estimation field.},
  keywords={Measurement;Systematics;Machine learning algorithms;Software algorithms;Estimation;Manuals;Market research;software development;effort estimation;literature review;research trends},
  doi={10.1109/EECSI59885.2023.10295716},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{4741216,
  author={Lee, Dongwon and Kim, Hung-sik and Kim, Eun Kyung and Yan, Su and Chen, Johnny and Lee, Jeongkyu},
  booktitle={2008 Tenth IEEE International Symposium on Multimedia},
  title={LeeDeo: Web-Crawled Academic Video Search Engine},
  year={2008},
  volume={},
  number={},
  pages={497-502},
  abstract={We present our vision and preliminary design toward Web-crawled academic video search engine, named as LeeDeo, that can search, crawl, archive, index, and browse ldquoacademicrdquo videos from the Web. Our proposal differs from existing general-purpose search engines such as Google or MSN whose focus is on the search of textual HTML documents or metadata of multimedia objects. Similarly, our proposal also differs from existing academic bibliographic search engines such as CiteSeer, arXiv, or Google Scholar whose focus is on the search and analysis of PDF or PS documents of academic papers. As desiderata of such an academic video search engine, we discuss various issues as follows: (1) Crawling: how to crawl, identify, and download academic videos from the Web? (2) Classification: how to determine the so-called academic videos from the rest? (3) Extraction: how to extract metadata and transcripts from the classified videos? (4) Indexing: how to build indexes for search engines? and (5) Interface: how to provide interface for efficient browse and search of academic videos?},
  keywords={Search engines;Videoconference;Data mining;Indexing;HTML;Video sharing;Web pages;Crawlers;USA Councils;Proposals;LeeDeo;Academic;Video;Search;Engine;Crawl;Web},
  doi={10.1109/ISM.2008.105},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10700176,
  author={Merino, Roberto and Jara, Pablo and Peralta, Billy and Nicolis, Orietta and Lobel, Hans and Caro, Luis},
  booktitle={2024 L Latin American Computer Conference (CLEI)},
  title={Self-Supervised Learning Applied to Variable Star Semi-Supervised Classification Using LSTM and GRU Networks},
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Recognizing variable stars is a task of interest in the astronomy community. Currently, this task has taken advantage of deep learning algorithms. However, these algorithms require a large amount of data to achieve high levels of precision. In this work, self-supervised learning is proposed to improve the classification of variable stars considering a reduced amount of data using recurrent networks. The experiments in Gaia dataset show that the proposed approach allows to improve performance, when compared with traditional initialization schemes, up to 7% and 13% in real databases in semi-supervised learning scenarios. In future work, we propose considering experiments with other variable star databases.},
  keywords={Deep learning;Databases;Stars;Self-supervised learning;Semisupervised learning;Robustness;Classification algorithms;Long short term memory;Self-supervised learning;Variable star;Semi-supervised classification},
  doi={10.1109/CLEI64178.2024.10700176},
  ISSN={2771-5752},
  month={Aug},}@INPROCEEDINGS{10193176,
  author={Ivanova, Malinka and Stefanov, Svetlin},
  booktitle={2023 8th International Conference on Smart and Sustainable Technologies (SpliTech)},
  title={Digital Forensics Investigation Models: Current State and Analysis},
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={A digital forensics investigation (DFI) is characterized with activities related to search, identification, collection, analysis of digital evidences as all activities should be documented in a given format. Different models and methodologies are proposed to facilitate DFI, trying to describe procedures and activities performed by the investigator. Continuous technological development leads to the emergence of new sophisticated attacks, which also requires new approaches to be taken to crime scene investigation. In this paper a summary and analysis of contemporary achievements in DFI is performed to outline the current state of DFI development and to draw challenging issues and trending research topics. A conceptual model generalizing current state in DFI models is also presented.},
  keywords={Surveys;Machine learning algorithms;Databases;Computational modeling;Digital forensics;Machine learning;Documentation;digital forensics;investigation process modeling;behavior profiling;artificial intelligence;machine learning},
  doi={10.23919/SpliTech58164.2023.10193176},
  ISSN={},
  month={June},}@ARTICLE{4663859,
  author={Faro, Alberto and Giordano, Daniela and Maiorana, Francesco and Spampinato, Concetto},
  journal={IEEE Transactions on Information Technology in Biomedicine},
  title={Discovering Genes-Diseases Associations From Specialized Literature Using the Grid},
  year={2009},
  volume={13},
  number={4},
  pages={554-560},
  abstract={This paper proposes a novel method for text mining on the Grid, aimed at pointing out hidden relationships for hypothesis generation and suitable for semi-interactive querying. The method is based on unsupervised clustering and the outputs are visualized with contextual information. Grid implementation is crucial for feasibility. We demonstrate it with a mining run for discovering genes-diseases associations from bibliographic sources and annotated databases. The proposed methodology is in view of a Grid architecture specialized in bioinformatics mining tasks. Some performance considerations are provided.},
  keywords={Data mining;Text mining;Bioinformatics;Diseases;Grid computing;Algorithm design and analysis;Genomics;Databases;Mesh generation;Visualization;Genes-diseases association;Grid;knowledge discovery;text mining;unsupervised clustering},
  doi={10.1109/TITB.2008.2007755},
  ISSN={1558-0032},
  month={July},}@INPROCEEDINGS{10718368,
  author={Vijayalakshmi, A. and Aruna, T. N. and N, Divya. and Chakravarthy, V J and Seenivasan, M.},
  booktitle={2024 Third International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)},
  title={Text Classification on Tweets predicting the Mental Illness using Natural Language Processing and Machine Learning Classifiers},
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={In the 21st Century, people’s are investing additional occasions on social media reflecting their feelings by means of the languages(text). The modern inquire about is looking at whether people's conventional dialect gives a great flag for anticipating the event of mental ailment. Dialect tests (Tweets) were assembled from the social media site, with tweets utilized to portray different sorts of mental condition. As hypothesized, tweets taken from the client can be utilized to separate a assortment of mental sicknesses (ADHD, uneasiness, bipolar clutter, and sadness). At last, models prepared on tweets particular in expressions showing disorder-specific side effects, while models instructed to foresee mental states particular in words, showing the sorts of choices prescient of mental ailment. This paper points to appear an NLP spaCy and the machine learning classifiers to anticipate mental sickness on tweets by categorizing the content and gets the exactness of 99.25% in NLP - spaCy and 96.71% in ML Irregular Timberland & XGBoost Demonstrate.},
  keywords={Social networking (online);Text categorization;Machine learning;Natural language processing;Information and communication technology;Clutter;Tweets;Natural language processing;Text Classification;Spacy;Machine learning;ADHD;Anxiety;Bipolar;Depression},
  doi={10.1109/ICEEICT61591.2024.10718368},
  ISSN={},
  month={July},}@ARTICLE{10209186,
  author={Guetari, Ramzi and Kraiem, Naoufel},
  journal={IEEE Access},
  title={CoMod: An Abstractive Approach to Discourse Context Identification},
  year={2023},
  volume={11},
  number={},
  pages={82744-82770},
  abstract={Generative text summarization can condense large volumes of information into a concise summary. It helps users quickly grasp the main points of a text without having to read the entire document. Machine learning (ML) plays a pivotal role in this domain, offering significant advantages in information processing and comprehension. In this paper we present CoMod, an abstractive method for generating the context of a document, from its content and that of the referenced documents, if any. CoMod analyzes the intricate patterns and relationships within a document’s content, thereby extracting and inferring the underlying context. The context generation process involves using a word linearization process as well as a Markov model, specifically a Bigram model, to predict the likelihood of word sequences. The Markov model is trained on a corpus of text and used to generate coherent sentences based on the probabilities of transitioning from one word to another. Markov tables allows to adapt the generated context to a specific domain and can be built on the fly in CoMod. The approach was compared to other methods and demonstrated very encouraging capabilities by outperforming other approaches tested on the same datasets. It thus confirms the potential of generative methods in the field of automatic text summarization and their ability of leveraging the power of machine learning for context generation to revolutionize information management, boosting productivity, scalability and knowledge discovery in various domains.},
  keywords={Context modeling;Adaptation models;Metadata;Cultural differences;Underwater vehicles;Semantics;Markov processes;Context identification;generative models;machine learning;natural language processing;text summarizing;topic identification},
  doi={10.1109/ACCESS.2023.3302179},
  ISSN={2169-3536},
  month={},}@ARTICLE{9625858,
  author={Kaur, Avneet and Bhatia, Munish},
  journal={IEEE Transactions on Engineering Management},
  title={Scientometric Analysis of Smart Learning},
  year={2024},
  volume={71},
  number={},
  pages={400-413},
  abstract={Research and development frequently proceed based on words and phrases. New ideas are introduced to outline developments often with definitions. The word smart education was introduced to describe an advancement in technology-enhancing education. Moreover, the developments in information and communication technology have accelerated global efforts toward the idea of smart education. Over the past few years, the growing interest and popularity of smart education across countries have resulted in a significant number of research contributions. Henceforth, the significant of current research involves the comprehensive analysis of the academic structure and development, which can be assessed through quantitative methods. The current article provides a two-dimensional scientometric analysis of smart education literature from the context of educational and research areas. Specifically, it is based on broad bibliographic data obtained from web of science for the period 2010–2021. The current research presents detailed insights into the smart learning environment for publishing trends, citing trends, cooccurrence evaluation of keywords, and technical innovations. Moreover, it gives a comprehensive understanding of academic developments, and structure for future directives and collaborations in the area of smart learning and academic research.},
  keywords={Education;Bibliometrics;Performance evaluation;Market research;Tools;Databases;Cloud computing;Information and communication technologies (ICT);internet of things (IoT);smart education;VOSviewer;web of science (WoS)},
  doi={10.1109/TEM.2021.3124977},
  ISSN={1558-0040},
  month={},}@INPROCEEDINGS{10143604,
  author={Shi, Jingjing and Narasuman, Suthagar and Ning, Huichun and Yue, Fang},
  booktitle={2022 3rd International Conference on Information Science and Education (ICISE-IE)},
  title={Research Trend Analysis with SATI on Hybrid-learning in ESL/EFL Since COVID-19},
  year={2022},
  volume={},
  number={},
  pages={174-182},
  abstract={In the era of internet+, hybrid learning is bound to become a trend, while the Covid-19 pandemic acts as an accelerator of hybrid learning. This research selected 1773 articles from WOS and opted SATI as the major platform to do the bibliographic information processing. The results of word frequency analysis and co-word analysis were then interpreted to identify the current trend and hotspots of research in the fields of hybrid learning in ESL/EFL. The purpose is to seek more effective teaching mode reform and innovation in the future and make new technology really serve teaching.},
  keywords={COVID-19;Technological innovation;Information science;Pandemics;Education;Information processing;Market research;bibliographic;frequency analysis;co-word;cluster;interpretation},
  doi={10.1109/ICISE-IE58127.2022.00044},
  ISSN={},
  month={Nov},}@ARTICLE{9147044,
  author={Luo, Dongsheng and Ma, Shuai and Yan, Yaowei and Hu, Chunming and Zhang, Xiang and Huai, Jinpeng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={A Collective Approach to Scholar Name Disambiguation},
  year={2022},
  volume={34},
  number={5},
  pages={2020-2032},
  abstract={Scholar name disambiguation remains a hard and unsolved problem, which brings various troubles for bibliography data analytics. Most existing methods handle name disambiguation separately that tackles one name at a time, and neglect the fact that disambiguation of one name affects the others. Further, it is typically common that only limited information is available for bibliography data, e.g., only basic paper and citation information is available in DBLP. In this study, we propose a collective approach to name disambiguation, which takes the connection of different ambiguous names into consideration. We reformulate bibliography data as a heterogeneous multipartite network, which initially treats each author reference as a unique author entity, and disambiguation results of one name propagate to the others of the network. To further deal with the sparsity problem caused by limited available information, we also introduce word-word and venue-venue similarities, and we finally measure author similarities by assembling similarities from four perspectives. Using real-life data, we experimentally demonstrate that our approach is both effective and efficient.},
  keywords={Bibliographies;Wide area networks;Search problems;Merging;Data analysis;Indexes;Data integrity;Name disambiguation;collective clustering;information network},
  doi={10.1109/TKDE.2020.3011674},
  ISSN={1558-2191},
  month={May},}@INPROCEEDINGS{8554655,
  author={Das, R Arun and Afsal, PM and Thushara, MG},
  booktitle={2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
  title={Tagging of Research Publications based on Author and Year Extraction},
  year={2018},
  volume={},
  number={},
  pages={892-896},
  abstract={Tagging provides a way to give a token of link to research publications which facilitates recommendation and search process. This paper proposes an auto-tagging methodology using Conditional Random Fields (CRF) for the precise extraction of authors and publication year from the research publications. This proposed system use auto-tagging methodology for research publication recommendations. This mechanism suggests a research publication based on the corresponding tags they have created. The system also generates various dynamic reports for graphical analysis of projects carried out in each research domain. These statistics benefits help users to get an overview of the trends of research works done over the past few years and ongoing researches in each department. The proposed system helps the students, faculty and other academicians to get involved in ongoing researches and also to obtain ideas in their respective research domain. It makes aware staffs and students with new inclinations in research publications.},
  keywords={Tagging;Data mining;Feature extraction;Computer science;Training;Hidden Markov models;Market research;tagging;extraction;training;domain;classification},
  doi={10.1109/ICACCI.2018.8554655},
  ISSN={},
  month={Sep.},}@ARTICLE{9357335,
  author={Muzaffar, Abdul Wahab and Tahir, Muhammad and Anwar, Muhammad Waseem and Chaudry, Qaiser and Mir, Shamaila Rasheed and Rasheed, Yawar},
  journal={IEEE Access},
  title={A Systematic Review of Online Exams Solutions in E-Learning: Techniques, Tools, and Global Adoption},
  year={2021},
  volume={9},
  number={},
  pages={32689-32712},
  abstract={E-learning in higher education is exponentially increased during the past decade due to its inevitable benefits in critical situations like natural disasters (e.g. COVID-19 pandemic etc.) and war circumstances. The reliable, fair, and seamless execution of online exams in E-learning is highly significant. Particularly, online exams are conducted on E-learning platforms without the physical presence of students and instructors at the same place. This poses several issues like integrity and security during online exams. To address such issues, researchers frequently proposed different techniques and tools. However, a study summarizing and analyzing latest developments, particularly in the area of online examination, is hard to find in the literature. In this article, a Systematic Literature Review (SLR) of online examination is performed to select and analyze 53 studies published during the last five years (i.e. Jan 2016 to July 2020). Subsequently, five leading online exams features targeted in the selected studies are identified. Moreover, underlying development approaches for the implementation of online exams solutions are explored. Furthermore, 16 important techniques / algorithms and 11 datasets are presented. In addition to this, 21 online exams tools proposed in the selected studies are identified. Additionally, 25 leading existing tools used in the selected studies are also presented. Finally, the participation of countries in online exam research is investigated. Key factors for the global adoption of online exams are identified and compared with major online exams features. This facilitates the selection of right online exam system for a particular country on the basis of existing E-learning infrastructure and overall cost. To conclude, the findings of this article provide a solid platform for the researchers and practitioners of the domain to select appropriate features along with underlying development approaches, tools, and techniques for the implementation of a particular online exams solution as per given requirements.},
  keywords={Electronic learning;Tools;Biometrics (access control);Software;Systematics;Security;Reliability;Online examination;online proctoring;systematic literature review;e-learning;biometric attendance},
  doi={10.1109/ACCESS.2021.3060192},
  ISSN={2169-3536},
  month={},}@ARTICLE{8718661,
  author={Reinel, Tabares-Soto and Raúl, Ramos-Pollán and Gustavo, Isaza},
  journal={IEEE Access},
  title={Deep Learning Applied to Steganalysis of Digital Images: A Systematic Review},
  year={2019},
  volume={7},
  number={},
  pages={68970-68990},
  abstract={Steganography consists of hiding messages inside some object known as a carrier in order to establish a covert communication channel so that the act of communication itself goes unnoticed by observers who have access to that channel. The steganalysis is dedicated to the detection of hidden messages using steganography; these messages can be implicit in different types of media, such as digital images, video files, audio files or plain text. Traditionally, steganalysis has been divided into two separate stages, the first stage consists of manual extraction of sophisticated features and the second stage is classification using Ensemble Classifiers or Support Vector Machines. In recent years, the development of Deep Learning has made it possible to unify and automate the two traditional stages into an end to end approach with promising results. This paper shows the evolution of steganalysis in recent years using the Deep Learning techniques. The results of these techniques have surpassed those obtained with conventional methods - Rich Models with Ensemble Classifiers - both in the spatial and frequency (JPEG) domains. Since 2014, researchers have used The Convolutional Neural Networks to solve this problem generating diverse architectures and strategies to improve the detection percentages of steganographic images on the last generation algorithms (WOW, S-UNIWARD, HUGO, J-UNIWARD, among others). The Deep Learning, being applied to steganalysis, is now in the process of construction and results so far are encouraging for researchers that are interested in the topic.},
  keywords={Feature extraction;Transform coding;Deep learning;Databases;Frequency-domain analysis;Payloads;Manuals;Convolutional neural network;deep learning;steganalysis;steganography},
  doi={10.1109/ACCESS.2019.2918086},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10418736,
  author={Sáez, Paula and Morales, Jenny and Silva-Aravena, Fabián},
  booktitle={2023 IEEE CHILEAN Conference on Electrical, Electronics Engineering, Information and Communication Technologies (CHILECON)},
  title={Digital Transformation in Organizations: Implications for the Workforce},
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Digital transformation has caused profound changes in how organizations operate, interact, and adapt to the current business environment to maintain and/or increase their productivity and competitiveness. One of the main effects of continuous technological dynamism has been in the labor market, particularly in the labor force, where organizations and individuals must invest in new skills and knowledge to avoid obsolescence. The coronavirus (COVID-19) pandemic accelerated the technology adoption process, causing organizations to opt to automate repetitive tasks, driving demand for new skills and fostering flexibility at work. The contribution presented in this work focuses on a literature review that addresses the labor market and digital transformation to determine how digital change in organizations has impacted the workforce. A bibliographic review protocol at an exploratory level was followed to meet this objective, consulting the Scopus database. In said review, it was found that, of 22 related studies, 15 of them account, directly or indirectly, for the need to acquire new skills to face the challenges imposed by technological changes implemented in organizations on the workforce, this being a transversal requirement in different areas, such as real estate, educational, entrepreneurial, engineering, medical, among others. In future work, the study will be complemented with a systematic literature review to explore how higher education institutions prepare the workforce to respond to the new demands regarding skills required in the work context.},
  keywords={COVID-19;Systematics;Bibliographies;Force;Organizations;Aging;Task analysis;labor market;working market;labor force;working force;digital transformation;technology;new employment;new skills},
  doi={10.1109/CHILECON60335.2023.10418736},
  ISSN={2832-1537},
  month={Dec},}@INPROCEEDINGS{10085221,
  author={Bonkra, Anupam and Bhatt, Pramod Kumar and Kaur, Amandeep and Kamboj, Sushil},
  booktitle={2023 International Conference on Artificial Intelligence and Smart Communication (AISC)},
  title={Scientific Landscape and the Road Ahead for Deep Learning: Apple Leaves Disease Detection},
  year={2023},
  volume={},
  number={},
  pages={869-873},
  abstract={Agriculture industry experiences a significant loss in crop productivity as a result of unpredictably occurring environmental factors such hailstorms, draughts, fog, and untimely rains. Leaves infections are one of the main causes. Thus, it becomes imperative to identify plant leaf diseases in advance in order to prevent them and reduce crop output loss. A bibliometric analysis of deep learning-based apple leaf disease diagnoses is summarized in the publication. The research concentrates on 139 scientific publications from various periodicals, including journals, articles, and book chapters. Such materials were extracted from the Scopus database after it was searched for keywords relating to the taxonomy of plant diseases and deep learning. The papers are examined between the years of 2017 and 2022. Tools like VOSviewer, an open source programme, were used for the investigation.},
  keywords={Deep learning;Productivity;Plant diseases;Rain;Databases;Roads;Bibliometrics;Apple leaves;Deep convolution neural network;Bibliometric;Deep Learning;Scopus;Cluster},
  doi={10.1109/AISC56616.2023.10085221},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10084049,
  author={Nair, Sreerengan V R and Saha, Saibal Kumar},
  booktitle={2023 International Conference on Intelligent Systems, Advanced Computing and Communication (ISACC)},
  title={Advancements in Cyber Security and Information Systems in Healthcare from 2004 to 2022: A Bibliometric Analysis},
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The main goals of the multifaceted healthcare system were to prevent, identify, and treat illnesses or conditions that affect human health. As the usage of IT in healthcare increased, the complexities in managing the IT infrastructure also increase, emphasizing the need of robust cyber security systems. The study aims to emphasize the advancements made in cyber security and information systems in healthcare, based on bibliometric analysis. 5,487 document's metadata was obtained from Scopus and data was analyzed using Vos Viewer. Ranking of articles was done with average yearly citations of the publications. Bibliometric analysis was performed based on “bibliographic coupling of countries”, “co-occurrence of all keywords”, “author-based co-authorship”, and “term co-occurrence based on text data”. It was found that United States had the maximum publications (1337). “Department of Information Systems and Cyber Security, The University of Texas at San Antonio, United States” is the most influential organization with 159 publications. IEEE Access is the most preferred platform for publication related to cyber security and information systems in healthcare (231 publications). 167 publications have received more than 100 citations. Choo K. K.R. is the most influential author with 185 publications.},
  keywords={Deep learning;Smart cities;Computational modeling;Bibliometrics;Medical services;Organizations;Time factors;Cyber Security;Information Systems;Healthcare;Bibliometric Analysis;Citation Analysis},
  doi={10.1109/ISACC56298.2023.10084049},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{9144077,
  author={Mulunda, Christine and Waiganjo, Peter and Muchemi, Lawrence},
  booktitle={2020 IST-Africa Conference (IST-Africa)},
  title={Towards Implementation of an Information Dissemination Tool for Health Publications: Case of a Developing Country},
  year={2020},
  volume={},
  number={},
  pages={1-11},
  abstract={Health related publications have been growing at a considerable rate over the years. They are archived at public or private repositories for research or decisionmaking. In developing countries there is an increase in the need for technological infrastructure and funding towards research that enables the voluminous unstructured data to be effectively identified, stored, analysed and visualized to enable prompt decision making. Analysing data in real-time will assist knowledge seekers and researchers in timely access hence quick approach to solutions. We propose a web based, low-cost and user-friendly health information dissemination tool based on machine learning algorithms that analyses full-text publications sequentially and cluster related documents for ease of access. Information retrieval aspect of the model is enhanced through use of a semi-supervised approach that optimises topic selections during search operations. As future works, we propose to scale-up the prototype for bulky data processing and apply it to big data environments.},
  keywords={Tools;Information retrieval;Data visualization;Real-time systems;Prototypes;Semantics;Analytical models;health;information dissemination tool;publications;real-time analysis;information retrieval},
  doi={},
  ISSN={2576-8581},
  month={May},}@INPROCEEDINGS{6805987,
  author={Boudhief, Asma and Maraoui, Mohsen and Zrigui, Mounir},
  booktitle={2014 6th International Conference on Computer Science and Information Technology (CSIT)},
  title={Elaboration of a model for an indexed base for teaching Arabic language to disabled people},
  year={2014},
  volume={},
  number={},
  pages={110-116},
  abstract={In this paper we carried out a bibliographic research concerning distance learning, pedagogical indexing and integration of accessibility in e-Learning platforms. Then, to achieve the goals we have chosen “IMS AccessForALL” as standard to realize our model. The model that we realized is consistent with the standard followed. We also taken into consideration the guidelines of the WAI (Web accessibility Initiative) presented in WCAG. Finally, for the step of indexing, we started with the creation of the XML schema, then the indexation of resources in the true sense of the term.},
  keywords={Standards;Electronic learning;Least squares approximations;Computers;Training;Computer aided instruction;E-learning;pedagogical indexation;complexity of the Arabic language;standards of description pedagogical resources;Accessibility;the standard IMS;specification Access for All AFA},
  doi={10.1109/CSIT.2014.6805987},
  ISSN={},
  month={March},}@INPROCEEDINGS{8903945,
  author={Kayani, Mehwish and Hassan, Saeed-Ul and Aljohani, Naif Radi and Dancey, Darren and Liu, Leo and Nawaz, Raheel},
  booktitle={2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4)},
  title={Towards Interdisciplinary Research: A Bibliometric View of Information Communication Technology for Development in Different Disciplines},
  year={2019},
  volume={},
  number={},
  pages={185-192},
  abstract={We present a novel bibliometric view to create a taxonomy of the interdisciplinary field of Information Communication Technology for Development (ICTD or ICT4D), using scientific documents published in the fields related to information communication technologies that were indexed in the Scopus database from 2001 to 2015. Our research approach utilizes a set of relevant terms and venues (journals and conferences) to procure publications on ICTD topics, and further to analyze them statistically to identify emerging or emerged sub-disciplines. The sub-disciplines prominently include Economic Growth/E-Government/Political Economy, Cloud Enterprises/Entrepreneurship/ICT Innovation, and Smart Energy/Smart Grid. We identify the relevance and uniqueness of all ICTD topics, generated by the algorithm, by employing well-known indices of the coherence coefficient and Kullback-Leibler divergence, indicating the qualifications of a given topic as an established sub-discipline of ICTD. This paper attempts to discuss the interdisciplinary nature of the research field ICTD and recommends prospective directions for new subdisciplines based on the text analysis.},
  keywords={Clustering algorithms;Bibliographies;Feature extraction;Partitioning algorithms;Bibliometrics;Databases;Communications technology;Bibliometrics;Entrepreneurship;ICT Innovation;ICTD;ICT4D;Inter-disciplinary Research;Political Economy;Smart Energy;Semantic Analysis;Text Analysis;Topic Modeling},
  doi={10.1109/WorldS4.2019.8903945},
  ISSN={},
  month={July},}@INPROCEEDINGS{9092289,
  author={Chand, Dipesh and Ogul, Hasan},
  booktitle={2020 3rd International Conference on Information and Computer Technologies (ICICT)},
  title={Content-Based Search in Lecture Video: A Systematic Literature Review},
  year={2020},
  volume={},
  number={},
  pages={169-176},
  abstract={With the advancement of science and technology, everything is directly or indirectly related to it. In the same way, the learning method is also changing. Nowadays e-learning is one of the most popular methods of learning where lecture video plays a great role. Although lots of work have been done in the area of lecture video limited work have been done using content-based searching. This paper aims to obtain an overview of the topic of lecture video content-based search. To achieve that, a protocol for systemic literature review has been adapted and subsequently implemented from literature review protocol standards. The objective is to assess the existing inputs and outputs in the literature to create the best lecture video content-based search system. This paper reveals benefits, obstacles and challenges, and data source and contents which can be extracted and utilized for content-based search.},
  keywords={Bibliographies;Systematics;Optical character recognition software;Search problems;Databases;Data mining;Feature extraction;Systematic literature review;Lecture video;Content-based search},
  doi={10.1109/ICICT50521.2020.00034},
  ISSN={},
  month={March},}@ARTICLE{1010069,
  author={Bregzis, R. and Gotlieb, C. and Moore, C.},
  journal={IEEE Annals of the History of Computing},
  title={The beginning of automation in the University of Toronto Library, 1963-1972},
  year={2002},
  volume={24},
  number={2},
  pages={50-70},
  abstract={In 1962, the Province of Ontario established five new universities and asked the University of Toronto Library (UTL) to help build libraries for them, which it did. The main task was to determine a record format, coordinated with that developed later for the Library of Congress's MARC project. Eventually, UTL established the University of Toronto Library Automation Systems. The early decisions have enabled the UTL to develop electronic indexes and full-text document distribution systems at a rate that has kept it among the world's leading libraries.},
  keywords={Automation;Libraries;New products catalog;Computer science;Processor scheduling;Production systems;Books;Petroleum;Refining},
  doi={10.1109/MAHC.2002.1010069},
  ISSN={1934-1547},
  month={April},}@ARTICLE{5703067,
  author={Magnisalis, Ioannis and Demetriadis, Stavros and Karakostas, Anastasios},
  journal={IEEE Transactions on Learning Technologies},
  title={Adaptive and Intelligent Systems for Collaborative Learning Support: A Review of the Field},
  year={2011},
  volume={4},
  number={1},
  pages={5-20},
  abstract={This study critically reviews the recently published scientific literature on the design and impact of adaptive and intelligent systems for collaborative learning support (AICLS) systems. The focus is threefold: 1) analyze critical design issues of AICLS systems and organize them under a unifying classification scheme, 2) present research evidence on the impact of these systems on student learning, and 3) identify current trends and open research questions in the field. After systematically searching online bibliographic databases, 105 articles were included in the review with 70 of them reporting concrete evaluation data on the learning impact of AICLS systems. Systems design analysis led us to propose a classification scheme with five dimensions: pedagogical objective, target of adaptation, modeling, technology, and design space. The reviewed articles indicate that AICLS systems increasingly introduce Artificial Intelligence and Web 2.0 techniques to support pretask interventions, in-task peer interactions, and learning domain-specific activities. Findings also suggest that AICLS systems may improve both learners' domain knowledge and collaboration skills. However, these benefits are subject to the learning design and the capability of AICLS to adapt and intervene in an unobtrusive way. Finally, providing peer interaction support seems to motivate students and improve collaboration and learning.},
  keywords={Adaptive systems;Collaborative work;Adaptation model;Collaboration;Intelligent systems;Computational modeling;Adaptive collaborative learning support;intelligent support systems;adaptive hypermedia;collaborative learning.},
  doi={10.1109/TLT.2011.2},
  ISSN={1939-1382},
  month={Jan},}@ARTICLE{9737031,
  author={Roman, Muhammad and Shahid, Abdul and Khan, Shafiullah and Yu, Lisu and Asif, Muhammad and Ghadi, Yazeed Yasin},
  journal={IEEE Access},
  title={Investigating Maps of Science Using Contextual Proximity of Citations Based on Deep Contextualized Word Representation},
  year={2022},
  volume={10},
  number={},
  pages={31397-31419},
  abstract={The citation intent extraction and classification has long been studied as it is a good measure of relevancy. Different approaches have classified the citations into different classes; including weak and strong, positive and negative, important and unimportant. Others have gone further from binary classification to multi-classes, including extension, use, background, or comparison. Researchers have utilized various elements of the information, including both meta and contents of the paper. The actual context of any referred article lies within the citation context where a paper is referred. Various attempts have been made to study the citation context to capture the citation intent, but very few have encoded the words to their contextual representations. For automated classification, we need to train deep learning models, which take the citation context as input and provides the reason for citing a paper. Deep neural models work on numeric data, and therefore, we must convert the text information to its numeric representation. Natural languages are much complex than computer languages. Computer languages have a pre-defined fixed syntax where each word has a unique meaning. In contrast, every word in natural language may have a different meaning and may well be understood by understanding the position, previous discussion, and neighboring words. The extra information provides the context of a word within a sentence. We have, therefore, used contextual word representation, which is trained through deep neural networks. Deep models require massive data for generalizing the model, however, the existing state-of-the-art datasets don’t provide much information for the training models to get generalized. Therefore, we have developed our own scholarly dataset, Citation Context Dataset with Intent (C2D-I), an extension of the C2D dataset. We used a transformers based model for capturing the contextual representation of words. Our proposed method outperformed the existing benchmark methods with F1 score of 89%.},
  keywords={Metadata;Deep learning;Context modeling;Transformers;Natural languages;Classification;Numerical models;Citation analysis;Citation intent classification;citation reason;citation context;transformers model;research paper similarity},
  doi={10.1109/ACCESS.2022.3159980},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8750698,
  author={Bodó, Zalán},
  booktitle={2018 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)},
  title={A CiteSeerX-Based Dataset for Record Linkage and Metadata Extraction},
  year={2018},
  volume={},
  number={},
  pages={230-236},
  abstract={Data cleaning constitutes an important problem in information science. Collecting data about the same entities from multiple sources or following distinct methodologies might result in slightly different, inconsistent data. The objective of data cleaning is to produce a fused version combining the differing data, resulting in a cleaner dataset. In this paper we collect document metadata records from CiteSeerX and build a supervised record linker to Crossref. The supervised method is trained using a manually linked dataset containing 512 verified DOIs-to our knowledge, up to now being the largest such dataset for bibliographic record linkage. We experiment using different supervised learning methods, and also prove experimentally that the accuracy of the attached metadata records can improve the performance of automatic metadata extraction systems.},
  keywords={Metadata;Couplings;Data mining;Supervised learning;Portable document format;Cleaning;Libraries;record linkage, data cleaning, metadata extraction},
  doi={10.1109/SYNASC.2018.00044},
  ISSN={},
  month={Sep.},}@ARTICLE{7442097,
  author={West, Jevin D. and Wesley-Smith, Ian and Bergstrom, Carl T.},
  journal={IEEE Transactions on Big Data},
  title={A Recommendation System Based on Hierarchical Clustering of an Article-Level Citation Network},
  year={2016},
  volume={2},
  number={2},
  pages={113-123},
  abstract={The scholarly literature is expanding at a rate that necessitates intelligent algorithms for search and navigation.For the most part, the problem of delivering scholarly articles has been solved. If one knows the title of an article, locating it requires little effort and, paywalls permitting, acquiring a digital copy has become trivial. However, the navigational aspect of scientific search - finding relevant, influential articles that one does not know exist - is in its early development. In this paper, we introduce EigenfactorRecommends - a citation-based method for improving scholarly navigation. The algorithm uses the hierarchical structure of scientific knowledge, making possible multiple scales of relevance for different users. We implement the method and generate more than 300 million recommendations from more than 35 million articles from various bibliographic databases including the AMiner dataset. We find little overlap with co-citation, another well-known citation recommender, which indicates potential complementarity. In an online A-B comparison using SSRN, we find that our approach performs as well as co-citation, but this new approach offers much larger recommendation coverage. We make the code and recommendations freely available at babel.eigenfactor.organd provide an API for others to use for implementing and comparing the recommendations on their own platforms.},
  keywords={Big data;Navigation;Libraries;Couplings;Search engines;Portals;Scholarships;scholarly recommendation;citation networks;hierarchical clustering;community detection;big scholarly data},
  doi={10.1109/TBDATA.2016.2541167},
  ISSN={2332-7790},
  month={June},}@ARTICLE{10412041,
  author={Nugroho, Heru and Surendro, Kridanto},
  journal={IEEE Access},
  title={A Comprehensive Bibliometric Analysis of Missing Value Imputation},
  year={2024},
  volume={12},
  number={},
  pages={14819-14846},
  abstract={Data quality plays a crucial role in tasks, such as enhancing the accuracy of data analytics and avoiding the accumulation of redundant data. One of the significant challenges in data quality is dealing with missing data, which has been extensively explored by the scholarly community and has resulted in a significant increase in related publications. It is important to recognize that the landscape of missing data in computer science offers numerous opportunities for further research. However, upon closer examination of existing studies, it becomes evident that many have not fully utilized bibliometric analysis tools and software for comprehensive literature reviews. Therefore, this study aims to explore the essential characteristics, trends, and prevailing themes in the field of missing data imputation. Through a thorough bibliometric analysis, this study demonstrated the evolution of knowledge and key focal points in the field of missing data imputation. The analysis consisted of 352 journal papers in computer science published between 2012 and 2023, all centered on missing data imputation. Among these publications, “IEEE Access” has become a highly respected source. To systematically explore various aspects of missing data imputation, a conceptual framework was used to uncover potential research directions and underlying themes. Ultimately, a thematic map serves as a valuable tool for providing a comprehensive understanding, categorizing significant concepts into basic or overarching, developing, or declining, central, highly developed, and isolated themes. These overarching and underlying themes offer valuable insights and pave the way for prospective directions and critical areas of study.},
  keywords={Bibliometrics;Market research;Computer science;Machine learning;Databases;Bibliographies;Systematics;Data integrity;Missing data;imputation;literature review;bibliometric analysis},
  doi={10.1109/ACCESS.2024.3357533},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9651834,
  author={Pepper, Joel and Greenberg, Jane and Bakiş, Yasin and Wang, Xiaojun and Bart, Henry and Breen, David},
  booktitle={2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
  title={Automatic Metadata Generation for Fish Specimen Image Collections},
  year={2021},
  volume={},
  number={},
  pages={31-40},
  abstract={Metadata are key descriptors of research data, particularly for researchers seeking to apply machine learning (ML) to the vast collections of digitized specimens. Unfortunately, the available metadata is often sparse and, at times, erroneous. Additionally, it is prohibitively expensive to address these limitations through traditional, manual means. This paper reports on research that applies machine-driven approaches to analyzing digitized fish images and extracting various important features from them. The digitized fish specimens are being analyzed as part of the Biology Guided Neural Networks (BGNN) initiative, which is developing a novel class of artificial neural networks using phylogenies and anatomy ontologies. Automatically generated metadata is crucial for identifying the high-quality images needed for the neural network's predictive analytics. Methods that combine ML and image informatics techniques allow us to rapidly enrich the existing metadata associated with the 7,244 images from the Illinois Natural History Survey (INHS) used in our study. Results show we can accurately generate many key metadata properties relevant to the BGNN project, as well as general image quality metrics (e.g. brightness and contrast). Results also show that we can accurately generate bounding boxes and segmentation masks for fish, which are needed for subsequent machine learning analyses. The automatic process outperforms humans in terms of time and accuracy, and provides a novel solution for leveraging digitized specimens in ML. This research demonstrates the ability of computational methods to enhance the digital library services associated with the tens of thousands of digitized specimens stored in open-access repositories worldwide.},
  keywords={Measurement;Machine learning;Metadata;Ontologies;Fish;Feature extraction;Libraries;bioinformatics;metadata;image analysis;applied machine learning},
  doi={10.1109/JCDL52503.2021.00015},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{8572519,
  author={Rui, Ji and Yu, Shiming and Yan, Hongwei and Ding, Suren and Wang, Ben and Zong, Hui and Zhu, Quanyin},
  booktitle={2018 17th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)},
  title={Visualization and Forecast Analysis of Science and Technology Intelligence Based on Knowledge Graph},
  year={2018},
  volume={},
  number={},
  pages={44-47},
  abstract={it is difficult for scientific researchers to extract strutted knowledge from massive data and clarify the process and complex connections between it. The proposed system collects scientific and technological intelligence via data mining. Then we conduct information extraction, knowledge fusion, and knowledge processing to build Knowledge Graphs of scientific and technological intelligence. Finally, we can realize visualization of scientific and technological intelligence. The knowledge base constructed by the system can help researchers more intuitively understand the history, status, and future of discipline development. It can help researchers better understand the classification of disciplines and deepen their understanding of knowledge. Also, the system can conduct empirical research through citation analysis, bibliographic coupling analysis and other bibliometric methods. Via trend extrapolation, Tri-training and other algorithms, we can predict the trend of the frontier.},
  keywords={Ontologies;Resource description framework;Databases;Data visualization;Knowledge based systems;Data mining;Knowledge Graph;Science and Technology Intelligence;Visualization;Data mining;Hotspot Prediction},
  doi={10.1109/DCABES.2018.00021},
  ISSN={2473-3636},
  month={Oct},}@INPROCEEDINGS{4233734,
  author={Zhu, Dengya and Dreher, Heinz},
  booktitle={2007 Inaugural IEEE-IES Digital EcoSystems and Technologies Conference},
  title={An Integrating Text Retrieval Framework for Digital Ecosystems Paradigm},
  year={2007},
  volume={},
  number={},
  pages={367-372},
  abstract={The purpose of the research is to provide effective information retrieval services for digital 'organisms' in a digital ecosystem by leveraging the power of Web searching technology. A novel integrating digital ecosystem search framework (a new digital organism) is proposed which employs the Web search technology and traditional database searching techniques to provide economic organisms with comprehensive, dynamic, and organization-oriented information retrieval ranging from the Internet to personal (semantic) desktop.},
  keywords={Ecosystems;Information retrieval;Organisms;Ontologies;Search engines;Electronic mail;Web search;Databases;Thesauri;History;information retrieval;self-organizing search;categorization;crawler},
  doi={10.1109/DEST.2007.372000},
  ISSN={2150-4946},
  month={Feb},}@ARTICLE{9754584,
  author={Angioni, Simone and Salatino, Angelo and Osborne, Francesco and Recupero, Diego Reforgiato and Motta, Enrico},
  journal={IEEE Access},
  title={The AIDA Dashboard: A Web Application for Assessing and Comparing Scientific Conferences},
  year={2022},
  volume={10},
  number={},
  pages={39471-39486},
  abstract={Scientific conferences are essential for developing active research communities, promoting the cross-pollination of ideas and technologies, bridging between academia and industry, and disseminating new findings. Analyzing and monitoring scientific conferences is thus crucial for all users who need to take informed decisions in this space. However, scholarly search engines and bibliometric applications only provide a limited set of analytics for assessing research conferences, preventing us from performing a comprehensive analysis of these events. In this paper, we introduce the AIDA Dashboard, a novel web application, developed in collaboration with Springer Nature, for analyzing and comparing scientific conferences. This tool introduces three major new features: 1) it enables users to easily compare conferences within specific fields (e.g., Digital Libraries) and time-frames (e.g., the last five years); 2) it characterises conferences according to a 14K research topics from the Computer Science Ontology (CSO); and 3) it provides several functionalities for assessing the involvement of commercial organizations, including the ability to characterize industrial contributions according to 66 industrial sectors (e.g., automotive, financial, energy, electronics) from the Industrial Sectors Ontology (INDUSO). We evaluated the AIDA Dashboard by performing both a quantitative evaluation and a user study, obtaining excellent results in terms of quality of the analytics and usability.},
  keywords={Ontologies;Computer science;Metadata;Bibliographies;Knowledge engineering;Collaboration;Science - general;Scholarly data;knowledge graph;conference analytics;bibliographic data;scholarly ontologies;science of science},
  doi={10.1109/ACCESS.2022.3166256},
  ISSN={2169-3536},
  month={},}@ARTICLE{10177729,
  author={Abadade, Youssef and Temouden, Anas and Bamoumen, Hatim and Benamar, Nabil and Chtouki, Yousra and Hafid, Abdelhakim Senhaji},
  journal={IEEE Access},
  title={A Comprehensive Survey on TinyML},
  year={2023},
  volume={11},
  number={},
  pages={96892-96922},
  abstract={Recent spectacular progress in computational technologies has led to an unprecedented boom in the field of Artificial Intelligence (AI). AI is now used in a plethora of research areas and has demonstrated its capability to bring new approaches and solutions to various research problems. However, the extensive computation required to train AI algorithms comes with a cost. Driven by the need to reduce the energy consumption, the carbon footprint and the cost of computers running machine learning algorithms, TinyML is nowadays considered as a promising AI alternative focusing on technologies and applications for extremely low-profile devices. This paper presents the results of a literature survey of all TinyML applications and related research efforts. Our survey builds a taxonomy of TinyML techniques that have been used so far to bring new solutions to various domains, such as healthcare, smart farming, environment, and anomaly detection. Finally, this survey highlights the remaining challenges and points out possible future research directions. We anticipate that this survey will motivate further discussions on the various fields of applications of TinyML and the synergy of resource-constrained devices and edge intelligence.},
  keywords={Surveys;Medical services;Internet of Things;Hardware;Costs;Smart agriculture;Anomaly detection;TinyML;embedded machine learning;deep learning;edge intelligence},
  doi={10.1109/ACCESS.2023.3294111},
  ISSN={2169-3536},
  month={},}@ARTICLE{7230259,
  author={Zheng, Yu},
  journal={IEEE Transactions on Big Data},
  title={Methodologies for Cross-Domain Data Fusion: An Overview},
  year={2015},
  volume={1},
  number={1},
  pages={16-34},
  abstract={Traditional data mining usually deals with data from a single domain. In the big data era, we face a diversity of datasets from different sources in different domains. These datasets consist of multiple modalities, each of which has a different representation, distribution, scale, and density. How to unlock the power of knowledge from multiple disparate (but potentially connected) datasets is paramount in big data research, essentially distinguishing big data from traditional data mining tasks. This calls for advanced techniques that can fuse knowledge from various datasets organically in a machine learning and data mining task. This paper summarizes the data fusion methodologies, classifying them into three categories: stage-based, feature level-based, and semantic meaning-based data fusion methods. The last category of data fusion methods is further divided into four groups: multi-view learning-based, similarity-based, probabilistic dependency-based, and transfer learning-based methods. These methods focus on knowledge fusion rather than schema mapping and data merging, significantly distinguishing between cross-domain data fusion and traditional data fusion studied in the database community. This paper does not only introduce high-level principles of each category of methods, but also give examples in which these techniques are used to handle real big data problems. In addition, this paper positions existing works in a framework, exploring the relationship and difference between different data fusion methods. This paper will help a wide range of communities find a solution for data fusion in big data projects.},
  keywords={Data integration;Big data;Data mining;Feature extraction;Roads;Semantics;Trajectory;Big data;cross-domain data mining;data fusion;multi-modality data representation;deep neural networks;multi-view learning;matrix factorization;probabilistic graphical models;transfer learning;urban computing;Big Data;cross-domain data mining;data fusion;multi-modality data representation;deep neural networks;multi-view learning;matrix factorization;probabilistic graphical models;transfer learning;urban computing},
  doi={10.1109/TBDATA.2015.2465959},
  ISSN={2332-7790},
  month={March},}@ARTICLE{9667164,
  author={Abduljabbar, Rusul L. and Dia, Hussein},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  title={A Bibliometric Overview of IEEE Transactions on Intelligent Transportation Systems (2000–2021)},
  year={2022},
  volume={23},
  number={9},
  pages={14066-14087},
  abstract={The IEEE Transactions on Intelligent Transport Systems was founded in 2000 to enhance the sharing of international research on theoretical and practical technology developments in the ITS field. Since then, it has become a leading journal in the field and has attracted a high caliber of multi-disciplinary authors and publications. In recognition of twenty plus years of contributions to the field, this paper analyses the evolution of the journal over its lifetime for the period 2000–2021. A bibliometric analysis is conducted on 3,428 peer-reviewed publications (articles and reviews) using data collected from Core Collection Database of Web of Science. The paper identifies the most influential and cited articles and their impacts on the development of research in the ITS field. The analysis also includes detailed information on top leading authors, their organizations and countries where the research was funded and developed. The analysis shows how the growing interest and diversity of transport technology topics has led to an increase in the number and quality of publications in journal over the past twenty plus years. A visualization of bibliographic coupling, co-authorship and keywords analysis is also presented using the VOSviewer software leading to insightful findings regarding the journal’s impact and standing in this field of research.},
  keywords={Bibliometrics;Organizations;Databases;Couplings;Systematics;Statistical analysis;Software;IEEE Transactions on ITS;Web of Science;VOSviewer;systematic literature review;co-authorship analysis;bibliometric networks},
  doi={10.1109/TITS.2021.3136215},
  ISSN={1558-0016},
  month={Sep.},}@INPROCEEDINGS{9696631,
  author={Muppidi, Satish},
  booktitle={2021 Innovations in Power and Advanced Computing Technologies (i-PACT)},
  title={Ranking Authors in Citation Networks with Automated Keyword Extraction using Word Embeddings},
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={The main objective of this present article is to propose an efficient way to evaluate and rank the researchers and academic authors by using citation network dataset. To rank the authors in the citation network to predict the prominent authors in the research domain for making ease for authors for identifying the predominant authors in their area of research work. Popular independent metrics like h-index, number of citations, and so on is not very reliable when it comes to ranking authors. Author come up with a new method called Citation enhanced Ranking of Authors (CeRA) to rank authors. The proposed approach CeRA approach utilizes content-related similarity between abstracts that extract the keywords by using the word2vec model. K-means clustering algorithm is applied to extract authors clusters based on domains. Finally, specific authors rank is retrieved using the page rank of the author&#x0027;s node in a cluster.},
  keywords={Measurement;Knowledge engineering;Computers;Technological innovation;Semantics;Clustering algorithms;Data mining;Citation Networks;CeRA;Page Rank;Word2Vec;K-means;Word Embeddings;Doc2Vec},
  doi={10.1109/i-PACT52855.2021.9696631},
  ISSN={},
  month={Nov},}@ARTICLE{10413347,
  author={},
  journal={IEEE P3168/D3, August 2023},
  title={IEEE Approved Draft Standard for Robustness Evaluation Test Methods for a Natural Language Processing Service that uses Machine Learning},
  year={2024},
  volume={},
  number={},
  pages={1-27},
  abstract={The natural language processing (NLP) services using machine learning have rich applications in solving various tasks and have been widely deployed and used, usually accessible by application programming interface (API) calls. The robustness of the NLP services is challenged by various well-known general corruptions and adversarial attacks. Inadvertent or random deletion, addition, or repetition of characters or words are examples of general corruptions. Adversarial characters, words, or sentence samples are generated by adversarial attacks, causing the models underpinning the NLP services to produce incorrect results. A method for quantitatively evaluating the robustness the NLP services is proposed by this standard. Under the method, different cases the evaluation needs to perform against are specified. Robustness metrics and their calculation are defined. With the standard, understanding of the robustness of the services can be developed by the service stakeholders including the service developer, service providers, and service users. The evaluation can be performed during various phases in the life cycle of the NLP services, the testing phase, in the validation phase, after deployment, and so forth.},
  keywords={IEEE Standards;Natural language processing;Robustness;Artificial intelligence;Performance evaluation;Measurement;Machine learning;artificial intelligence;evaluation metrics;IEEE 3168™;natural language processing service;robustness evaluation},
  doi={},
  ISSN={},
  month={March},}@INPROCEEDINGS{10563652,
  author={Ramteke, Vidyavati and Babu, Manojkumar and Verma, Pradeep},
  booktitle={2024 4th International Conference on Innovative Practices in Technology and Management (ICIPTM)},
  title={Technology Trends in Synthetic Media – Bibliometric analysis of Existing Literature and Future Scope},
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Fake media is being created and floated around for a long time now and so does the research on techniques to generate, detect, and improve them. The earliest available research in the Scopus database dates back to the year 1996. However, the dynamics of this field and thereby the research changed once this has become a problem that found machine learning applications. The field of synthetic media has grown so much beyond fake text into machine-generated images, audio, and video. This led to a new terminology called deepfakes. These are media developed by leveraging machine learning and artificial intelligence techniques with a purpose to modify existing content or create new ones. In this article, we analyze the existing research articles to identify the current trajectory of research and recent developments. We also propose a use case to emphasize the future directions that research could take in this field.},
  keywords={Deepfakes;Terminology;Databases;Bibliometrics;Machine learning;Media;Market research;synthetic;fake media;deep fake;media;synthesis;artificial intelligence;bibliometric},
  doi={10.1109/ICIPTM59628.2024.10563652},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{6118700,
  author={Guo, Zhixin and Jin, Hai},
  booktitle={2011 10th International Symposium on Distributed Computing and Applications to Business, Engineering and Science},
  title={A Rule-Based Framework of Metadata Extraction from Scientific Papers},
  year={2011},
  volume={},
  number={},
  pages={400-404},
  abstract={Most scientific documents on the web are unstructured or semi-structured, and the automatic document metadata extraction process becomes an important task. This paper describes a framework for automatic metadata extraction from scientific papers. Based on a spatial and visual knowledge principle, our system can extract title, authors and abstract from scientific papers. We utilize format information such as font size and position to guide the metadata extraction process. The experiment results show that our system achieves a high accuracy in header metadata extraction which can effectively assist the automatic index creation for digital libraries.},
  keywords={Portable document format;Data mining;Semantics;Libraries;Accuracy;XML;Layout;document metadata;information extraction;rule-based approach},
  doi={10.1109/DCABES.2011.14},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10110764,
  author={Bobunov, Artyom and Korobkin, Dmitriy and Fomenkov, Sergey},
  booktitle={2023 International Russian Smart Industry Conference (SmartIndustryCon)},
  title={Development of the Concept and Architecture of an Automated System for Updating Physical Knowledge for Information Support of Search Design},
  year={2023},
  volume={},
  number={},
  pages={281-288},
  abstract={For developing an automated system for updating physical knowledge for information support of search design, it is necessary to choose a technology stack that would meet the implementation requirements. In view of the sanctions currently imposed on the Russian Federation, it is worth considering mainly open projects and/or domestic developments. We will highlight the main criteria that it is desirable to take into account when designing the architecture of an automated system to support the synthesis of new technical systems and technologies: (a) ability to store and process large amounts of data; (b) unification access for all data analysis procedures; (c) maximum automation of all stages; (d) modularity of the structure, focus on the expansion of functionality; (e) focus on open source solutions and software of domestic manufacturers, excluding rigid binding to paid foreign solutions. As a result of the work done, various aspects of the implementation of the required automated system were analyzed. A review of various software systems and cloud products showed that the concept of building data lakes (Data Lake) in conjunction with the distributed processing tools of the Apache Hadoop ecosystem is used for big data processing. An architecture framework based on a centralized data warehouse and Hadoop components is proposed. It will be possible to increase the functionality of the platform by adding new microservices that connect to the storage and distributed processing tools via the API, as well as using a single web service for managing and displaying data analysis results from these microservices.},
  keywords={Distributed processing;Data analysis;Web services;Architecture;Ecosystems;Microservice architectures;Computer architecture;architecture;physical knowledge;information support;search design},
  doi={10.1109/SmartIndustryCon57312.2023.10110764},
  ISSN={},
  month={March},}@BOOK{8187416,
  author={Shokouhi, Milad and Si, Luo},
  title={Federated Search},
  year={2011},
  volume={},
  number={},
  pages={},
  abstract={Web search has significantly evolved in recent years. For many years, web search engines such as Google and Yahoo! were only providing search service over text documents. Aggregated search was one of the first steps to go beyond text search, and was the beginning of a new era for information seeking and retrieval. These days, web search engines support aggregated search over a number of verticals, and blend different types of documents (e.g. images, videos) in their search results. Moreover, web search engines have started to crawl and search the hidden web. Federated search (federated information retrieval or distributed information retrieval) has played a key role in providing the technology for aggregated search and crawling the hidden web. The application of federated search is not limited to the web search engines. There are many scenarios such as digital libraries in which information is distributed across different sources/servers. Peer-to-peer networks and personalized search are two examples in which federated search has been successfully used for searching multiple independent collections. Federated Search provides a comprehensive summary of the research done to date, looks at some of the challenges still to be faced, and suggests some directions for future research on this important and current topic.},
  keywords={Databases;Information Retrieval;Machine Learning},
  doi={10.1561/1500000010},
  ISSN={},
  publisher={now},
  isbn={9781601984234},
  url={https://ieeexplore.ieee.org/document/8187416},}@INPROCEEDINGS{5277548,
  author={Gao, Liangcai and Tang, Zhi and Lin, Xiaofan and Tao, Xin and Chu, Yimin},
  booktitle={2009 10th International Conference on Document Analysis and Recognition},
  title={Analysis of book documents' table of content based on clustering},
  year={2009},
  volume={},
  number={},
  pages={911-915},
  abstract={Table of contents (TOC) recognition has attracted a great deal of attention in recent years. After reviewing the merits and drawbacks of the existing TOC recognition methods, we have observed that book documents are multi-page documents with intrinsic local format consistency. Based on this finding we introduce an automatic TOC analysis method through clustering. This method first detects the decorative elements in TOC pages. Then it learns a layout model used in the TOC pages through clustering. Finally, it generates TOC entries and extracts their hierarchical structure under the guidance of the model. More specifically, broken lines are taken into account in the method. Experimental results show that this method achieves high accuracy and efficiency. In addition, this method has been successfully applied in a commercial e-book production software package.},
  keywords={Books;Layout;Data mining;Clustering algorithms;Adaptation model;Algorithm design and analysis},
  doi={10.1109/ICDAR.2009.143},
  ISSN={2379-2140},
  month={July},}@ARTICLE{8952616,
  author={Kronemeyer, Lena L. and Eilers, Kathi and Wustmans, Michael and Moehrle, Martin G.},
  journal={IEEE Transactions on Engineering Management},
  title={Monitoring Competitors’ Innovation Activities: Analyzing the Competitive Patent Landscape Based on Semantic Anchor Points},
  year={2021},
  volume={68},
  number={5},
  pages={1272-1287},
  abstract={Monitoring the movements of competitors as well as their innovative endeavors and the targets of their efforts is an important task for the business intelligence of a company. In addition to market-oriented means, such as observing competitors' publications and websites, or analyzing existing products, patent analysis may offer future-oriented insights. Usually, analysts in companies search for competitors' patents and analyze them manually, producing qualitative information for the abovementioned task. Apart from this qualitative information, quantitative information might help to measure, visualize, and communicate what competitors are really doing. For this purpose, we develop a method based on semantic anchor points aiming at analyzing the competitive landscape to achieve insights as to how the innovation activities of one company are reflected in the core competencies of another company over time. As novel contributions, we adapt the forming of anchor points for the case of companies and develop several quantitative analysis types, e.g., regarding basic competitor landscaping and high risk analysis. We explain the four steps of our approach and demonstrate its use in monitoring direct competitors in the technology field of gearing, which is part of the automotive industry. Our method can be transferred to other industries due to specific suppositions. It is able to form the core of an automated monitoring system.},
  keywords={Patents;Tech mining;Technological innovation;Semantics;Text mining;Monitoring;Portfolios;Infometrics;Competitive intelligence;Competitive indicators;competitor analysis;informetric measures;patent analysis;patent maps;semantic anchor points;visualization},
  doi={10.1109/TEM.2019.2958518},
  ISSN={1558-0040},
  month={Oct},}@INPROCEEDINGS{8791225,
  author={Boukhers, Zeyd and Ambhore, Shriharsh and Staab, Steffen},
  booktitle={2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
  title={An End-to-End Approach for Extracting and Segmenting High-Variance References from PDF Documents},
  year={2019},
  volume={},
  number={},
  pages={186-195},
  abstract={This paper addresses the problem of extracting and segmenting references from PDF documents. The novelty of the presented approach lies in its capability to discover highly varying references mainly in terms of content, length and location in the document. Unlike existing works, the proposed method does not follow the classical pipeline that consists of sequential phases. It rather learns the different characteristics of references to be used in a coherent scheme that reduces the error accumulation by following a probabilistic approach. Contrary to conventional references, mentioning the sources of information in some publications, such as those of social science, is not subject to the same specifications such as being located in a unique reference section. Therefore, the proposed method aims to extract references of highly varying reference characteristics by relaxing the restrictions of existing methods. Additionally, we present in this paper a new challenging dataset of annotated references in German social science publications. The main purpose of this work is to serve the indexation of missing references by extracting them from challenging publications such as those of German social science. The effectiveness of the presented methods in terms of both extraction and segmentation is evaluated on different datasets, including the German social science set.},
  keywords={Hidden Markov models;Feature extraction;Support vector machines;Social sciences;Tools;Portable document format;Task analysis;Reference Extraction;Reference Segmentation;Conditional Random Fields;Random Forest},
  doi={10.1109/JCDL.2019.00035},
  ISSN={},
  month={June},}@ARTICLE{9845428,
  author={Maglietta, Rosalia and Carlucci, Roberto and Fanizza, Carmelo and Dimauro, Giovanni},
  journal={IEEE Access},
  title={Machine Learning and Image Processing Methods for Cetacean Photo Identification: A Systematic Review},
  year={2022},
  volume={10},
  number={},
  pages={80195-80207},
  abstract={Photo identification is an essential method to identify cetaceans, by using natural marks over their body, and allows experts to acquire straightforward information on these animals. The importance of cetaceans lies in te fact that they play a crucial role in maintaining the healthiness of marine ecosystems, however they are exposed to several anthropogenic stressors, under which they could collapse with extreme consequences on the marine ecosystem functioning. Hence, obtaining new knowledge on their status is extremely urgent for the marine biodiversity conservation. The smart use of technology to automate the individual recognition can speed up the photo identification process, opening the door to large-scale studies that are manually unfeasible. We performed a systematic review on systems based on machine learning and statistical methods for cetacean photo identification, following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses statement. This review highlights that interest has been increasing in recent years and several intelligent systems have been presented. However, there are still some open questions, and further efforts to develop more effective automated systems for cetacean photo identification are recommended.},
  keywords={Systematics;Animals;Machine learning;Dolphins;Databases;Task analysis;Feature extraction;Machine learning algorithms;convolutional neural networks;feature extraction;reviews;oceanic engineering and marine technology;image processing},
  doi={10.1109/ACCESS.2022.3195218},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8215533,
  author={Zhang, Jiawei and Xia, Congying and Zhang, Chenwei and Cui, Limeng and Fu, Yanjie and Yu, Philip S.},
  booktitle={2017 IEEE International Conference on Data Mining (ICDM)},
  title={BL-MNE: Emerging Heterogeneous Social Network Embedding Through Broad Learning with Aligned Autoencoder},
  year={2017},
  volume={},
  number={},
  pages={605-614},
  abstract={Network embedding aims at projecting the network data into a low-dimensional feature space, where the nodes are represented as a unique feature vector and network structure can be effectively preserved. In recent years, more and more online application service sites can be represented as massive and complex networks, which are extremely challenging for traditional machine learning algorithms to deal with. Effective embedding of the complex network data into low-dimension feature representation can both save data storage space and enable traditional machine learning algorithms applicable to handle the network data. Network embedding performance will degrade greatly if the networks are of a sparse structure, like the emerging networks with few connections. In this paper, we propose to learn the embedding representation for a target emerging network based on the broad learning setting, where the emerging network is aligned with other external mature networks at the same time. To solve the problem, a new embedding framework, namely "Deep alIgned autoencoder based eMbEdding" (DIME), is introduced in this paper. DIME handles the diverse link and attribute in a unified analytic based on broad learning, and introduces the multiple aligned attributed heterogeneous social network concept to model the network structure. A set of meta paths are introduced in the paper, which define various kinds of connections among users via the heterogeneous link and attribute information. The closeness among users in the networks are defined as the meta proximity scores, which will be fed into DIME to learn the embedding vectors of users in the emerging network. Extensive experiments have been done on real-world aligned social networks, which have demonstrated the effectiveness of DIME in learning the emerging network embedding vectors.},
  keywords={Twitter;Heterogeneous networks;Complex networks;Machine learning algorithms;Terminology;Network Embedding;Multiple Aligned Social Networks;Heterogeneous Information Network;Broad Learning},
  doi={10.1109/ICDM.2017.70},
  ISSN={2374-8486},
  month={Nov},}@INPROCEEDINGS{1336139,
  author={Han, H. and Giles, L. and Zha, H. and Li, C. and Tsioutsiouliklis, K.},
  booktitle={Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 2004.},
  title={Two supervised learning approaches for name disambiguation in author citations},
  year={2004},
  volume={},
  number={},
  pages={296-305},
  abstract={Due to name abbreviations, identical names, name misspellings, and pseudonyms in publications or bibliographies (citations), an author may have multiple names and multiple authors may share the same name. Such name ambiguity affects the performance of document retrieval, Web search, database integration, and may cause improper attribution to authors. We investigate two supervised learning approaches to disambiguate authors in the citations. One approach uses the naive Bayes probability model, a generative model; the other uses support vector machines (SVMs) [V. Vapnik (1995)] and the vector space representation of citations, a discriminative model. Both approaches utilize three types of citation attributes: coauthor names, the title of the paper, and the title of the journal or proceeding. We illustrate these two approaches on two types of data, one collected from the Web, mainly publication lists from homepages, the other collected from the DBLP citation databases.},
  keywords={Supervised learning;Computer science;Bibliographies;Information retrieval;Statistics;Web search;Databases;Permission;Software libraries;Public healthcare},
  doi={10.1145/996350.996419},
  ISSN={},
  month={June},}@INPROCEEDINGS{8783590,
  author={Navas, Ederick and Chacón Rivas, Mario and Garita, Cesar},
  booktitle={2018 XIII Latin American Conference on Learning Technologies (LACLO)},
  title={Evaluating Concept Maps within an E-Learning Platform},
  year={2018},
  volume={},
  number={},
  pages={77-80},
  abstract={Concept Maps (CMs) have been used as an evaluation tool in learning processes since they allow the representation and analysis of the knowledge developed by a student regarding a specific subject. In order to properly evaluate and grade CMs developed by students, it is necessary to define a set of criteria in relation to for instance, the structure, size and quality of the CMs. The process of creation, submission, analysis, evaluation and grading of CM according to these criteria can be complex and time-consuming for tutors. In this context, this paper presents an implementation of the CMapTEC tool to support the semiautomated evaluation of CMs within TEC Digital - the elearning platform of the Costa Rica Institute of Technology (TEC). The proposed approach is based on the integration of the CmapServer and CmapAnalysis software from the Institute for Human and Machine Cognition with the evaluation package of .LRN learning management system. Therefore, the achieved solution is completely based on open-source initiatives.},
  keywords={Tools;Software;Visualization;Training;Atmospheric measurements;Particle measurements;Metadata;e learning, conceptual maps evaluation, open source},
  doi={10.1109/LACLO.2018.00028},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8453948,
  author={Prokhorenkov, Dmitry and Panfilov, Petr},
  booktitle={2018 IEEE 20th Conference on Business Informatics (CBI)},
  title={Notice of Violation of IEEE Publication Principles: Discovery of Technology Trends from Patent Data on the Basis of Predictive Analytics},
  year={2018},
  volume={02},
  number={},
  pages={148-152},
  abstract={Companies are increasingly paying close attention to the IP portfolio, which is a key competitive advantage, so patents and patent applications, as well as analysis and identification of future trends, become one of the important and strategic components of a business strategy. We argue that the problems of identifying and predicting trends or entities, as well as the search for technical features, can be solved with the help of easily accessible Big Data technologies, machine learning and predictive analytics, thereby offering an effective plan for development and progress. The purpose of this study is twofold, the first is an identification of technological trends, the second is an identification of application areas and/or that are most promising in terms of technology development and investment. The research was based on methods of clustering, processing of large text files and search queries in patent databases. The suggested approach is considered on the basis of experimental data in the field of moving connected UAVs and passive acoustic ecology control.},
  keywords={},
  doi={10.1109/CBI.2018.10062},
  ISSN={2378-1971},
  month={July},}@ARTICLE{9970716,
  author={Cunha, Vanice Canuto and Zavala, Arturo Zavala and Magoni, Damien and Inácio, Pedro R. M. and Freire, Mário M.},
  journal={IEEE Access},
  title={A Complete Review on the Application of Statistical Methods for Evaluating Internet Traffic Usage},
  year={2022},
  volume={10},
  number={},
  pages={128433-128455},
  abstract={Internet traffic classification aims to identify the kind of Internet traffic. With the rise of traffic encryption and multi-layer data encapsulation, some classic classification methods have lost their strength. In an attempt to increase classification performance, Machine Learning (ML) strategies have gained the scientific community interest and have shown themselves promising in the future of traffic classification, mainly in the recognition of encrypted traffic. However, some of these methods have a high computational resource consumption, which make them unfeasible for classification of large traffic flows or in real-time. Methods using statistical analysis have been used to classify real-time traffic or large traffic flows, where the main objective is to find statistical differences among flows or find a pattern in traffic characteristics through statistical properties that allow traffic classification. The purpose of this work is to address statistical methods to classify Internet traffic that were little or unexplored in the literature. This work is not generally focused on discussing statistical methodology. It focuses on discussing statistical tools applied to Internet traffic classification Thus, we provide an overview on statistical distances and divergences previously used or with potential to be used in the classification of Internet traffic. Then, we review previous works about Internet traffic classification using statistical methods, namely Euclidean, Bhattacharyya, and Hellinger distances, Jensen-Shannon and Kullback–Leibler (KL) divergences, Support Vector Machines (SVM), Correlation Information (Pearson Correlation), Kolmogorov-Smirnov and Chi-Square tests, and Entropy. We also discuss some open issues and future research directions on Internet traffic classification using statistical methods.},
  keywords={Internet;Statistical analysis;Protocols;Support vector machines;Correlation;Real-time systems;Quality of service;Encrypted internet traffic;traffic classification;statistical distances;statistical divergences;statistical methods;support vector machines},
  doi={10.1109/ACCESS.2022.3227073},
  ISSN={2169-3536},
  month={},}@ARTICLE{10701307,
  author={Castillo-Rojas, Wilson and Pastén Salinas, Javier},
  journal={IEEE Access},
  title={Forecasting Models Applied in Solar Photovoltaic and Wind Energy: A Systematic Mapping Study},
  year={2024},
  volume={12},
  number={},
  pages={151092-151111},
  abstract={The effective management of renewable energy facilities, particularly those utilizing solar PV and wind technologies, faces a number of challenges such as weather variability, data resolution, and handling large data volumes. The number of publications addressing these issues is increasing, and it is anticipated that this growth will continue in the near term. Implementing data analysis tools for power plant management can enhance our understanding of weather events and improve electric power generation forecasting accuracy. Despite common characteristics between solar PV and wind energy, including their reliance on weather conditions, handling data volumes, and processing time series data, few studies offer a comprehensive view of generalizing forecast models. Our objective is to determine the trends in forecasts models in this area, the most commonly used techniques that show better performance and efficiency, the variables involved in each type of energy, among other aspects. A systematic mapping study of scientific literature was performed, selecting 75 primary studies grouped by type of machine learning technique. Some common aspects were found regardless of the type of energy, solar PV or wind, mainly related to single or hybrid forecasting models. Other common factors found are the types of model optimization algorithms used and feature extraction and decomposition techniques. The study highlights the importance of hybrid models that demonstrate, compared to conventional or individual models, significant improvements in the level of accuracy of their forecasts for both solar PV and wind energy, combining various statistical, machine learning or deep learning techniques.},
  keywords={Wind forecasting;Predictive models;Meteorology;Renewable energy sources;Data models;Accuracy;Wind energy;Production;Forecasting;Photovoltaic systems;Wind power generation;Photovoltaic energy forecast;wind energy forecast;machine learning;deep learning;photovoltaic production;wind production},
  doi={10.1109/ACCESS.2024.3471073},
  ISSN={2169-3536},
  month={},}@ARTICLE{9623451,
  author={Kumar, Satish and Gupta, Sunanda and Arora, Sakshi},
  journal={IEEE Access},
  title={Research Trends in Network-Based Intrusion Detection Systems: A Review},
  year={2021},
  volume={9},
  number={},
  pages={157761-157779},
  abstract={Network threats and hazards are evolving at a high-speed rate in recent years. Many mechanisms (such as firewalls, anti-virus, anti-malware, and spam filters) are being used as security tools to protect networks. An intrusion detection system (IDS) is also an effective and powerful network security system to detect unauthorized and abnormal network traffic flow. This article presents a review of the research trends in network-based intrusion detection systems (NIDS), their approaches, and the most common datasets used to evaluate IDS Models. The analysis presented in this paper is based on the number of citations acquired by an article published, the total count of articles published related to intrusion detection in a year, and most cited research articles related to the intrusion detection system in journals and conferences separately. Based on the published articles in the intrusion detection field for the last 15 years, this article also discusses the state-of-the-arts of NIDS, commonly used NIDS, citation-based analysis of benchmark datasets, and NIDS techniques used for intrusion detection. A citation and publication-based comparative analysis to quantify the popularity of various approaches are also presented in this paper. The study in this article may be helpful to the novices and researchers interested in evaluating research trends in NIDS and their related applications.},
  keywords={Intrusion detection;Market research;Computer security;Search engines;Feature extraction;Computer hacking;Machine learning;Citation;machine learning;bio-inspired;intrusion detection system;NIDS;datasets},
  doi={10.1109/ACCESS.2021.3129775},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{7836021,
  author={Santos, Mariana and Afonso, Paulo and Bermejo, Paulo Henrique and Costa, Heitor},
  booktitle={2016 35th International Conference of the Chilean Computer Science Society (SCCC)},
  title={Metrics and statistical techniques used to evaluate internal quality of object-oriented software: A systematic mapping},
  year={2016},
  volume={},
  number={},
  pages={1-11},
  abstract={Efficient ways to measure and evaluate internal quality of object-oriented (OO) software are widely discussed in the literature. Studies in software engineering has used statistical techniques/models to evaluate quality in different aspects (quality characteristics and sub characteristics) and using different variables (traditional and object-oriented software metrics) to quantify it. This paper presents results of applying a systematic mapping to identify the most software metrics cited and the most statistical techniques/models used to evaluate quality in OO software. As a result, we highlighted 79 papers related to the research question in a total of 8,231 papers published in scientific journals. The main contributions of this work are (i) a list of the most used metrics and measurable properties, (ii) a list of the most used statistical techniques/models, and (ii) the main references, themes to be explored and recommendations for future research.},
  keywords={Software;Computer science;Software measurement;Search engines;Statistical analysis;Systematics;Global Software Development;Software Project Management},
  doi={10.1109/SCCC.2016.7836021},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10487248,
  author={Kang, Woojin and Kim, Yumi and Kim, Heesop and Lee, Jongwook},
  booktitle={2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)},
  title={An Analysis of Research Trends on Language Model Using BERTopic},
  year={2023},
  volume={},
  number={},
  pages={168-172},
  abstract={Although language models have played a crucial role in various natural language processing tasks, there has been little research that focuses on systematic analysis and review of research topic trends in these models. In this paper, we conducted a comprehensive analysis of 31 years of research trends in the field of language models, using publications from Scopus, an internationally renowned academic database, to identify research topics related to language models. We adopted BERTopic, a state-of-the-art topic modeling technique, on the 13,754 research articles about language models. The research on language models has gradually increased since 1991, and there is a sudden increase in the number of publications with the emergence of BERT and GPT in 2018. We assigned 14 main topics with meaningful keywords clustered by BERTopic model. Among 14 topics, research related to speech recognition, statistical language models, and pre-trained language models demonstrated the most vigorous research fields. Our results demonstrate a more systematic and comprehensive trend in language model research, which is expected to provide an important foundation for future research directions.},
  keywords={Analytical models;Systematics;Reviews;Databases;Computational modeling;Neural networks;Speech recognition;language models;topic modeling;BERT;re-search trends;Short Research Paper},
  doi={10.1109/CSCE60160.2023.00032},
  ISSN={},
  month={July},}@INPROCEEDINGS{8387245,
  author={Azimzadeh, Fatemeh and Norouzi, Farzaneh},
  booktitle={2018 4th International Conference on Web Research (ICWR)},
  title={Incorporating structural information in scientific document retrieval},
  year={2018},
  volume={},
  number={},
  pages={103-110},
  abstract={With the daily-increasing development of science, various methods have been designed to more and better retrieve the scientific documents based on the need and search of users. For some documents in the various scientific databases, no complete information exists and the users have to observe the inside of a document in order to catch up with its metadata inclusion the authors, their affiliations, the references cited and etc. Therefore, presence of a method based on extracting the information based on the available structural and geometrical properties in a document can assist the recovery of related and required documents. In addition, the available pitfall in the relational data based is the lack of direct and indirect relationships between the availabilities of each system for which a graph-oriented database can establish the relations between these availabilities. In this respect, after extracting metadata using the geometrical properties of document and using a graph-oriented model, the relations between various documents' availabilities such as authors, conferences, subjects and keywords and etc. are modeled in order to retrieve the information more effectively. The extracted data are refined and stored in the graph model and will be available for a user via a web-based user interface. To produce the results of each search, the related documents will be retrieved based on the graph relations and be weighed according to the rate of relatedness of each document and the number of references. In order to evaluate the proposed method, PubMed Database is used. The results of experiments show the proposed methods outperformed 60% in contrast to the PubMed Database search engine in terms of the retrieved documents. Furthermore, based in the F-measure, and nDCG-measure of proposed method considerably outperformed the PubMed Database search engine in terms of the quality of retrieved documents.},
  keywords={Metadata;Databases;Data mining;Feature extraction;Machine learning;Probability density function;Standards;Metadata Extraction;Information Retrieval;Graph Data Model;Structural Data},
  doi={10.1109/ICWR.2018.8387245},
  ISSN={},
  month={April},}@INPROCEEDINGS{5563060,
  author={Qi, Xingqin and Christensen, Kyle and Duval, Robert and Fuller, Edgar and Spahiu, Arian and Wu, Qin and Zhang, Cun-Quan},
  booktitle={2010 International Conference on Advances in Social Networks Analysis and Mining},
  title={A Hierarchical Algorithm for Clustering Extremist Web Pages},
  year={2010},
  volume={},
  number={},
  pages={458-463},
  abstract={Extremist political movements have proliferated on the web in recent years due to the advent of minimal publication costs coupled with near universal access, resulting in what appears to be an abundance of groups that hover on the fringe of many socially divisive issues. Whether white-supremacist, neo- Nazi, anti-abortion, black separatist, radical Christian, animal rights, or violent environmentalists, all have found a home (and voice) on the Web. These groups form social networks whose ties are predicated primarily on shared political goals. Little is known about these groups, their interconnections, their animosities, and most importantly, their growth and development and studies such as the Dark Web Project, while considering domestic extremists, have focused primarily on international terrorist groups. Yet here in the US, there has been a complex social dynamic unfolding as well. While left-wing radicalism declined throughout the 80s and 90s, right wing hate groups began to flourish. Today, the web offers a place for any brand of extremism, but little is understood about their current growth and development. While there is much to gain from in-depth studies of the content provided by these sites, there is also a surprising amount of information contained in their online network structure as manifested in links between and among these web sites. Our research follows the idea that much can be known about you by the company you keep. In this paper, we propose an approach to measure the intrinsic relationships (i.e., similarities) of a set of extremist web pages. In this model, the web presence of a group is thought of as a node in a social network and the links between these pages are the ties between groups. This approach takes the bi-directional hyperlink structure of web pages and, based on similarity scores, applies an effective multi-membership clustering algorithm known as the quasi clique merger method to cluster these web pages using a derived hierarchical tree. The experimental results show that this new similarity measurement and hierarchical clustering algorithm gives an improvement over traditional link based clustering methods.},
  keywords={Web pages;Clustering algorithms;Communities;Algorithm design and analysis;Couplings;Joining processes;Social network services},
  doi={10.1109/ASONAM.2010.81},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{7863683,
  author={Ghaleb, Taher Ahmed and Mohammed, Mawal Ali and Ramadan, Emad},
  booktitle={2016 2nd International Conference on Open Source Software Computing (OSSCOM)},
  title={Automated analysis of flow cytometry data: a systematic review of recent methods},
  year={2016},
  volume={},
  number={},
  pages={1-7},
  abstract={Flow cytometry (FCM) is a very well-known method that is broadly used in clinical and research laboratories. Both clinical and research laboratories have been the target domains of FCM applications. The key research question in this particular field is “how to effectively automate FCM data analysis?”. To answer this question, this paper systematically reviews current advances in the automation of FCM data analysis. All recent techniques have been studied in a way readers can recognize current trends, challenges, limitations and future directions. For future research, we have identified three main venues. First, the identification of the number of clusters prior to starting cell population identification is still a challenging process. Second, automating the process of cluster labeling still requires more improvement to be fully automated. Last, benchmark datasets are essential in order for researchers to be able to comparatively evaluate different techniques of FCM data analysis under fixed conditions.We end up this paper with a discussion about how flow cytometry data analysis techniques and datasets are correlated with open source technology.},
  keywords={Clustering algorithms;Algorithm design and analysis;Data analysis;Sociology;Statistics;Laboratories;Labeling;Automated Gating;Clustering;Data analysis;Flow Cytometry (FCM);Multidimensional data;Open Source Software},
  doi={10.1109/OSSCOM.2016.7863683},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10549456,
  author={Imad, Hamdi and Sara, Zada and Hajji, Morad and Yassine, Tounsi and Abdelkrim, Nassim},
  booktitle={2024 4th International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)},
  title={Recent Advances in SAR Image Analysis Using Deep Learning Approaches: Examples of Speckle Denoising and Change Detection},
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Deep learning is a machine learning technique that has significantly improved results in many areas such as computer vision, speech recognition, machine translation, and biomedical imaging analysis and understanding. Recently in the field of synthetic aperture radar (SAR) images analysis, deep learning approaches have become a powerful tool making information extraction from SAR images very accurate and giving good interpretations related to environment and earth surface observation. This work examines the recent development of scientific productions on the applications of deep learning approaches in the SAR imaging field. These applications concern the speckle noise reduction from SAR images and the algorithms-based method for change detection and classification of these remote sensing images. Moreover, an analysis of scientific production in this field is discussed by exploiting the Scopus database.},
  keywords={Deep learning;Databases;Reviews;Noise reduction;Production;Speech recognition;Speckle;SAR image;Speckle reducing;Deep learning;Change detection;Bibliometric analysis;Image classification},
  doi={10.1109/IRASET60544.2024.10549456},
  ISSN={},
  month={May},}@ARTICLE{10456906,
  author={García, Roberto and Angles, Renzo},
  journal={IEEE Access},
  title={Path Querying in Graph Databases: A Systematic Mapping Study},
  year={2024},
  volume={12},
  number={},
  pages={33154-33172},
  abstract={Path querying refers to the evaluation of path queries in a graph database. New research in this topic is crucial for the development of graph database systems as path queries are associated with relevant use-cases and application domains. The aim of this article is to identify and establish what is currently known about path querying in graph databases. To achieve this, we conducted a systematic mapping study (SMS) in which we explored four digital libraries and collected research papers published from 1970 to 2022. These articles were filtered, classified and analyzed to extract quantitative and qualitative information which is presented in this article. Additionally, we provide a concise description of keywords, use-cases and application domains associated with path querying in graph databases.},
  keywords={Reviews;Systematics;Databases;Database languages;Surveys;Data models;Optimization methods;Query processing;Path planning;Graphical models;Path;path querying},
  doi={10.1109/ACCESS.2024.3371976},
  ISSN={2169-3536},
  month={},}@ARTICLE{1193658,
  author={He, Y. and Hui, S.C. and Fong, A.C.M.},
  journal={IEEE Intelligent Systems},
  title={Citation-based retrieval for scholarly publications},
  year={2003},
  volume={18},
  number={2},
  pages={58-65},
  abstract={Scholarly publications are available online and in digital libraries, but existing search engines are mostly ineffective for these publications. The proposed publication retrieval system is based on Kohonen's self-organizing map and offers fast retrieval speeds and high precision in terms of relevance.},
  keywords={Indexing;Search engines;Information retrieval;Intelligent agent;Internet;Data mining;Page description languages;Deductive databases;Software libraries},
  doi={10.1109/MIS.2003.1193658},
  ISSN={1941-1294},
  month={March},}@INPROCEEDINGS{9707362,
  author={Tobias, Rogelio Ruzcko and Roxas, Rachel Edita and Abisado, Mideth},
  booktitle={TENCON 2021 - 2021 IEEE Region 10 Conference (TENCON)},
  title={Science Mapping of Social Media Analytics in Health Through Artificial Intelligence},
  year={2021},
  volume={},
  number={},
  pages={750-755},
  abstract={This paper presents a systematic literature review and bibliometric analyses of Scopus-indexed documents in social media analytics in health during the CoVid-19 pandemic that used artificial intelligence methodologies. From the 179 extracted Scopus-indexed publications in August 2021, 128 were left after removing 51 documents using the Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) procedure. Analyses and visualizations using VOSviewer reveal research productivity, affiliation and collaboration networks, and the corresponding relationship between research productivity and the research networks. Conclusions and recommendations for future work are presented to further nurture the current research environment of social media analytics through artificial intelligence methodologies.},
  keywords={Productivity;Visualization;Systematics;Text analysis;Social networking (online);Pandemics;Conferences;artificial intelligence;bibliometrics;covid-19;health;social media},
  doi={10.1109/TENCON54134.2021.9707362},
  ISSN={2159-3450},
  month={Dec},}@INPROCEEDINGS{5693916,
  author={Price, Simon and Flach, Peter A. and Spiegler, Sebastian and Bailey, Christopher and Rogers, Nikki},
  booktitle={2010 IEEE Sixth International Conference on e-Science},
  title={SubSift Web Services and Workflows for Profiling and Comparing Scientists and Their Published Works},
  year={2010},
  volume={},
  number={},
  pages={182-189},
  abstract={Scientific researchers, laboratories and organisations can be profiled and compared by analysing their published works, including documents ranging from academic papers to web sites, blog posts and Twitter feeds. This paper describes how the vector space model from information retrieval, more normally associated with full text search, has been employed in the open source Sub Sift software to support workflows to profile and compare such collections of documents. Sub Sift was originally designed to match submitted conference or journal papers to potential peer reviewers based on the similarity between the paper's abstract and the reviewer's publications as found in online bibliographic databases. The software is implemented as a family of Restful web services that, composed into a re-usable workflow, have already been used to support several major data mining conferences. Alternative workflows and service compositions are now enabling other interesting applications.},
  keywords={Web services;XML;Software;HTML;Resource description framework;Data mining;Abstracts},
  doi={10.1109/eScience.2010.29},
  ISSN={},
  month={Dec},}@ARTICLE{9240970,
  author={Liu, Dongjiang and Li, Leixiao and Ma, Zhiqiang},
  journal={IEEE Access},
  title={A Community Detection Algorithm for Heterogeneous Information Networks},
  year={2020},
  volume={8},
  number={},
  pages={195655-195663},
  abstract={With the fast development of Internet, many fields have accumulated great amount of data. And plenty of them are organized in heterogeneous information network (HIN), so analyzing HIN efficiently is very necessary. Several HIN clustering algorithms have been proposed in recent years. Most of them are based on meta-path. As instances of meta-paths can connect all the target objects directly, while clustering, all these algorithms only focus on the relationship of two target objects that are connected directly by an instance of meta-paths. In this situation, information contained in the relationship of two target objects that are not directly connected by an instance of meta-paths is neglected. These target object pairs may be very helpful for obtaining better clustering result. In order to take these target object pairs into consideration, a structural neighbor searching method is applied into the proposed algorithm. By using this method, all the indirectly connected neighbors of target objects are considered while performing clustering. Moreover, as more than one meta-paths can be found in a heterogeneous information network and each meta-path can impact the clustering result in different degree, a weight value should be assigned to each meta-path. These weight values are used to represent the relative importance of meta-paths. To obtain these weight values, a calculation method is proposed and the algorithm described in this paper tries to calculate weight values by using this method. With weight values of meta-paths and structural neighbors of target objects, all the vectors of target objects can be calculated. Then the hierarchical clustering procedure will be performed based on these vectors.},
  keywords={Clustering algorithms;Social networking (online);Image edge detection;Task analysis;Detection algorithms;Sun;Medical services;Clustering;community detection;heterogeneous information networks;meta-path;vectors},
  doi={10.1109/ACCESS.2020.3034214},
  ISSN={2169-3536},
  month={},}@ARTICLE{9369392,
  author={Trinidad, Manuel and Ruiz, Mercedes and Calderón, Alejandro},
  journal={IEEE Access},
  title={A Bibliometric Analysis of Gamification Research},
  year={2021},
  volume={9},
  number={},
  pages={46505-46544},
  abstract={Gamification has rapidly emerged as one of the favorite persuasive technologies widely used with the aim of promoting a positive change in the user's behavior by means of including game-like elements in non-game contexts. As a research discipline, gamification is growing fast, maturing from basic and fundamental questions such as what and why gamify to more mature ones such as how to gamify, when and when not, and still facing empirical and theoretical challenges to prove the effects of its practice and consolidate the principles that guide meaningful gamification designs. The purpose of this paper is to conduct a bibliometric study to describe how gamification as a scientific discipline is structured and how it has evolved over time. To do this, we make use of bibliometric performance analysis and science mapping methods to display and analyze the intellectual, conceptual and social network structures of gamification research, as well as the evolution and dynamical aspects of the discipline. The results reveal the research fronts and intellectual structures of the field, the internal relationships among articles, authors and keywords, the existing networks of collaboration, the emerging trends, the hot topics, and the most influential authors, publications and sources. Together, they picture the intellectual landscape of gamification as a scientific field that will be useful for junior and senior researchers, practitioners, funding agencies and policymakers.},
  keywords={Bibliometrics;Education;Games;Social networking (online);Market research;Collaboration;Bibliographies;Bibliometrics;computers and information processing;gamification;science mapping;scientometrics},
  doi={10.1109/ACCESS.2021.3063986},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{4119095,
  author={F. Laender, Alberto H. and Da Silva, Altigran S. and Goncalves, Marcos Andre and De Carvalho, Moises G.},
  booktitle={Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)},
  title={Learning to deduplicate},
  year={2006},
  volume={},
  number={},
  pages={41-50},
  abstract={Identifying record replicas in Digital Libraries and other types of digital repositories is fundamental to improve the quality of their content and services as well as to yield eventual sharing efforts. Several deduplication strategies are available, but most of them rely on manually chosen settings to combine evidence used to identify records as being replicas. In this paper, we present the results of experiments we have carried out with a novel Machine Learning approach we have proposed for the deduplication problem. This approach, based on Genetic Programming (GP), is able to automatically generate similarity functions to identify record replicas in a given repository. The generated similarity functions properly combine and weight the best evidence available among the record fields in order to tell when two distinct records represent the same real-world entity. The results of the experiments show that our approach outperforms the baseline method by Fellegi and Sunter by more than 12% when identifying replicas in a data set containing researcher's personal data, and by more than 7%, in a data set with article citation data.},
  keywords={Genetic programming;Software libraries;Computer science;Machine learning;Permission;Writing;Quality management;Optical character recognition software;Books;Information retrieval;deduplication;digital libraries;genetic programming},
  doi={10.1145/1141753.1141760},
  ISSN={},
  month={June},}@BOOK{9650791,
  author={Liu, Chang and Liu, Ying-Hsang and Liu, Jingjing and Bierig, Ralf},
  title={Search Interface Design and Evaluation},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Information seeking and use is now routine in people’s everyday lives. Searching through various information retrieval systems such as web search engines or search functions within information systems allows users to gain access to information on the Internet. Whereas most research in this area has focused on the algorithms behind the search engines from technical perspectives, in this monograph, the authors focus on the search interface, the place where searchers interact with the search system. Search Interface Design and Evaluation reviews the research on the design and evaluation of search user interfaces of the past 10 years. The authors’ primary goal is to integrate state-of-the-art search interface research in the areas of information seeking behavior, information retrieval, and human-computer interaction. The monograph describes the history and background of the development of the search interface and introduces information search behavior models that help conceptualize users’ information needs. The authors also characterize the major components of search interfaces that support different subprocesses based on Marchonini’s information seeking process model, review the design of search interfaces for different user groups, and identify evaluation methods of search interfaces and how they were implemented. Lastly, they provide an outlook on the future trends of search interfaces that includes conversational search interfaces, search interfaces supporting serendipity and creativity, and searching in immersive and virtual reality environments.},
  keywords={},
  doi={},
  ISSN={},
  publisher={now},
  isbn={9781680839234},
  url={https://ieeexplore.ieee.org/document/9650791},}@INPROCEEDINGS{10392932,
  author={Muppidi, Satish},
  booktitle={2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT)},
  title={Recommending High Impact Scholarly Articles using Context-Based Network Analysis},
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Recommending similar articles is one of the essential tools in social network analysis, used to identify relationships that are not else perceived. Commonly, such a system relies on similarity metrics, with the notion that similar pairs are likely to have common preferences. However, traditional methods neither consider imprecise data, binary citation frequencies nor the citation context. The present method proposes a supervised model with 3 features. Three features in specific, link-based doc2vec for assessing authors’ co-cited sentence similarity. Author fetched preprints in the condensed matter under the computer science category based on the category called autism. Our model performs well, with precision, recall values of between 0.78 and 0.67 as this is a real time dataset, and our model provides the top 5 recommendations of articles to the authors.},
  keywords={Measurement;Computer science;Social networking (online);Computational modeling;Network analyzers;Machine learning;Evolutionary computation;Context-based;Link Prediction;Recommendation;precision;co-occurrences},
  doi={10.1109/EASCT59475.2023.10392932},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9637043,
  author={Rezqa, Eman Y. and Baraka, Rebhi S.},
  booktitle={2021 Palestinian International Conference on Information and Communication Technology (PICICT)},
  title={Document Classification Based on Metadata and Keywords Extraction},
  year={2021},
  volume={},
  number={},
  pages={18-24},
  abstract={We present a model for automatic extraction of metadata and keywords to be used in the classification of scientific documents. The model mainly consists of metadata extraction, keywords extraction and documents classification. At the metadata extraction stage, various metadata items are extracted from research documents in the domain of commerce such title of the thesis/research article, author/s, advisor/s, year, publisher, type, and abstract. At the keywords extraction stage, Latent Semantic Indexing (LSI) is used to extract the underlying topics from these documents. At the classification stage which depends on the metadata and keywords extraction stages, three classification algorithms are used which are Stochastic Gradient Descent (SGD), Linear Support Vector (LSVC) and K-Nearest Neighbor (KNN). SGD has achieved the highest classification accuracy (80.5%) compared to LSVC and KNN when applied to Arabic document corpus. LSVC has achieved the highest classification accuracy (81.5%) compared to SGD and KNN when applied to the English document corpus.},
  keywords={Semantics;Neural networks;Support vector machine classification;Stochastic processes;Metadata;Libraries;Large scale integration;metadata extraction;keyword extraction;Arabic classification;research repository;machine learning},
  doi={10.1109/PICICT53635.2021.00016},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{6603923,
  author={Casillas, Luis and Daradoumis, Thanasis and Caballé, Santi},
  booktitle={2013 Seventh International Conference on Complex, Intelligent, and Software Intensive Systems},
  title={A Network Analysis Method for Selecting Personalized Content in e-Learning Programs},
  year={2013},
  volume={},
  number={},
  pages={407-411},
  abstract={Academic programs demand a constant updating. As the reader might agree, today's societies demand a new merge of knowledge elements. Current web-infrastructure is able to support new approaches in science development. On the one hand, due to current social, scientific and technological advances, academic programs in universities are clearly under pressure to be constantly restructured and renewed in order to provide valid and up-to-date knowledge to students. On the other hand, students obviously have different academic needs and backgrounds. So every person needs to follow a specific path in its training process. Our approach aims at proposing a well-ordered method for selecting personalized contents in e-Learning environments through the use of complex network analysis. Due to the similar nature and structure of knowledge and despite the diversity of knowledge types, the proposed method could be easily extrapolated to different academic programs and even account for some diversity in human profiles.},
  keywords={Ontologies;Educational institutions;Complex networks;Electronic learning;Computer science;Proposals;complex network analysis;personal learning environment;academic knowledge;knowledge discovery and management;scale-free networks},
  doi={10.1109/CISIS.2013.74},
  ISSN={},
  month={July},}@INPROCEEDINGS{942180,
  author={Kawano, H.},
  booktitle={Proceedings 2000 Kyoto International Conference on Digital Libraries: Research and Practice},
  title={Overview of Mondou Web search engine using text mining and information visualizing technologies},
  year={2000},
  volume={},
  number={},
  pages={234-244},
  abstract={As the volume of Web pages on the Internet is increasing rapidly, it is becoming hard for users to discover valuable Web resources. It is especially difficult for naive users to discover informative pages by popular Web search engines, since they don't have background and domain knowledge about the status of Web systems. Therefore, many kinds of Web search engines have been developed in order to support the processes of Web information retrieval. We are developing the Japanese Web search engine "Mondou (RCAAU)". Though our engine is one of the first generation of Web search engines, we tried to implement the rapidly emerging technologies of data mining in our search engine from 1995. We are also implementing Java applets based on information visualization. The author presents technical overviews of the Mondou Web search engine. One of the most important techniques is the text mining algorithms based on the primitive association rules. Mondou provides highly relevant feedback keywords to users, in order to support search steps. Using the associative keywords, users can modify the combination of keywords in the initial query. We also introduce the concept of an integrated query mechanism for different search engines based on the KQML agents. Furthermore, in order to visualize the characteristics of search results, we are developing Java applets to display the ROC graph and the clusters of specific documents. We are also trying the improve Web robots for the Mondou system from the view point of data cleaning. Finally, we discuss the effectiveness and performance of our Web search engine.},
  keywords={Web search;Search engines;Text mining;Java;Data visualization;Web pages;Internet;Information retrieval;Data mining;Association rules},
  doi={10.1109/DLRP.2000.942180},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9055955,
  author={Nazir, Shahzad and Asif, Muhammad and Ahmad, Shahbaz},
  booktitle={2020 3rd International Conference on Advancements in Computational Sciences (ICACS)},
  title={Exploring the Proportion of Content Represented by the Metadata of Research Articles},
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={In this era, to find out relevant research articles is considered an important task to track the state-of-the-art-work, and it is termed as research paper recommender system. Considering the massive increase in research corpora, the research community has turned its focus towards finding the most relevant research papers. Researchers have adopted different techniques that are bibliographic information based, content-based, and collaborative filtering based. The most common approach for the research paper recommender system is content-based. According to a survey, 55% of research paper recommender systems use a content-based approach. On the other hand, due to the unavailability of the full text of research papers, researchers started utilizing the Meta-data. But it is still unclear that what proportion of full content can be represented by the Meta-data. This research explored the significant portion of the full content contained by the Metadata of research articles. We applied two different techniques; in the first technique, we implemented the TF-IDF over Metadata and full content and considered the intersection of key terms. Secondly, similarity scores of Meta-data and full content were calculated by applying cosine similarity. This approach was assessed on a dataset of 271 research articles that were automatically downloaded from CiteseerX. The results revealed that the Meta-data of research articles could effectively represent the 47% proportion.},
  keywords={content;meta-data;common words;intersection;similarity score;proportion},
  doi={10.1109/ICACS47775.2020.9055955},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{6245615,
  author={Fiannaca, Antonino and Gaglio, Salvatore and La Rosa, Massimo and Rizzo, Riccardo and Urso, Alfonso},
  booktitle={2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems},
  title={An Intelligent System for Building Bioinformatics Workflows},
  year={2012},
  volume={},
  number={},
  pages={212-218},
  abstract={In this paper a new intelligent system designed to support the researcher in the development of a workflow for bio informatics experiments is presented. The proposed system is capable to suggest one or more strategies in order to resolve the selected problem and to support the user in the assembly of a workflow for complex experiments, using a a Knowledge base, representing the expertise about the application domain, and a Rule-Based system for decision-making activity. Moreover, the system can represent this workflow at different abstraction layers, freeing the user from implementation details and assisting him in the correct configuration of the algorithms. A sample workflow for protein complex extraction from protein-protein interaction network is presented in order to show the main features of the proposed workflow representation.},
  keywords={Ontologies;Decision making;Knowledge based systems;Cognition;Proteins;Bioinformatics;Workflow Generation;Dynamic Treemap Visualization;Knowledge Management;Decision Support System},
  doi={10.1109/CISIS.2012.141},
  ISSN={},
  month={July},}@INPROCEEDINGS{9307781,
  author={López-Acosta, Araceli and García-Hernández, Alejandra and Vázquez-Reyes, Sodel and Mauricio-González, Alejandro},
  booktitle={2020 8th International Conference in Software Engineering Research and Innovation (CONISOFT)},
  title={A Metadata Application Profile to Structure a Scientific Database for Social Network Analysis (SNA)},
  year={2020},
  volume={},
  number={},
  pages={208-215},
  abstract={There are a number of challenges associated to metadata in its different applications including data quality, data acquisition, computing resources, interoperability, and discoverability. This work presents an approach to structure metadata of scientific information for social network analysis based on an academic case study from scientific articles published by universities, to evaluate the area of risk assessment. Studying metadata for scientists' social networks helps identify authors' relevance based on their position within the network. By using Elasticsearch (ES) and Python technologies, this work addresses big data analysis issues related to data structure and volume, given ES full-text search engine capabilities for indexing and searching data, and Python's processing support. The data is obtained from the ArnetMiner (Aminer) open scientific database providing a fresh overview of scientific records up to January 2019. From a sample of 64,070 publications, a total of 45, 000 relations are graphed in a co-authorship network. Through the computation of network centrality measures, this work identifies central-positioned authors, clusters of research, and their affiliations. The results show that degree centrality is an important measure to identify prominent scientists in this co-authorship network, and closeness and betweenness centralities together are dominant measures to pinpoint the key players in the flow of information within the network. We conclude that the application of this approach allows rapid full-text search, visualizing dense co-authorship networks, and identifying central authors through centrality metrics. The results presented in this work can help researchers or research groups identify key research collaborators, multi-disciplinary areas, and international stakeholders.},
  keywords={Metadata;Social networking (online);Databases;Search engines;Interoperability;Internet;Libraries;Scientific Data;Metadata;Elasticsearch;Social Network Analysis},
  doi={10.1109/CONISOFT50191.2020.00038},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{1393960,
  author={Xiaohua Hu and Yoo, I. and Song, I.-Y. and Song, M. and Jianchao Han and Lechner, M.},
  booktitle={2004 Symposium on Computational Intelligence in Bioinformatics and Computational Biology},
  title={Extracting and mining protein-protein interaction network from biomedical literature},
  year={2004},
  volume={},
  number={},
  pages={244-251},
  abstract={We present a biomedical literature data mining system SPIE-DM (Scalable and Portable Information Extraction and Data Mining) to extract and mine the protein-protein interaction network from biomedical literature such as MedLine. SPIE-DM consists of two phases: in phase 1, we develop a scalable and portable ie method (SPIE) to extract the protein-protein interaction from the biomedical literature. These extracted protein-protein interactions form a scale-free network graph. In phase 2, we apply a novel clustering method SFCluster to mine the protein-protein interaction network. The clusters in the network graph represent some potential protein complexes, which are very important for biologist to study the protein functionality. The clustering algorithm considers the characteristics of the scale-free network graphs and is based on the local density of the vertex and its neighborhood functions that can be used to find more meaningful clusters at different density levels. The experiments of SPIE-DM on around 1600 chromatin proteins indicate that our system is very promising for extracting and mining from biomedical literature databases.},
  keywords={Data mining;Databases;Abstracts;Bioinformatics;Protein engineering;Genomics;Clustering methods;Clustering algorithms;Text mining;Natural languages},
  doi={10.1109/CIBCB.2004.1393960},
  ISSN={},
  month={Oct},}@ARTICLE{5228692,
  author={Licsar, Attila and Sziranyi, Tamàs and Kovacs, Làszlò and Pataki, Balàzs},
  journal={IEEE MultiMedia},
  title={A Folk Song Retrieval System with a Gesture-Based Interface},
  year={2009},
  volume={16},
  number={3},
  pages={48-59},
  abstract={This article describes how a folk song retrieval system uses a gesture-based interface to recognize Kodály hand signs and formulate search queries.},
  keywords={Application software;Computer interfaces;Multimedia systems;Humans;Computer applications;Home computing;Cameras;Automation;User interfaces;Education;digital archive;Web-based information search and retrieval;computer vision;vision-based hand-gesture recognition;multimedia and graphics},
  doi={10.1109/MMUL.2009.41},
  ISSN={1941-0166},
  month={July},}@INPROCEEDINGS{8280056,
  author={Costa, Ruben and Figueiras, Paulo and Jardim-Gonçalves, Ricardo and Ramos-Filho, José and Lima, Celson},
  booktitle={2017 International Conference on Engineering, Technology and Innovation (ICE/ITMC)},
  title={Semantic enrichment of product data supported by machine learning techniques},
  year={2017},
  volume={},
  number={},
  pages={1472-1479},
  abstract={The process of transforming big data into understandable information is the key of sustainable innovation within an Industry 4.0 factory. Machine learning techniques and cyber-physical systems are closely related to realize a new thinking of production management and factory transformation. Textual data collected in machinery logs or product documentation, does not exhibit a rich structure which can be easily understandable by both humans and machines. Therefore, data in an unstructured format needs to be enriched and transformed into a representation schema that exhibits a higher degree of structure, before it can be used and shared. The paper, introduces a novel conceptual framework to create knowledge representations from unstructured data sources, based on enriched Semantic Vectors, using a classical vector space model extended with ontological support. Hence, this research explores how traditional knowledge representations can be enriched through incorporation of implicit information derived from the complex relationships (i.e., semantic associations) modelled by domain ontologies with the addition of information presented in documents, addresses the challenges concerning data exchange and its understanding within Industry 4.0 scenarios, when supported by semantic technologies. The proposed approach is validated with industrial examples of product data used in the building and construction domain (e.g., technical specifications concerning climate control, electric power and lighting products) showing its benefits in a real-world use case.},
  keywords={Semantics;Text analysis;Big Data;Interoperability;Ontologies;Big data;machine learning;semantic interoperability;ontologies;product data},
  doi={10.1109/ICE.2017.8280056},
  ISSN={},
  month={June},}@INPROCEEDINGS{10598022,
  author={Papadakis, George and Kirielle, Nishadi and Christen, Peter and Palpanas, Themis},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)},
  title={A Critical Re-evaluation of Record Linkage Benchmarks for Learning-Based Matching Algorithms},
  year={2024},
  volume={},
  number={},
  pages={3435-3448},
  abstract={Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four complementary approaches to assessing the difficulty and appropriateness of 13 commonly used datasets: two theoretical ones, which involve new measures of linearity and existing measures of complexity, and two practical ones - the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most existing benchmark datasets pose rather easy classification tasks. As a result, they are not suitable for properly evaluating learning-based matching algorithms. To address this issue, we propose a new methodology for yielding benchmark datasets. We put it into practice by creating four new matching tasks, and we verify that these new benchmarks are more challenging and therefore more suitable for further advancements in the field.},
  keywords={Deep learning;Couplings;Databases;Aggregates;Linearity;Benchmark testing;Data engineering;Record Linkage;Supervised Matching;Deep Learning;Benchmark Analysis},
  doi={10.1109/ICDE60146.2024.00265},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10695343,
  author={Wang, Hongwei and Ren, Shuaitao and Zhou, Hua and Kong, Chuang},
  booktitle={2024 7th International Conference on Computer Information Science and Application Technology (CISAT)},
  title={Visual Analytic Study on Vehicle Target Detection Driven by CiteSpace},
  year={2024},
  volume={},
  number={},
  pages={469-474},
  abstract={As a critical issue in computer vision and image processing, vehicle target detection plays a vital role in boosting vehicle recognition accuracy and information extraction efficiency, directly influencing applications such as autonomous driving, traffic monitoring, and safety management. This study employs CiteSpace tool to conduct an extensive visualization analysis of 2,641 scientific publications on vehicle target detection from the Web of Science Core Collection between 2019 and 2024, aiming to uncover research dynamics, core topics, collaboration networks, and future trends in this field. By designing a meticulous search strategy, we focus on the most authoritative SCI and SSCI literature and utilize CiteSpace version 6.3.1 for in-depth data mining and analysis. The findings reveal that deep learning, feature extraction, autonomous driving, and intelligent sensing technologies hold central positions in vehicle target detection, with significant contributions from authors like Li, Wei. A prolific research team, led by Yang, Jian, Zhang, Yu, and He, You, has emerged. Keyword analysis discloses that “object detection,” “deep learning,” and “feature extraction” are the most recurrent research foci, while “aerial vehicle” constitutes a persistent research hotspot, indicating the growing importance of drone technology and novel deep learning models, such as “Swin Transformer,” in enhancing detection efficiency and intelligence. Furthermore, the study identifies auto-target recognition and detection techniques in low-observable environments as key areas of interest, with a technological development trend leaning towards more intelligent and efficient solutions. Integration of drone systems and interdisciplinary technology fusion are expected to significantly advance vehicle target detection technology. This research not only quantifies the growth of research activities but also predicts potential future directions through keyword burst analysis, providing valuable insights for the field of vehicle target detection and holding significant theoretical and practical value for the development of intelligent transportation systems.},
  keywords={Deep learning;Target recognition;Collaboration;Object detection;Market research;Feature extraction;Transformers;Sensors;Autonomous vehicles;Drones;Vehicle Target Detection;Machine Learning;Deep Learning;Object Detection;Feature Extraction;Intelligent Sensing;Intelligent Transportation;Swin Transformer;UAV},
  doi={10.1109/CISAT62382.2024.10695343},
  ISSN={},
  month={July},}@INPROCEEDINGS{9080032,
  author={Abbasi, Iqra Iftikhar and Abbas, Muhammad Azeem and Hammad, Shiza and Jilani, Muhammad Taha and Ahmed, Shabbir and Nisa, Saba un},
  booktitle={2020 International Conference on Information Science and Communication Technology (ICISCT)},
  title={A Hybrid Approach for the Recommendation of Scholarly Journals},
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={The increasing number of scholarlyjournals have made it difficult for authors to select the most suitable journal that publishes their research. Existing search systems that recommend journals for manuscript submission are either based on author 's profile, bibliographic data or the copublication network. These approaches are not useful for beginner researchers who have no publication records or for those who are interested in new research domains. The present work proposes a hybrid approach that combines clustering and document similarity for the recommendation of scholarly venues. The proposal was evaluated both objectively and subjectively using domain experts. The results of mean average precision (0.84) and normalized discounted cumulative gain (0.89) shows positive recommendations made by the proposed approach.},
  keywords={recommendation system;journal recommendations;clustering.},
  doi={10.1109/ICISCT49550.2020.9080032},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10678663,
  author={Yu, Lilin and Charlton, Ash and Terras, Melissa and Filgueira, Rosa},
  booktitle={2024 IEEE 20th International Conference on e-Science (e-Science)},
  title={Advancing frances: New Heritage Textual Ontology, Enhanced Knowledge Graphs, and Refined Search Capabilities},
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={This paper presents significant enhancements to the frances platform, incorporating the Heritage Textual Ontology (HTO), advanced knowledge graphs, and sophisticated search capabilities, along with innovative data visualization methods. The HTO integrates diverse historical collections and unifies various sources. Leveraging this ontology, the new knowledge graphs connect data across different sources and editions, linking to external resources like Wikipedia and Dbpedia to enrich semantic relationships. We employed deep-learning-based spell correction for OCR error correction. Enhanced search functionalities, powered by Elasticsearch and semantic technologies, enable precise retrieval and analysis. Additionally, new data visualization approaches offer multifaceted interpretations of search results. A case study tracking slavery references in historical editions of the Encyclopaedia Britannica 1768-1860 demonstrates the platform’s effectiveness in analyzing historical text and validating frances’s capabilities.},
  keywords={Data analysis;Optical character recognition;Data visualization;Knowledge graphs;Encyclopedias;Ontologies;Internet;Heritage Textual Ontology;Knowledge Graphs;Historical Text Analysis;Semantic Linkage;OCR Error Correction;ElasticSearch;Temporal Data Analysis},
  doi={10.1109/e-Science62913.2024.10678663},
  ISSN={2325-3703},
  month={Sep.},}@INPROCEEDINGS{7751743,
  author={Aguiar, Z. Camila and Cury, Davidson},
  booktitle={2016 XI Latin American Conference on Learning Objects and Technology (LACLO)},
  title={A categorization of technological approaches to concept maps construction},
  year={2016},
  volume={},
  number={},
  pages={1-9},
  abstract={Concept maps are resources for the representation and construction of knowledge. They allow the showing, through concepts and relationships, how knowledge about a subject is organized. Technological advances have boosted the development of technological approaches that help the automatic construction of a map, in order to facilitate and provide the benefits of that resource more broadly. Because of the need to better identify and analyze the functionalities and characteristics of the technological approaches inserted in this context, we developed a categorization based on bibliographic review of the area between 1994 and 2015. This work aims to present the proposed categorization and the main features and limitations of the approaches studied observing this categorization.},
  keywords={Context;Data mining;Ontologies;Semantics;Guidelines;Pragmatics;Concept Maps;Concept Map Miner;Knowledge Representation;Natural Language Processing},
  doi={10.1109/LACLO.2016.7751743},
  ISSN={},
  month={Oct},}@ARTICLE{5482578,
  author={Koch, Steffen and Bosch, Harald and Giereth, Mark and Ertl, Thomas},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  title={Iterative Integration of Visual Insights during Scalable Patent Search and Analysis},
  year={2011},
  volume={17},
  number={5},
  pages={557-569},
  abstract={Patents are of growing importance in current economic markets. Analyzing patent information has, therefore, become a common task for many interest groups. As a prerequisite for patent analysis, extensive search for relevant patent information is essential. Unfortunately, the complexity of patent material inhibits a straightforward retrieval of all relevant patent documents and leads to iterative, time-consuming approaches in practice. Already the amount of patent data to be analyzed poses challenges with respect to scalability. Further scalability issues arise concerning the diversity of users and the large variety of analysis tasks. With "PatViz”, a system for interactive analysis of patent information has been developed addressing scalability at various levels. PatViz provides a visual environment allowing for interactive reintegration of insights into subsequent search iterations, thereby bridging the gap between search and analytic processes. Because of its extensibility, we expect that the approach we have taken can be employed in different problem domains that require high quality of search results regarding their completeness.},
  keywords={Scalability;Information analysis;Data analysis;Information retrieval;Intellectual property;Visualization;Environmental economics;Iterative methods;Graphical user interfaces;Statistics;Visual analytics;graphical user interfaces;information search and retrieval.},
  doi={10.1109/TVCG.2010.85},
  ISSN={1941-0506},
  month={May},}@BOOK{8187148,
  author={Melucci, Massimo},
  title={Contextual Search: A Computational Framework},
  year={2012},
  volume={},
  number={},
  pages={},
  abstract={The growing availability of data in electronic form, the expansion of the World Wide Web and the accessibility of computational methods for large-scale data processing have allowed researchers in Information Retrieval (IR) to design systems which can effectively and efficiently constrain search within the boundaries given by context, thus transforming classical search into contextual search. Contextual Search: A Computational Framework introduces contextual search within a computational framework based on contextual variables, contextual factors and statistical models. It describes how statistical models can process contextual variables to infer the contextual factors underlying the current search context. It also provides background to the subject by: placing it among other surveys on relevance, interaction, context, and behaviour; providing a description of the contextual variables used for implementing the statistical models which represent and predict relevance and contextual factors; and providing an overview of the evaluation methodologies and findings relevant to this subject. Contextual Search: A Computational Framework is a highly recommended read, both for beginners who are embarking on research in this area and as a useful reference for established IR researchers.},
  keywords={},
  doi={10.1561/1500000023},
  ISSN={},
  publisher={now},
  isbn={9781601986016},
  url={https://ieeexplore.ieee.org/document/8187148},}@ARTICLE{5962053,
  author={Olsen, Kai A. and Indredavik, B.},
  journal={IEEE Potentials},
  title={A proofreading tool using brute force techniques},
  year={2011},
  volume={30},
  number={4},
  pages={18-22},
  abstract={Proofreading is both a tedious and difficult task. It is expensive to hire a professional to do the job, and it may be also difficult to find someone competent. Newspapers skip proofreaders to save money, and most writers try to do the job themselves. The consequences are clearly seen from errors in text written by professionals to incomprehensive e-mails and blogs. This creates misunderstandings, offers false data, and, in the worst cases, may give a very bad impression.},
  keywords={Professional communication;Career developmnet;Grammar;Search engines;Electronic mail},
  doi={10.1109/MPOT.2011.940378},
  ISSN={1558-1772},
  month={July},}@BOOK{8187206,
  author={Bast, Hannah and Buchhold, Björn and Haussmann, Elmar},
  title={Semantic Search on Text and Knowledge Bases},
  year={2016},
  volume={},
  number={},
  pages={},
  abstract={This monograph provides a comprehensive overview of the broad area of semantic search on text and knowledge bases. In a nutshell, semantic search is "search with meaning". This "meaning" can refer to various parts of the search process: understanding the query (instead of just finding matches of its components in the data), understanding the data (instead of just searching it for such matches), or representing knowledge in a way suitable for meaningful retrieval. Semantic search is studied in a variety of different communities with a variety of different views of the problem. Semantic Search on Text and Knowledge Bases classifies this work according to two dimensions: the type of data (text, knowledge bases, combinations of these) and the kind of search (keyword, structured, natural language). All nine combinations are considered. The focus is on fundamental techniques, concrete systems, and benchmarks. The monograph also considers advanced issues: ranking, indexing, ontology matching and merging, and inference. It also provides a succinct overview of fundamental natural language processing techniques: POS-tagging, named-entity recognition and disambiguation, sentence parsing, and distributional semantics. Semantic Search on Text and Knowledge Bases is as self-contained as possible, and should thus also serve as a good tutorial for newcomers to this fascinating and highly topical field.},
  keywords={Indexing and retrieval of structured documents;Information extraction;Natural language processing for IR;Question Answering;Text Mining;Semantic Web},
  doi={10.1561/1500000032},
  ISSN={},
  publisher={now},
  isbn={9781680831658},
  url={https://ieeexplore.ieee.org/document/8187206},}@ARTICLE{9826729,
  author={Manzoor, Ayesha and Asghar, Sohail and Amjad, Tehmina},
  journal={IEEE Access},
  title={Toward a New Paradigm for Author Name Disambiguation},
  year={2022},
  volume={10},
  number={},
  pages={76055-76068},
  abstract={Author Name Disambiguation (AND) has emerged as a significant challenge in the bibliometric context with the growing volume of scientific literature. When citations written by different authors have the same names (polysemy or homonym names), and when an author has different names, there is ambiguity (synonyms or name variants). It is difficult to associate a citation with the correct author. Polysemy and synonyms cause merging and splitting anomalies in the citations. These anomalies affect the quantification of an author’s productivity (bibliometric analysis) and the reliability and quality of the information retrieved. Many techniques for AND have been proposed in the literature; most of them do not go beyond string matching or text matching. Most of the existing work do not consider the context or semantics of the terms used in the citations. In this study, the AND problem is resolved semantically using the deep learning technique on the PubMed dataset. The experimental results show that the proposed method achieves overall (11.72 %, 12.5 %, and 12.1 %) higher precision, recall, and f-measure than the pairwise class classification.},
  keywords={Semantics;Deep learning;Convolutional neural networks;Training;Support vector machines;Libraries;Collaboration;Author name disambiguation;digital library;deep learning;classification;word embedding;journal descriptor;semantic type},
  doi={10.1109/ACCESS.2022.3190088},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9943876,
  author={Godoy, Carlo H. and Diego, Nicole Jehru R. and Tagumasi, Russel E. and Lerit, Jordan C. and Costales, Jefferson A.},
  booktitle={2022 5th International Conference on Data Science and Information Technology (DSIT)},
  title={Cybersecurity Scientometric Analysis: Mapping of Scientific Articles using Scopus API for Data Mining and Webscrapping},
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={In terms of computers, cybersecurity has experienced massive technological and operational transformations in recent years, with data science at the forefront. Cybersecurity in the Philippines is still believed to be in its infancy. Though it is commonplace in other areas of the world, just a few scholars in the Philippines have dared to concentrate in this field. As a result, a need for assistance in promoting cybersecurity in the Philippines must be identified. Because it is difficult to persuade scholars in the Philippines to work on a certain field of computing without figures, Scientometric Analysis is required to persuade scholars in the Philippines to handle cybersecurity. The findings of this study revealed that there is no link between the frequency with which authors publish cybersecurity and their chances of receiving a high number of citations, either by year or overall. The study also found that having a lot of publications does not ensure a higher Google Scholar ranking. According to the study, the most interesting themes in the field of cybersecurity are to connect it with another discipline, thus an author should choose an attractive topic in this field to acquire a higher rank in Googlescholar and especially in Scopus. Machine Learning and IoT are the two fields that most people want to see integrated into cybersecurity. Hence, this topics can be used to encourage cybersecurity experts in the Philippines to used this topic on their research or speaking engagements.},
  keywords={Computers;Bibliometrics;Machine learning;Data science;Internet;Data mining;Computer security;cybersecurity;data mining;webscrapping;scientometric analysis;scientific mapping},
  doi={10.1109/DSIT55514.2022.9943876},
  ISSN={},
  month={July},}@INBOOK{6279992,
  author={Braman, Sandra},
  booktitle={Change of State: Information, Policy, and Power},
  title={Bibliographic Essays},
  year={2006},
  volume={},
  number={},
  pages={329-418},
  abstract={This chapter contains sections titled: An Introduction to Information Policy, Forms and Phases of Power: The Bias of the Informational State, Bounding the Domain: Information Policy for the Twenty-first Century, Constitutional Principles and the Information Spaces They Create, Information Policy and Identity, Information Policy and Structure, Information Policy and Borders, Information Policy and Change},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262255813},
  url={https://ieeexplore.ieee.org/document/6279992},}@INPROCEEDINGS{6261069,
  author={Huynh, Tin and Hoang, Kiem and Do, Loc and Tran, Huong and Luong, Hiep and Gauch, Susan},
  booktitle={2012 International Conference on Collaboration Technologies and Systems (CTS)},
  title={Scientific publication recommendations based on collaborative citation networks},
  year={2012},
  volume={},
  number={},
  pages={316-321},
  abstract={To learn about the state of the art for a research project, researchers must conduct a literature survey by searching for, collecting, and reading related scientific articles. Popular search systems, online digital libraries, and Web of Science (WoS) sources such as IEEE Explorer, ACM, SpringerLink, and Google Scholar typically return results or articles that are similar to keywords in the user's query. Some digital libraries also include content-based recommenders that suggest papers similar to one the user likes based on the contents of paper, i.e., the keywords it contains. In this work, we present a recommender module that suggests papers to users based on the seed paper's Citation Network. This work takes into account the combination of the co-citation and co-reference factors to improve algorithm's effectiveness. We applied and improved the the CCIDF (Common Citation Inverse Document Frequency) algorithm used by the CiteSeer digital library. This improved algorithm, called CCIDF+, was evaluated using data collected from Microsoft Academic Search (MAS). Experimental results show that CCIDF+ outperforms CCIDF.},
  keywords={Recommender systems;Collaboration;Libraries;Computer science;Databases;Social network services;Educational institutions;Adaptive Networks for Collaboration;Support for Adaptive Collaboration;Recommender Systems;Related Publication;Citation Network;CCIDF},
  doi={10.1109/CTS.2012.6261069},
  ISSN={},
  month={May},}@INPROCEEDINGS{6690734,
  author={Nguyen, Hung Son},
  booktitle={2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)},
  title={Tolerance Rough Set Model and Its Applications in Web Intelligence},
  year={2013},
  volume={3},
  number={},
  pages={237-244},
  abstract={Tolerance Rough Set Model (TRSM) has been introduced as a tool for approximation of hidden concepts in text databases. In recent years, numerous successful applications of TRSM in web intelligence including text classification, clustering, thesaurus generation, semantic indexing, and semantic search, etc., have been proposed. This paper will review the fundamental concepts of TRSM, some of its possible extensions and some typical applications of TRSM in text mining. Moreover, the architecture o a semantic information retrieval system, called SONCA, will be presented to demonstrate the main idea as well as stimulate the further research on TRSM.},
  keywords={Semantics;Vectors;Approximation methods;Indexes;Standards;Information retrieval;Ontologies;Tolerance rough set model;semantic indexing;semantic search;classification;clustering},
  doi={10.1109/WI-IAT.2013.189},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8929808,
  author={Lytvyn, Vasyl and Vysotska, Victoria and Peleshchak, Ivan and Basyuk, Taras and Kovalchuk, Viktoriia and Kubinska, Solomiya and Rusyn, Bohdan and Pohreliuk, Liubomyr and Chyrun, Lyubomyr and Salo, Tetyana},
  booktitle={2019 IEEE 14th International Conference on Computer Sciences and Information Technologies (CSIT)},
  title={Identifying Textual Content Based on Thematic Analysis of Similar Texts in Big Data},
  year={2019},
  volume={2},
  number={},
  pages={84-91},
  abstract={The system designed for working with artistic texts in English is developed. It provides the opportunity to obtain the necessary information for the user, that is, the ordered data in the format Author - Title on the artwork, which correspond to the thematic query. This software tool operates to identify a plurality of content based on thematic analysis of texts. It is used as an application on a PC. Except for the main function the application does not perform additional actions. It consists of a part that serves as a user interface and a database that is located on the server. The input is an English-language text snippet that is similar to what the user wants to find. Output is a list of the names of works and the names of their authors, sorted by descending similarity.},
  keywords={Semantics;Ontologies;Search problems;Metadata;Libraries;Task analysis;Information systems;content;information retrieval;content search;content analysis;similar content},
  doi={10.1109/STC-CSIT.2019.8929808},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9664064,
  author={Ji, Yong},
  booktitle={2021 7th International Conference on Systems and Informatics (ICSAI)},
  title={Intelligent Patent Text Summarization Analysis Method},
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Patent mining and patent analysis of patented technologies will help protect the interests of intellectual property rights and provide enterprises with correct scientific research directions. In order to study the profitable patents of pharmaceutical companies, this paper proposes an Abstractive RL-LSTM neural network method based on patent texts. The reinforcement learning method is introduced into LSTM. The purpose is to rely on Q-learning to learn the relationship between the main layers. The two parallel layers share the weight of attention from the Q value, and realize the hierarchical control between the LSTM structure of the patent document and the LSTM structure of the sentence. The experimental results show that compared with other methods, the method proposed in this paper can further improve the ROUGE index and alleviate the dependence of the decoder on the input.},
  keywords={Patents;Q-learning;Neural networks;Companies;Intellectual property;Decoding;Computational efficiency;Patent analysis;Text summarization;RL-LSTM;Machine learning},
  doi={10.1109/ICSAI53574.2021.9664064},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8689125,
  author={Nazir, Shahzad and Asif, Muhammad and Ahmad, Shahbaz},
  booktitle={2019 2nd International Conference on Advancements in Computational Sciences (ICACS)},
  title={The Evolution of Trends and Techniques used for Data Mining},
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Data mining has got the festivity among the most emerging fields of the current epoch. Extensive data is being generated therefore the need of the hour is to mine the interesting trends and patterns. For mining such trends, different techniques and algorithms are introduced and applied during the last decades. In this paper, we are extracting the essence from massive text corpora in the form of potential techniques used in the esteemed field of data mining. For the purpose of exploring the trends, we considered the time span from 2014 to 2018. For this purpose, a novel dataset comprising of the abstracts and metadata of 5,843 articles is procured. All the extracted contents of data mining are published in esteemed journals indexed by ScienceDirect. Text mining techniques including noun phrase mining and TF-IDF are employed to reveal the evolution of hot trends in the field of data mining during the underlying span. For better visualization, the year-wise and overall evolution of these hot trends is presented in the form of the word cloud. Machine learning algorithms are found as the shoulders of the giants, on which, data mining domain is standing.},
  keywords={interesting trends;data mining;text mining;text corpora;evolution},
  doi={10.23919/ICACS.2019.8689125},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{4746740,
  author={Yuzana and Tun, Khin Marlar},
  booktitle={2008 Third International Conference on Digital Information Management},
  title={A comparison of collation algorithm for Myanmar language},
  year={2008},
  volume={},
  number={},
  pages={538-543},
  abstract={Myanmar language has no white spaces and word boundary. There is lack of support in Unicode database application such as collation and searching. Powerful collation strategy has necessitated to the all embracing research in the locality of natural language processing. Consequently, we propose a new collation algorithm MyCollate2 extend from MyCollate1 for Myanmar language. This collation algorithm is based on heuristics chart or table. This method foremost slices the syllables of names and then collates them according to the traditional standard Myanmar language dictionary book order. Propose new heuristics chart can work well not only for syllable segmentation but also for collation of words. This algorithm can collate Myanmar names as well as Myanmar words with complex syllable structure such as Pali, Pali loan styles, subscript styles and kinzi styles. This paper tested with Myanmar name, Pali words from Damma books and dictionary words from dictionary book. The experimental result shows that syllable slicing accuracy get 99.55% compare with others and show slicing performance. Collation accuracy gets 95.88% and is better accuracy than previous collation algorithm MyCollate1.},
  keywords={Books;Dictionaries;Natural language processing;Sorting;Natural languages;Clustering algorithms;Databases;Libraries;Information retrieval;White spaces},
  doi={10.1109/ICDIM.2008.4746740},
  ISSN={},
  month={Nov},}@ARTICLE{1024761,
  author={Griffiths, J.-M. and King, D.W.},
  journal={IEEE Annals of the History of Computing},
  title={US information retrieval system evolution and evaluation (1945-1975)},
  year={2002},
  volume={24},
  number={3},
  pages={35-55},
  abstract={Information retrieval has evolved through four phases: manual and mechanical devices; off line computing; online computing and vendor access; and distributed, networked, and mass computing. The article primarily addresses the first three phases. We examine IR systems in terms of four basic functions within a broader communications system context: analysis of document information content including description of document elements (such as author, title, volume, and so on), abstracting, indexing, and other processes; identification and location of sources of documents through bibliographic browsing and searching; evaluation and assessment of bibliographic search output such as screening of bibliographic descriptions; and provision of physical access to document information content through various media such as print on paper, microform, and screen displays.},
  keywords={Information retrieval;Computer networks;Distributed computing;Databases;Optical computing;Technological innovation;Boolean functions;Encoding;Optical devices;Sorting},
  doi={10.1109/MAHC.2002.1024761},
  ISSN={1934-1547},
  month={July},}@INPROCEEDINGS{8125290,
  author={Ranaei, Samira and Suominen, Arho},
  booktitle={2017 Portland International Conference on Management of Engineering and Technology (PICMET)},
  title={Using Machine Learning Approaches to Identify Emergence: Case of Vehicle Related Patent Data},
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={Bibliometric studies have long used simple search strings, publications count, and word counts to track the emergence of technologies. Novel machine learning methods open new possibilities to study bibliometric data and use algorithmic approaches to uncover emergence of a technology. This study looks at the large and complex dataset of vehicle related patents to uncover emergence indicators. By using machine learning methods this study focuses on if, and to what extent different methods can produce patterns of emergence from the data directly. The data extracted from PATSTAT contains 711296 granted US patent abstracts between the years 1980 and 2014 resulting from a search for "vehicle" creating a complex dataset of technologies from automotive to medical applications. Using Latent Dirichlet Allocation and Dynamic Topic Modeling we show different emergence patterns. Finally, we discuss in detail the possibilities of using machine learning approaches to draw emergence dynamics of technologies.},
  keywords={Patents;Bibliometrics;Predictive models;Technological innovation;Databases;Data mining},
  doi={10.23919/PICMET.2017.8125290},
  ISSN={},
  month={July},}@ARTICLE{10558749,
  author={Alcayde, Alfredo and Ventura, Jorge and Montoya, Francisco G.},
  journal={IEEE Signal Processing Magazine},
  title={Hypercomplex Techniques in Signal and Image Processing Using Network Graph Theory: Identifying core research directions [Hypercomplex Signal and Image Processing]},
  year={2024},
  volume={41},
  number={2},
  pages={14-28},
  abstract={This article aims to identify core research directions and provide a comprehensive overview of major advancements in the field of hypercomplex signal and image processing techniques using network graph theory. The methodology employs community detection algorithms on research networks to uncover relationships among researchers and topic fields in the hypercomplex domain. This is accomplished through a comprehensive academic database search and metadata analysis from pertinent papers. The article focuses on the utility of these techniques in various applications and the value of mathematically rich frameworks. The results demonstrate how optimized network-based approaches can determine common topics and emerging lines of research. The article identifies distinct core research directions, including significant advancements in image/video processing, computer vision, signal processing, security, navigation, and machine learning within the hypercomplex domain. Current trends, challenges, opportunities, and the most promising directions in hypercomplex signal and image processing are highlighted based on a thorough literature analysis. This provides actionable insights for researchers to advance this domain.},
  keywords={Surveys;Computer vision;Navigation;Algebra;Image processing;Quaternions;Research initiatives;Complexity theory;Metadata;Market research;Machine learning;Detection algorithms;Databases;Graph theory},
  doi={10.1109/MSP.2024.3365463},
  ISSN={1558-0792},
  month={June},}@ARTICLE{9919162,
  author={Rizvi, Syed and Scanlon, Mark and Mcgibney, Jimmy and Sheppard, John},
  journal={IEEE Access},
  title={Application of Artificial Intelligence to Network Forensics: Survey, Challenges and Future Directions},
  year={2022},
  volume={10},
  number={},
  pages={110362-110384},
  abstract={Network forensics focuses on the identification and investigation of internal and external network attacks, the reverse engineering of network protocols, and the uninstrumented investigation of networked devices. It lies at the intersection of digital forensics, incident response and network security. Network attacks exploit software and hardware vulnerabilities and communication protocols. The scope of a network forensic investigation can range from Internet-wide down to a single device’s network traffic. Network analysis tools (NATs) aid security professionals and law enforcement in the capturing, identification and analysis of network traffic. However, in most instances, the sheer volume of data to be analyzed is enormous and, despite some built-in NAT automation, the investigation of network traffic is often an arduous process. Furthermore, significant expert time remains wasted in the investigation of a high frequency of false positive alerting from automated systems. To address this globally impacting problem, artificial intelligence based approaches are becoming increasingly employed to automatically detect attacks and increase network traffic classification accuracy. This paper provides a comprehensive survey of the state-of-the-art in network forensics and the application of expert systems, machine learning, deep learning, and ensemble/hybrid approaches to a range of application areas in the field. These include network traffic analysis, intrusion detection systems, Internet-of-Things devices, cloud forensics, DNS tunneling, smart grid forensics, and vehicle forensics. In addition, the current challenges and future research directions for each of the aforementioned application areas is discussed.},
  keywords={Forensics;Artificial intelligence;Telecommunication traffic;Internet of Things;Computer crime;Security;Digital forensics;Computer security;Network forensics;artificial intelligence;cybersecurity;digital forensics},
  doi={10.1109/ACCESS.2022.3214506},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{6644129,
  author={Pascalau, Emilian and Nalepa, Grzegorz J. and Kluza, Krzysztof},
  booktitle={2013 Federated Conference on Computer Science and Information Systems},
  title={Towards a better understanding of context-aware applications},
  year={2013},
  volume={},
  number={},
  pages={959-962},
  abstract={With the new technological advances and strong move towards Future Internet and Internet as a Platform a new environment is emerging. This environment is generative, social, strongly interactive and collaborative, so users play a fundamental role in it. Business applications are simplifying, webifying and getting more user-centric. In this environment, context and context-awareness plays a fundamental role, as context gives meaning and accurately describes the situation of an user. This paper introduces the basis for a new research methodology that aims to address and visualize the topic of context and context-awareness from a holistic point of view, by means of text mining and text clustering.},
  keywords={Context;Business;Context modeling;Computational modeling;Context-aware services;Clustering algorithms;Text mining},
  doi={},
  ISSN={},
  month={Sep.},}@ARTICLE{10251928,
  author={Ahuja, Babita and Pillai, Anuradha and Punj, Deepika and Verma, Jyoti},
  journal={Journal of Web Engineering},
  title={Dynamic Query Processing for Hidden Web Data Extraction From Academic Domain},
  year={2020},
  volume={19},
  number={7–8},
  pages={931-970},
  abstract={The web documents lying on WWW can be classified as hidden web and surface web. The web documents from surface web are indexable as well as crawlable by the search engines and hence they can be displayed to users as per their input query. In contrast to this, hidden web documents are neither indexable nor crawlable by the traditional search engines due to disconnected URL's, no-index tag, user authentication, web form processing. Also, since the information is scattered across multiple web pages, users find it difficult to hop between multiple pages to find the desired information. Hence, there is dire need of hidden web crawlers which could extract the data from hidden web databases and uncover this big part of WWW. In this research, a novel framework “Dynamic Query Processing for Hidden Web Data Extraction (DQPHDE)” has been proposed to extract such hidden web data and integrate it with the data from surface web to meet user's requirements. DQPHDE makes use of clustering, semantic based text mining and fuzzy rule based system to carry out the desired task. The results of the proposed work were compared with the existing academic search engines like ‘Microsoft Academic’ and ‘Academia.edu’ etc, and our proposed work outperforms them in fetching the information and then integrating the related information for other pages.},
  keywords={Text mining;Databases;Query processing;Aggregates;Semantics;Knowledge based systems;Web pages;Surface web;hidden web;dynamic query processing;text summarization;semantic fuzzy rules},
  doi={10.13052/jwe1540-9589.19782},
  ISSN={1544-5976},
  month={November},}@ARTICLE{10729241,
  author={Kheddar, Hamza and Dawoud, Diana W. and Awad, Ali Ismail and Himeur, Yassine and Khan, Muhammad Khurram},
  journal={IEEE Communications Surveys & Tutorials},
  title={Reinforcement-Learning-Based Intrusion Detection in Communication Networks: A Review},
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={Modern communication networks have to meet the performance requirements of contemporary industrial control systems (ICSs), which are increasingly being connected to the external Internet. This connectivity exposes them to vulnerabilities that necessitate timely and effective protection measures. The integration of intrusion-detection systems (IDSs) into communication networks serves as a preventive mechanism to defend against malicious threats and hostile activities, ensuring secure operations within the broader industrial infrastructure. This review explores the cutting-edge artificial-intelligence techniques that are employed in the development of IDSs for diverse industrial control networks, emphasizing the application of deep reinforcement learning (DRL) within IDS-based systems across various communication networks. DRL has been successful in solving complex sequential decision-making problems in various domains, including robotics, game playing, and natural-language processing. The review examines a broad scope of publications, and these are categorized into three groups: DRL-only and IDS-only in the introduction and background, and DRL-based IDS papers in the core section of the review. This seeks to provide researchers with an overview of the current state of DRL approaches in IDSs for various network types. Through a meticulous comparative analysis with existing surveys, our review stands out, emphasizing its uniqueness and comprehensiveness. This inclusivity extends beyond traditional boundaries, encompassing a wide array of IDS techniques and environments, ranging from the Internet of Things to ICSs, smart grids, and other domains. Additionally, this review provides useful information such as the datasets used, types of DRL employed, pretrained networks, IDS techniques, evaluation metrics, and improvements gained. Furthermore, the algorithms and methods used in several studies are presented to illustrate the principles of each DRL-based IDS subcategory clearly and in depth. A detailed taxonomy is presented, providing nuanced insights into diverse applications with a triple focus on IDSs, deep-learning, and DRL techniques, which makes this review unique.},
  keywords={Protocols;Communication networks;Security;Q-learning;Internet of Things;Reviews;Monitoring;Wireless sensor networks;Surveys;Ultra reliable low latency communication;Intrusion-detection systems;machine learning;deep learning;reinforcement learning;Internet of Things (IoT) security;industrial control systems security},
  doi={10.1109/COMST.2024.3484491},
  ISSN={1553-877X},
  month={},}@ARTICLE{9211449,
  author={Oprescu, Andreea M. and Miró-amarante, Gloria and García-Díaz, Lutgardo and Beltrán, Luis M. and Rey, Victoria E. and Romero-Ternero, MCarmen},
  journal={IEEE Access},
  title={Artificial Intelligence in Pregnancy: A Scoping Review},
  year={2020},
  volume={8},
  number={},
  pages={181450-181484},
  abstract={Artificial Intelligence has been widely applied to a majority of research areas, including health and medicine. Certain complications or disorders that can appear during pregnancy can endanger the life of both mother and fetus. There is enough scientific literature to support the idea that emotional aspects can be a relevant risk factor in pregnancy (such as anxiety, stress or depression, for instance). This paper presents a scoping review of the scientific literature from the past 12 years (2008-2020) to identify which methodologies, techniques, algorithms and frameworks are used in Artificial Intelligence and Affective Computing for pregnancy health and well-being. The methodology proposed by Arksey and O'Malley, in conjunction with PRISMA-ScR framework has been used to create this review. Despite the relevance that emotional status can have as a risk factor during pregnancy, one of the main findings of this study is that there is still not a significant amount of literature on automatic analysis of emotion. Health enhancement and well-being for pregnant women can be achieved with artificial intelligence or affective computing based devices, hence future work on this topic is strongly suggested.},
  keywords={Pregnancy;Affective computing;Signal to noise ratio;Fetus;Psychology;Artificial intelligence;affective computing;pregnancy health;pregnancy well-being;machine learning;methodology;algorithm;framework;IT security;data privacy},
  doi={10.1109/ACCESS.2020.3028333},
  ISSN={2169-3536},
  month={},}@ARTICLE{9207913,
  author={Saeed, Imtithal A. and Selamat, Ali and Rohani, Mohd Foad and Krejcar, Ondrej and Chaudhry, Junaid Ahsenali},
  journal={IEEE Access},
  title={A Systematic State-of-the-Art Analysis of Multi-Agent Intrusion Detection},
  year={2020},
  volume={8},
  number={},
  pages={180184-180209},
  abstract={Multi-agent architectures have been successful in attaining considerable attention among computer security researchers. This is so, because of their demonstrated capabilities such as autonomy, embedded intelligence, learning and self-growing knowledge-base, high scalability, fault tolerance, and automatic parallelism. These characteristics have made this technology a de facto standard for developing ambient security systems to meet the open and dynamic nature of today's online communities. Although multi-agent architectures are increasingly studied in the area of computer security, there is still not enough empirical evidence on their performance in intrusions and attacks detection. The aim of this paper is to report the systematic literature review conducted in the context of specific research questions, to investigate multi-agent IDS architectures to highlight the issues that affect their performance in terms of detection accuracy and response time. We used pertinent keywords and terms to search and retrieve the most recent research studies, on multi-agent IDS architectures, from the major research databases and digital libraries such as SCOPUS, Springer, and IEEE Explore. The search processes resulted in a number of studies; among them, there were journal articles, book chapters, conference papers, dissertations, and theses. The obtained studies were assessed and filtered out, and finally, there were over 71 studies chosen to answer the research questions. The results of this study have shown that multi-agent architectures include several advantages that can help in the development of ambient IDS. However, it has been found that there are several issues in the current multi-agent IDS architectures that may degrade the accuracy and response time of intrusions and attacks detection. Based on our findings, the issues of multi-agent IDS architectures include limitations in the techniques, mechanisms, and schemes used for multi-agent IDS adaptation and learning, load balancing, scalability, fault-tolerance, and high communication overhead. It has also been found that new measurement metrics are required for evaluating multi-agent IDS architectures.},
  keywords={Computer architecture;Intrusion detection;Systematics;Bibliographies;Computer networks;Computer security;Multi-agent;IDS architectures;intrusion detection;attacks;review;malware;cyberphysical system},
  doi={10.1109/ACCESS.2020.3027463},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8511215,
  author={Dörpinghaus, Jens and Darms, Johannes and Jacobs, Marc},
  booktitle={2018 Federated Conference on Computer Science and Information Systems (FedCSIS)},
  title={What was the Question? a Systematization of Information Retrieval and NLP Problems},
  year={2018},
  volume={},
  number={},
  pages={471-478},
  abstract={In this paper we suggest a novel systematization of Information Retrieval and Natural Language Processing problems. Using this rather general description of problems we are able to discuss and proof the equivalence of some problems. We provide reformulations of well-known problems like Named Entity Recognition using our novel description and discuss further research and the expected outcome. We will discuss the relation of two problems, cluster labeling and search query finding. With these results we are able to provide a novel optimization approach to both problems. This novel systematization approach provides a yet unknown view generating new classes of problems in NLP. It brings application and algorithmic approaches together and offers a better description with concepts of theoretical computer science.},
  keywords={Standards;Natural language processing;Semantics;Search engines;Search problems;Optimization},
  doi={},
  ISSN={},
  month={Sep.},}@ARTICLE{9780569,
  author={Wang, Xiao and Bo, Deyu and Shi, Chuan and Fan, Shaohua and Ye, Yanfang and Yu, Philip S.},
  journal={IEEE Transactions on Big Data},
  title={A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources},
  year={2023},
  volume={9},
  number={2},
  pages={415-436},
  abstract={Heterogeneous graphs (HGs) also known as heterogeneous information networks have become ubiquitous in real-world scenarios; therefore, HG embedding, which aims to learn representations in a lower-dimension space while preserving the heterogeneous structures and semantics for downstream tasks (e.g., node/graph classification, node clustering, link prediction), has drawn considerable attentions in recent years. In this survey, we perform a comprehensive review of the recent development on HG embedding methods and techniques. We first introduce the basic concepts of HG and discuss the unique challenges brought by the heterogeneity for HG embedding in comparison with homogeneous graph representation learning; and then we systemically survey and categorize the state-of-the-art HG embedding methods based on the information they used in the learning process to address the challenges posed by the HG heterogeneity. In particular, for each representative HG embedding method, we provide detailed introduction and further analyze its pros and cons; meanwhile, we also explore the transformativeness and applicability of different types of HG embedding methods in the real-world industrial environments for the first time. In addition, we further present several widely deployed systems that have demonstrated the success of HG embedding techniques in resolving real-world application problems with broader impacts. To facilitate future research and applications in this area, we also summarize the open-source code, existing graph learning platforms and benchmark datasets. Finally, we explore the additional issues and challenges of HG embedding and forecast the future research directions in this field.},
  keywords={Semantics;Benchmark testing;Open source software;Mercury (metals);Fuses;Big Data;Telecommunications;Heterogeneous graph;graph embedding;machine learning;deep learning},
  doi={10.1109/TBDATA.2022.3177455},
  ISSN={2332-7790},
  month={April},}@ARTICLE{9635770,
  author={Costa Junior, Evilasio and Andrade, Rossana Maria De Castro and Rocha, Leonardo S. and Taramasco, Carla and Ferreira, Leonardo},
  journal={IEEE Access},
  title={Computational Solutions for Human Falls Classification},
  year={2021},
  volume={9},
  number={},
  pages={161590-161602},
  abstract={In the last two decades, studies about using technology for automatic detection of human falls increased considerably. The automatic detection of falls allows for quicker aid that is key to increasing the chances of treatment and mitigating the consequences of falls. However, each type of fall has its specificities and determining the correct type of fall can help treat the person who has fallen. Although it is essential to use computational methods to classify falls, there are few studies about that in the literature, especially compared to the studies that propose solutions for fall detection. In this sense, we execute a systematic literature review (SLR) using the (Kitchenham et al., 2009) method to investigate the computational solutions used to classify the different types of falls. We performed a search on Scopus, Web of Science, and PubMed scientific databases looking for computational methods to fall classification in their papers. We use the grounded theory methodology for a more detailed qualitative analysis of the papers. As a result of our search, we selected a total of 36 studies for our review and found two different computational methods for classifying falls. Related to the steps used in each method, we found fourteen different types of sensors, four different techniques for background and foreground extraction of videos, twenty-one techniques for feature extraction, and seven different fall classification strategies. Finally, we also identified fifty-one different types of falls. In conclusion, we believe that the methods and techniques analyzed in our study can help developers to create new and better systems for classification, detection, and prevention of falls and falls database. Besides, we identified gaps that can be explored in future research related to the automatic classification of falls.},
  keywords={Databases;Systematics;Statistics;Sociology;Fall detection;Protocols;Search problems;Automated falls;classification algorithms;e-health;falls;falls classification;types of falls},
  doi={10.1109/ACCESS.2021.3132796},
  ISSN={2169-3536},
  month={},}@BOOK{9647702,
  author={Bain, Ken},
  title={Super Courses: The Future of Teaching and Learning},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={From the bestselling author of What the Best College Teachers Do, the story of a new breed of amazingly innovative courses that inspire students and improve learningDecades of research have produced profound insights into how student learning and motivation can be unleashed—and it’s not through technology or even the best of lectures. In Super Courses, education expert and bestselling author Ken Bain tells the fascinating story of enterprising college, graduate school, and high school teachers who are using evidence-based approaches to spark deeper levels of learning, critical thinking, and creativity—whether teaching online, in class, or in the field.Visiting schools across the United States as well as in China and Singapore, Bain, working with his longtime collaborator, Marsha Marshall Bain, uncovers super courses throughout the humanities and sciences. At the University of Virginia, undergrads contemplate the big questions that drove Tolstoy—by working with juveniles at a maximum-security correctional facility. Harvard physics students learn about the universe not through lectures but from their peers in a class where even reading is a social event. And students at a Dallas high school use dance to develop growth mindsets—and many of them go on to top colleges, including Juilliard. Bain defines these as super courses because they all use powerful researched-based elements to build a “natural critical learning environment” that fosters intrinsic motivation, self-directed learning, and self-reflective reasoning. Complete with sample syllabi, the book shows teachers how they can build their own super courses.The story of a hugely important breakthrough in education, Super Courses reveals how these classes can help students reach their full potential, equip them to lead happy and productive lives, and meet the world’s complex challenges.},
  keywords={Learning;Teacher;Classroom;Learning environment;Lecture;Pedagogy;Thought;Scientist;Deep learning;Undergraduate education;Motivation;Grading (education);Engineering;Peer instruction;Curriculum;Student;Literature;Psychologist;Of Education;Syllabus;Medical school;Technology;Aptitude;Problem solving;Physician;Quiz;Requirement;Self-efficacy;Writing;Test (assessment);Textbook;Lecturer;Career;Graduate school;Princeton University Press;Scholarship;Carol Dweck;Eric Mazur;Higher education;Active learning;Psychology;Educational aims and objectives;Historical thinking;Sugata Mitra;Project;Critical thinking;Personal development;Institution;Education;Copyright;Poverty;Smartphone;Seminar;Prejudice;Homework;How People Learn;Adaptive expertise;Academic term;Russian literature;Harvard University;Ellen Langer;Profession;Annotation;Locus of control;Student engagement;Vocabulary;Georgia Institute of Technology;Homelessness;Role-playing;Mechanical engineering;Capstone course;Ingenuity;Theory;Secondary school;Problem set;Teaching method;Expert;Traditional education;Science education;Albert Bandura;Professor;Academic achievement;Feeling;Facilitator;Stereotype threat;Philosopher;Questionnaire;Uncertainty;Force Concept Inventory;Final examination;Private school;Lifelong learning;Spring break;Intelligence;David Hestenes;TRIZ;Social science;Mathematician;John Dewey;Facilitation},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691216591},
  url={https://ieeexplore.ieee.org/document/9647702},}@ARTICLE{9750937,
  author={Glenski, Maria and Ayton, Ellyn and Soni, Sannisth and Saldanha, Emily and Arendt, Dustin and Quiter, Brian and Cooper, Ren and Volkova, Svitlana},
  journal={IEEE Transactions on Nuclear Science},
  title={Learning Global Proliferation Expertise Evolution Using AI-Driven Analytics and Public Information},
  year={2022},
  volume={69},
  number={6},
  pages={1375-1384},
  abstract={Detecting and anticipating global proliferation expertise and capability evolution from unstructured, noisy, and incomplete public data streams is a highly desired, but extremely challenging task. In this article, we present our pioneering data-driven approach to support the non-proliferation mission to detect and explain the evolution of proliferation expertise and capability development globally from terabytes of publicly available information (PAI), focusing on our knowledge extraction pipeline and descriptive analytics. We first discuss how we fuse nine open-source data streams, including multilingual data, to convert 4 TB of unstructured data to structured knowledge and encode dynamically evolving proliferation expertise representations—content and context graphs. For this, we rely on natural language processing (NLP) and deep learning (DL) models to perform information extraction, topic modeling, and distributed text representation (aka embedding) learning. We then present interactive, usable, and explainable descriptive analytics to refine domain knowledge and present it in a human-understandable form. Finally, we introduce future work avenues that will leverage our dynamic knowledge representations and descriptive analytics to enable predictive and prescriptive inferences to achieve real-time domain understanding and contextual reasoning about global proliferation expertise and capability evolution.},
  keywords={Data models;Soft sensors;Open source software;Data mining;Knowledge representation;Data collection;Annotations;Artificial neural networks;big data applications;data mining;data visualization;decision support systems;knowledge discovery;knowledge representations;machine learning;natural language processing (NLP);prediction models},
  doi={10.1109/TNS.2022.3162216},
  ISSN={1558-1578},
  month={June},}@INBOOK{9779377,
  author={Glisic, Savo G. and Lorenzo, Beatriz},
  booktitle={Artificial Intelligence and Quantum Computing for Advanced Wireless Networks},
  title={Machine Learning Algorithms},
  year={2022},
  volume={},
  number={},
  pages={17-54},
  abstract={This chapter briefly presents a number of machine learning (ML) algorithms in a rather descriptive way. Support vector machine (SVM) is a supervised ML algorithm that can be used for both classification and regression challenges. Regression analysis deals with the problem of fitting straight lines to patterns of data. Logistic regression analysis studies the association between a categorical dependent variable and a set of independent (explanatory) variables. The decision tree is a type of supervised learning algorithm (having a predefined target variable) that is mostly used in classification problems. The chapter presents more details about the two most commonly used algorithms: Gradient Boosting and XGBoost. It provides more details on the performance analysis of the logistic regression. The chapter suggests a unified algorithmic framework for presenting these algorithms and describes the various splitting criteria and pruning methodologies.},
  keywords={Mathematical models;Logistics;Regression tree analysis;Machine learning algorithms;Linear regression;Terminology;Sociology},
  doi={10.1002/9781119790327.ch2},
  ISSN={},
  publisher={Wiley},
  isbn={9781119790280},
  url={https://ieeexplore.ieee.org/document/9779377},}@ARTICLE{8240662,
  author={Pereira, Crystiam Kelle and Siqueira, Sean Wolfgand Matsui and Nunes, Bernardo Pereira and Dietze, Stefan},
  journal={IEEE Transactions on Learning Technologies},
  title={Linked Data in Education: A Survey and a Synthesis of Actual Research and Future Challenges},
  year={2018},
  volume={11},
  number={3},
  pages={400-412},
  abstract={Linked Data principles and technologies are being investigated in various areas. In the Educational context, many studies are using Linked Data trying to solve problems of interoperability of educational data and resources, enriching educational content, and personalizing and recommending educational content and practices. This article presents a systematic mapping of proposals which have been adopting Linked Data to support education, and, based on analysis of these proposals, we discuss the tools, vocabularies, and datasets being used, providing a research landscape of the area. We also present challenges and trends which can foster future research in this area.},
  keywords={Education;Systematics;Tools;Vocabulary;Proposals;Market research;Linked data;web of data;linked open data;semantic web;semantic technology;educational technology;vocabulary;tools},
  doi={10.1109/TLT.2017.2787659},
  ISSN={1939-1382},
  month={July},}@ARTICLE{8114162,
  author={Mahmood, Qamar and Qadir, Muhammad Abdul and Afzal, Muhammad Tanvir},
  journal={IEEE Access},
  title={Application of COReS to Compute Research Papers Similarity},
  year={2017},
  volume={5},
  number={},
  pages={26124-26134},
  abstract={Over the decades, the immense growth has been reported in research publications due to continuous developments in science. To date, various approaches have been proposed that find similarity between research papers by applying different similarity measures collectively or individually based on the content of research papers. However, the contemporary schemes are not conceptualized enough to find related research papers in a coherent manner. This paper is aimed at finding related research papers by proposing a comprehensive and conceptualized model via building ontology named COReS: Content-based Ontology for Research Paper Similarity. The ontology is built by finding the explicit relationships (i.e., super-type sub-type, disjointedness, and overlapping) between state-of-the-art similarity techniques. This paper presents the applications of the COReS model in the form of a case study followed by an experiment. The case study uses InText citation-based and vector space-based similarity measures and relationships between these measures as defined in COReS. The experiment focuses on the computation of comprehensive similarity and other content-based similarity measures and rankings of research papers according to these measures. The obtained Spearman correlation coefficient results between ranks of research papers for different similarity measures and user study-based measure, justify the application of COReS for the computation of document similarity. The COReS is in the process of evaluation for ontological errors. In the future, COReS will be enriched to provide more knowledge to improve the process of comprehensive research paper similarity computation.},
  keywords={Ontologies;Semantics;Computational modeling;Extraterrestrial measurements;Indexes;Clustering algorithms;Encyclopedias;Comprehensive similarity computation;content based similarity;ontology;ranking;research paper similarity},
  doi={10.1109/ACCESS.2017.2771207},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{7991567,
  author={Castro, Eduardo P. S. and Chakravarty, Saurabh and Williamson, Eric and Pereira, Denilson Alves and Fox, Edward A.},
  booktitle={2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
  title={Classifying Short Unstructured Data Using the Apache Spark Platform},
  year={2017},
  volume={},
  number={},
  pages={1-10},
  abstract={People worldwide use Twitter to post updates about the events that concern them directly or indirectly. Study of these posts can help identify global events and trends of importance. Similarly, E-commerce applications organize their products in a way that can facilitate their management and satisfy the needs and expectations of their customers. However, classifying data such as tweets or product descriptions is still a challenge. These data are described by short texts, containing in their vocabulary abbreviations of sentences, emojis, hashtags, implicit codes, and other non-standard usage of written language. Consequently, traditional text classification techniques are not effective on these data. In this paper, we describe our use of the Spark platform to implement two classification strategies to process large data collections, where each datum is a short textual description. One of our solutions uses an associative classifier, while the other is based on a multiclass Logistic Regression classifier using Word2Vec as a feature selection and transformation technique. Our associative classifier captures the relationships among words that uniquely identify each class, and Word2Vec captures the semantic and syntactic context of the words. In our experimental evaluation, we compared our solutions, as well as Spark MLlib classifiers. We assessed effectiveness, efficiency, and memory requirements. The results indicate that our solutions are able to effectively classify the millions of data instances composed of thousands of distinct features and classes, found in our digital libraries.},
  keywords={Sparks;Logistics;Training;Twitter;Libraries;Data models;Machine learning algorithms},
  doi={10.1109/JCDL.2017.7991567},
  ISSN={},
  month={June},}@ARTICLE{1031947,
  author={Pal, S.K. and Talwar, V. and Mitra, P.},
  journal={IEEE Transactions on Neural Networks},
  title={Web mining in soft computing framework: relevance, state of the art and future directions},
  year={2002},
  volume={13},
  number={5},
  pages={1163-1177},
  abstract={The paper summarizes the different characteristics of Web data, the basic components of Web mining and its different types, and the current state of the art. The reason for considering Web mining, a separate field from data mining, is explained. The limitations of some of the existing Web mining methods and tools are enunciated, and the significance of soft computing (comprising fuzzy logic (FL), artificial neural networks (ANNs), genetic algorithms (GAs), and rough sets (RSs) are highlighted. A survey of the existing literature on "soft Web mining" is provided along with the commercially available systems. The prospective areas of Web mining where the application of soft computing needs immediate attention are outlined with justification. Scope for future research in developing "soft Web mining" systems is explained. An extensive bibliography is also provided.},
  keywords={Web mining;Fuzzy logic;Artificial intelligence;Data mining;Artificial neural networks;Genetic algorithms;Computer networks;Rough sets;Information retrieval;Search engines},
  doi={10.1109/TNN.2002.1031947},
  ISSN={1941-0093},
  month={Sep.},}@ARTICLE{9428346,
  author={Zanella, Gianluca and Liu, Charles Zhechao and Choo, Kim-Kwang Raymond},
  journal={IEEE Transactions on Engineering Management},
  title={Understanding the Trends in Blockchain Domain Through an Unsupervised Systematic Patent Analysis},
  year={2023},
  volume={70},
  number={6},
  pages={1991-2005},
  abstract={Patent analysis is crucial for technology monitoring, forecasting, and assessment, and facilitates entrepreneurs and different stakeholder groups to develop forward-looking technologies and business strategies. However, the speed and scale in the development of disruptive technologies, such as blockchain, present a challenge for analysts and experts. In this article, we propose an unsupervised systematic patent analysis framework that applies a mixture of cosine-based and density-based outlier analysis to the patent space. A sample of 13 393 blockchain-related patents published between January 2014 and June 2020 is used to test the proposed framework. Specifically, this framework merges cosine and density-based outlier detection methodologies to improve the identification of outliers within clusters of patents. The identified outliers are visualized through an age-outlier technology-opportunity analysis map that represents the different levels of novelty existing in each cluster of the patent sample. The map facilitates companies to better target their R&D efforts and maximize the return of technology investments. Benchmark results show that the proposed outlier detection method improves recall, precision, and f1 score. In addition, the results show that the cluster with a higher percentage of outliers represents the Internet of Things applications of blockchain technology.},
  keywords={Patents;Data mining;Blockchain;Market research;Security;Monitoring;Databases;Blockchain;distributed ledger technology;patent analysis;outlier detection;unsupervised learning},
  doi={10.1109/TEM.2021.3074310},
  ISSN={1558-0040},
  month={June},}@ARTICLE{9454320,
  author={Gonzalez, Hector A. and George, Richard and Muzaffar, Shahzad and Acevedo, Javier and Höppner, Sebastian and Mayr, Christian and Yoo, Jerald and Fitzek, Frank H.P. and Elfadel, Ibrahim M.},
  journal={IEEE Transactions on Biomedical Circuits and Systems},
  title={Hardware Acceleration of EEG-Based Emotion Classification Systems: A Comprehensive Survey},
  year={2021},
  volume={15},
  number={3},
  pages={412-442},
  abstract={Recent years have witnessed a growing interest in EEG-based wearable classifiers of emotions, which could enable the real-time monitoring of patients suffering from neurological disorders such as Amyotrophic Lateral Sclerosis (ALS), Autism Spectrum Disorder (ASD), or Alzheimer's. The hope is that such wearable emotion classifiers would facilitate the patients’ social integration and lead to improved healthcare outcomes for them and their loved ones. Yet in spite of their direct relevance to neuro-medicine, the hardware platforms for emotion classification have yet to fill up some important gaps in their various approaches to emotion classification in a healthcare context. In this paper, we present the first hardware-focused critical review of EEG-based wearable classifiers of emotions and survey their implementation perspectives, their algorithmic foundations, and their feature extraction methodologies. We further provide a neuroscience-based analysis of current hardware accelerators of emotion classifiers and use it to map out several research opportunities, including multi-modal hardware platforms, accelerators with tightly-coupled cores operating robustly in the near/supra-threshold region, and pre-processing libraries for universal EEG-based datasets.},
  keywords={Hardware;Software;Feature extraction;Electroencephalography;Biomedical monitoring;Training;Software algorithms;Emotion detection and classification;EEG;hardware acceleration;machine learning;monitoring of neurological disorders},
  doi={10.1109/TBCAS.2021.3089132},
  ISSN={1940-9990},
  month={June},}@ARTICLE{9151144,
  author={Memon, Jamshed and Sami, Maira and Khan, Rizwan Ahmed and Uddin, Mueen},
  journal={IEEE Access},
  title={Handwritten Optical Character Recognition (OCR): A Comprehensive Systematic Literature Review (SLR)},
  year={2020},
  volume={8},
  number={},
  pages={142642-142668},
  abstract={Given the ubiquity of handwritten documents in human transactions, Optical Character Recognition (OCR) of documents have invaluable practical worth. Optical character recognition is a science that enables to translate various types of documents or images into analyzable, editable and searchable data. During last decade, researchers have used artificial intelligence/machine learning tools to automatically analyze handwritten and printed documents in order to convert them into electronic format. The objective of this review paper is to summarize research that has been conducted on character recognition of handwritten documents and to provide research directions. In this Systematic Literature Review (SLR) we collected, synthesized and analyzed research articles on the topic of handwritten OCR (and closely related topics) which were published between year 2000 to 2019. We followed widely used electronic databases by following pre-defined review protocol. Articles were searched using keywords, forward reference searching and backward reference searching in order to search all the articles related to the topic. After carefully following study selection process 176 articles were selected for this SLR. This review article serves the purpose of presenting state of the art results and techniques on OCR and also provide research directions by highlighting research gaps.},
  keywords={Optical character recognition software;Character recognition;Databases;Optical imaging;Bibliographies;Protocols;Systematics;Optical character recognition;classification;languages;feature extraction;deep learning},
  doi={10.1109/ACCESS.2020.3012542},
  ISSN={2169-3536},
  month={},}@ARTICLE{10084747,
  author={Peña Ramírez, K and Smith, L C and Ramírez Alegría, S and Chené, A-N and González-Fernández, C and Lucas, P W and Minniti, D},
  journal={Monthly Notices of the Royal Astronomical Society},
  title={The VVV open cluster project – II. Near-infrared sequences of 37 open clusters on eight-dimensional parameter space},
  year={2021},
  volume={513},
  number={4},
  pages={5799-5813},
  abstract={Open clusters are key coeval structures that help us understand star formation, stellar evolution and trace the physical properties of our Galaxy. In the past years, the isolation of open clusters from the field has been heavily alleviated by the access to accurate large-scale stellar parallaxes and proper motions along a determined line of sight. Still, there are limitations regarding their completeness since large-scale studies rely on optical wavelengths. Here, we extend the open clusters sequences towards fainter magnitudes complementing the Gaia photometric and astrometric information with near-infrared data from the VVV survey. We performed a homogeneous analysis on 37 open clusters implementing two coarse-to-fine characterization methods: extreme deconvolution Gaussian mixture models coupled with an unsupervised machine learning method on eight-dimensional parameter space. The process allowed us to separate the clusters from the field at near-infrared wavelengths. We report an increase of ∼47 per cent new member candidates on average in our sample (considering only sources with high membership probability p ≧ 0.9). This study is the second in a series intended to reveal open cluster near-infrared sequences homogeneously.},
  keywords={methods: data analysis;stars: evolution;Galaxy: open clusters and associations: individual},
  doi={10.1093/mnras/stac1296},
  ISSN={1365-2966},
  month={Dec},}@INPROCEEDINGS{8584553,
  author={Yélémou, Tiguiane and Somé, Borlli Michel Jonas and Kiélem, Wilfried},
  booktitle={2018 1st International Conference on Smart Cities and Communities (SCCIC)},
  title={An Enhanced Moodle-based Learning Management System to Account for Low Bandwidths},
  year={2018},
  volume={},
  number={},
  pages={1-4},
  abstract={Since 2012, at Nazi BONI University (NBU), as in many other universities in West Africa, academic years overlap. In large classes (more than 1000 students), the lack of on-site teachers and classrooms to run several tutorial groups in parallel means that pedagogical activities of an academic year go on for more than 18 months. Integration of ICT in education can contribute significantly to solving the problem. We propose a technical solution that takes into account the narrowness of the bandwidth of Internet access (actually 2 Mbps). We strengthen the capacities of the NBU in IT resources and optimize their use. In this sense, we are making the intranet functional where Leaning Management System (LMS) servers can be accessed with a throughput of at least 10 Mbps from all university sites, including university residences. Optimal exploitation techniques of the Internet resource have been applied. We have conducted a thorough analysis of the causes of slow loading of pedagogical resources and learning activities and we propose technical solutions to alleviate the LMS Moodle. Thereby, on average, we managed to halve the loading time of this platform's resources. Even actors in areas with only 2G technology can interact with the platform.},
  keywords={Internet;Loading;Servers;Bandwidth;Videos;Cascading style sheets;Software;Learning Management System;Moodle reengineering;Context of narrowness bandwidth},
  doi={10.1109/SCCIC.2018.8584553},
  ISSN={},
  month={July},}@INPROCEEDINGS{9314600,
  author={Abdellaoui, Benyoussef and MOUMEN, Aniss and EL BOUZEKRI EL IDRISSI, Younes and Remaida, Ahmed},
  booktitle={2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)},
  title={Face Detection to Recognize Students’ Emotion and Their Engagement: A Systematic Review},
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Following the Coronavirus COVID pandemic 19, many countries have adopted distance education to ensure pedagogical continuity for which they have not been ready yet to deal with. So, to reduce the spread of the virus, since March 2020, Morocco has called for state health emergency and imposed strict confinement on the population. In response to this, all training establishments have been locked down. In such a context of crisis and disability, teachers, students, and their families have found themselves in a delicate situation to manage, where new distance learning needs and practical training have been imposed without adequate measures for conducting an education as an integral part of a normative academic curriculum. In this article we will discuss the teachers' inability to asses the students' emotional state and determine their engagement which can be clearly read from their faces in normal teaching situation. It aims at analyzing and responding to problems related to the detection of faces to automatically deduce emotions and study student engagement while exploring and analyzing the references available on the some databases cited in the article. This literature review is supplemented by thematic analysis and a meta-analysis of the corpus. This study has identified both quantitative and qualitative and/or experimental work.},
  keywords={Face recognition;Training;Emotion recognition;Face detection;Databases;Computer aided instruction;Tools;E-learning;TIC;Distance learning;prisma statement;face detection;emotion recognition;student engagement},
  doi={10.1109/ICECOCS50124.2020.9314600},
  ISSN={},
  month={Dec},}@ARTICLE{9969919,
  author={Gomez, Manuel J. and Ruipérez-Valiente, José A. and Clemente, Félix J. García},
  journal={IEEE Transactions on Learning Technologies},
  title={A Systematic Literature Review of Game-Based Assessment Studies: Trends and Challenges},
  year={2023},
  volume={16},
  number={4},
  pages={500-515},
  abstract={Technology has become an essential part of our everyday life, and its use in educational environments keeps growing. In addition, games are one of the most popular activities across cultures and ages, and there is ample evidence that supports the benefits of using games for assessment. This field is commonly known as game-based assessment (GBA), which refers to the use of games to assess learners' competencies, skills, or knowledge. In this article, we analyze the current status of the GBA field by performing the first systematic literature review on empirical GBA studies. It is based on 65 research papers that used digital GBAs to determine: the context where the study has been applied, the primary purpose, the domain of the game used, game/tool availability, the size of the data sample, the computational methods and algorithms applied, the targeted stakeholders of the study, and what limitations and challenges are reported by authors. Based on the categories established and our analysis, the findings suggest that GBAs are mainly used in K-16 education and for assessment purposes, and that most GBAs focus on assessing STEM content, and cognitive and soft skills. Furthermore, the current limitations indicate that future GBA research would benefit from the use of bigger data samples and more specialized algorithms. Based on our results, we discuss current trends in the field and open challenges (including replication and validation problems), providing recommendations for the future research agenda of the GBA field.},
  keywords={Games;Systematics;Education;Databases;Bibliographies;Video games;Market research;Educational technology;game-based assessment (GBA);game-based learning (GBL)},
  doi={10.1109/TLT.2022.3226661},
  ISSN={1939-1382},
  month={Aug},}@INPROCEEDINGS{7950406,
  author={Liu, Haichi and Wang, Ting and Tang, Jintao and Ning, Hong and Wei, Dengping},
  booktitle={2017 3rd International Conference on Information Management (ICIM)},
  title={Link prediction of datasets sameAS interlinking network on web of data},
  year={2017},
  volume={},
  number={},
  pages={346-352},
  abstract={In order to be considered as Linked Data, the datasets on the web must be linked to other datasets. Current studies on dataset interlinking prediction researches do not distinguish the type of links, which are of less help for real application scenarios, as dataset publishers still do not know what kinds of RDF links can be established and furthermore how to configure the data linking algorithms. In this paper, we focus on predicting the possible links between datasets with the most important RDF link type, owl:sameAs. Since the goal is to discriminate between linked dataset pairs against not-linked ones, we formulate the link prediction problem as a classification problem. We adopt Random Forest as the basic classifier to incorporate features of the scores output by unsupervised predictors, and apply the bagging technique to combine multiple forests to reduce variance and improve the accuracy. Experiments show we can improve the prediction performance by about 10% in AUROC.},
  keywords={Resource description framework;Computers;Electronic mail;Training;Measurement;Joining processes;linked data;link prediction;sameAs link;random forest},
  doi={10.1109/INFOMAN.2017.7950406},
  ISSN={},
  month={April},}@ARTICLE{10423633,
  author={Nguyen, Hoang-Sy and Voznak, Miroslav},
  journal={IEEE Access},
  title={A Bibliometric Analysis of Technology in Digital Health: Exploring Health Metaverse and Visualizing Emerging Healthcare Management Trends},
  year={2024},
  volume={12},
  number={},
  pages={23887-23913},
  abstract={The digital economy has engendered Health Metaverse, an innovative technology with vast potential to transform healthcare through immersive experiences. The Health Metaverse serves as a convergence point for a multitude of technologies, including artificial intelligence (AI), virtual reality in heath, augmented reality in health, internet-connected medical devices, quantum computing, and more. This convergence opens up possibilities, for advancing quality healthcare. Therefore, reviewing recent influential literature is critical to understand current methods and envision future improvements. This study utilizes a hybrid bibliometric-structured methodology combining descriptive and bibliometric network analysis. To gather information we conducted searches on the Web of Science database and reviewed references. Our inclusion criteria focused on articles and reviews published between January 2012 and June 2023. We used keyword groups for our searches. Then performed bibliometric analysis followed by content analysis. Papers were reviewed, analyzed and categorized into focuses on multimodal medical information standards, medical/social data fusion, telemedicine, online health management, and medical AI. This bibliometric analysis of 34 thousand publications over 10 years proposes medical and health informatics in the Metaverse. Five future research direction clusters were identified. It delineates intelligent solutions bridging healthcare barriers. In conclusion, this review examines the Metaverse, in healthcare explores cutting edge technologies, applications, projects and highlights areas where adaptation may be needed. It identifies adaptation issues and suggests solutions warranting further research.},
  keywords={Medical services;Metaverse;Bibliometrics;Artificial intelligence;Electronic healthcare;Visualization;Medical devices;Bioinformatics;Quality assessment;Medical services;Health metaverse;healthcare technology;health informatics;quality of care;digital health;data analysis;bibliometric analysis},
  doi={10.1109/ACCESS.2024.3363165},
  ISSN={2169-3536},
  month={},}@ARTICLE{9771410,
  author={Mezger, Benjamin W. and Santos, Douglas A. and Dilillo, Luigi and Zeferino, Cesar A. and Melo, Douglas R.},
  journal={IEEE Access},
  title={A Survey of the RISC-V Architecture Software Support},
  year={2022},
  volume={10},
  number={},
  pages={51394-51411},
  abstract={RISC-V is a novel open instruction set architecture that supports multiple platforms while maintaining simplicity and reliability. Despite its novelty, the software support for RISC-V has been increasing in the last years, given that popular toolchains and operating systems already have support for RISC-V. However, although many works have been exploring the RISC-V software ecosystem, no work that raised the current state of software support for RISC-V is available. In this context, this survey reviews the contributions introduced in the last years to understand the RISC-V’s software ecosystem and its usage in both academic and industrial environments. We classified and evaluated the works into four main categories: application fields, RISC-V implementations, software architecture, and deployment features. The primary goal of this research is to provide the community with a comprehensive overview of the current state of RISC-V software support and identify and highlight the main contributions from recent work.},
  keywords={Software;Computer architecture;Registers;Reliability;Hardware;Ecosystems;Operating systems;RISC-V;software support;operating systems},
  doi={10.1109/ACCESS.2022.3174125},
  ISSN={2169-3536},
  month={},}@ARTICLE{8750853,
  author={Tang, Chunlei and Zhu, Yangyong and Blackley, Suzanne V. and Plasek, Joseph M. and Wan, Meihan and Zhou, Li and Ma, Jing and Bates, David W.},
  journal={IEEE Access},
  title={Visualizing Literature Review Theme Evolution on Timeline Maps: Comparison Across Disciplines},
  year={2019},
  volume={7},
  number={},
  pages={90597-90607},
  abstract={Data-driven visualization techniques can be utilized to enhance the literature review process across different disciplines. In our work, 910 articles were retrieved using keyword search from bibliographic databases of two different disciplines (computer science: DBLP and medicine: MEDLINE) between 2001 and 2016. These articles’ titles were processed using dynamic latent Dirichlet allocation to generate a set of themes/topics, which were subsequently classified and assigned to regions in a spatiotemporal geographical map. Resulting data visualizations from both repositories were manually reviewed by independent annotators. The results from the DBLP and MEDLINE were comparable and, taken together, suggest potential benefits of increased future interaction amongst multidisciplinary fields. Our findings indicate that spiral timeline maps have the potential to help researchers acquire or compare knowledge efficiently without prior domain knowledge.},
  keywords={Bibliographies;Computer science;Spirals;Data visualization;Support vector machines;Task analysis;Visualization;Review;information storage and retrieval;research;medical informatics;epidemiology;MEDLINE;data visualization},
  doi={10.1109/ACCESS.2019.2925706},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10625425,
  author={Tomar, Lakshya and Diksha and Jain, Anshita and Sharma, Chetna and Gunn and Panseja, Gourav},
  booktitle={2024 2nd International Conference on Sustainable Computing and Smart Systems (ICSCSS)},
  title={Augmented Reality for Emergency Responders: A Bibliometric Review and Future Directions},
  year={2024},
  volume={},
  number={},
  pages={538-543},
  abstract={Technology innovation is led by Augmented Reality (AR), which provides immersive experiences that combine digital and real world. This research study presents a diversified review and analysis of the current state of augmented reality technology, its applications, challenges, and future directions. The purpose of this study is to provide scientific mapping on the role of deep learning in areas of augmented reality published online. Scopus and VOSviewer are used to create a concept map of 185 articles. Additionally, future research directions are suggested.},
  keywords={Deep learning;Technological innovation;Reviews;Education;Bibliometrics;Prototypes;User interfaces;Augmented Reality;navigation;bibliometric;emergency responders;healthcare},
  doi={10.1109/ICSCSS60660.2024.10625425},
  ISSN={},
  month={July},}@INPROCEEDINGS{6107361,
  author={Krummenacher, Reto and Toma, Ioan and Fensel, Dieter and Brehar, Raluca and Nedevschi, Sergiu},
  booktitle={2011 IST-Africa Conference Proceedings},
  title={Instrumenting and monitoring the LarKC reasearch infrastructure},
  year={2011},
  volume={},
  number={},
  pages={1-8},
  abstract={Reasoning is central to the idea of the Semantic Web and ontologies, however, the fundamental principles of reasoning - soundness and completeness - do not match the reality of the (Semantic) Web that is ruled by contradicting and incomplete data and claims. Furthermore, logical reasoning is strong for rather small numbers of axioms and facts, while the Web is growing at an impressive speed and hence offering tremendous amounts of data. `Reasearch' is a novel idea of combining reasoning with information retrieval methods (search) in order to respond to the requirements and constraints implied by reasoning on the Semantic Web. The Large Knowledge Collider is a modular reasoning platform that allows for reasearching with Web-scale data, and instrumenting and monitoring the platform is essential for verifying and assuring high performance, adaptability and well-functioning. These aspects are vital for experiments running on the platform.},
  keywords={Measurement;Cognition;Instruments;Monitoring;Semantic Web;Knowledge engineering;Semantics;Reasoning;Semantic Web;Instrumentation and Monitoring},
  doi={},
  ISSN={},
  month={May},}@ARTICLE{8955801,
  author={Hidri, Adel and Selmi, Ahmed and Hidri, Minyar Sassi},
  journal={IEEE Access},
  title={Discovery of Frequent Patterns of Episodes Within a Time Window for Alarm Management Systems},
  year={2020},
  volume={8},
  number={},
  pages={11061-11073},
  abstract={The sequential pattern mining field is expanding through numerous researches and has a large number of applications such as language processing, alarms management and event management on a broader scale. Its use began with processing items baskets to learn patterns and have a directed marketing strategy but it is generalized to telecommunication alarms management with several works. Our work is in line with this, as it tries to locate patterns and identify them to make predictive statements about certain patterns. It is axed around providing a way to break sequences into episodes and assigning them a value of confidence and support, more precisely in the discovery of frequent patterns of episodes within a time window. Experimental results have shown the effectiveness of our sequential pattern mining approach and its adaptability to alarm management and analytics.},
  keywords={Data mining;Software algorithms;Data visualization;Databases;Alarm systems;Data analysis;Sequential pattern mining;alarm management;association rules;data mining;artificial intelligence},
  doi={10.1109/ACCESS.2020.2965647},
  ISSN={2169-3536},
  month={},}@ARTICLE{8345598,
  author={Zhang, Da and Kabuka, Mansur R.},
  journal={IEEE Transactions on Emerging Topics in Computing},
  title={Distributed Relationship Mining over Big Scholar Data},
  year={2021},
  volume={9},
  number={1},
  pages={354-365},
  abstract={In this paper, we propose a system infrastructure to construct the big scholar data as a large knowledge graph, discover the meta paths between the entities and calculate the relevancy between entities in the graph. The core infrastructure is established on the secured and private Amazon Elastic Compute Cloud(Amazon EC2) platform. The infrastructure maintains the data evenly across the repositories, processes the data parallel by utilizing open source Spark framework, manages computing resources optimally by utilizing YARN and Hadoop HDFS, and discovers the relationship distributedly between different types of entities. We incorporate four relationship discovery tasks including citation recommendation, potential collaborator discovery, similar venue measurement and paper to venue recommendation on top of this infrastructure. For relationship mining tasks, we propose a mixed and weighted meta path (MWMP) method to explore the potential relationship between different types of entities. To verify the accuracy and measure parallelization speedup of our algorithm, we set up clusters through Amazon EC2 platform.},
  keywords={Task analysis;Distributed databases;Data mining;Big Data;Semantics;Knowledge engineering;Complexity theory;Scholarly big data;distributed system;heterogeneous information network;graph recommendation},
  doi={10.1109/TETC.2018.2829772},
  ISSN={2168-6750},
  month={Jan},}@ARTICLE{4472077,
  author={Casey, Michael A. and Veltkamp, Remco and Goto, Masataka and Leman, Marc and Rhodes, Christophe and Slaney, Malcolm},
  journal={Proceedings of the IEEE},
  title={Content-Based Music Information Retrieval: Current Directions and Future Challenges},
  year={2008},
  volume={96},
  number={4},
  pages={668-696},
  abstract={The steep rise in music downloading over CD sales has created a major shift in the music industry away from physical media formats and towards online products and services. Music is one of the most popular types of online information and there are now hundreds of music streaming and download services operating on the World-Wide Web. Some of the music collections available are approaching the scale of ten million tracks and this has posed a major challenge for searching, retrieving, and organizing music content. Research efforts in music information retrieval have involved experts from music perception, cognition, musicology, engineering, and computer science engaged in truly interdisciplinary activity that has resulted in many proposed algorithmic and methodological solutions to music search using content-based methods. This paper outlines the problems of content-based music information retrieval and explores the state-of-the-art methods using audio cues (e.g., query by humming, audio fingerprinting, content-based music retrieval) and other cues (e.g., music notation and symbolic representation), and identifies some of the major challenges for the coming years.},
  keywords={Music information retrieval;Content based retrieval;Multiple signal classification;Marketing and sales;Organizing;Cognition;Computer science;Signal processing algorithms;Fingerprint recognition;Digital signal processing;Audio signal processing;content-based music information retrieval;symbolic processing;user interfaces},
  doi={10.1109/JPROC.2008.916370},
  ISSN={1558-2256},
  month={April},}@ARTICLE{8700244,
  author={Arzamasova, Natalia and Böhm, Klemens and Goldman, Bertrand and Saaler, Christian and Schäler, Martin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={On the Usefulness of SQL-Query-Similarity Measures to Find User Interests},
  year={2020},
  volume={32},
  number={10},
  pages={1982-1999},
  abstract={In the sciences and elsewhere, the use of relational databases has become ubiquitous. An important challenge is finding hot spots of user interests. In principle, one can discover user interests by clustering the queries in the query log. Such a clustering requires a notion of query similarity. This, in turn, raises the question of what features of SQL queries are meaningful. We have studied the query representations proposed in the literature and corresponding similarity functions and have identified shortcomings of all of them. To overcome these limitations, we propose new similarity functions for SQL queries. They rely on the so-called access area of a query and, more specifically, on the overlap and the closeness of the access areas. We have carried out experiments systematically to compare the various similarity functions described in this article. The first series of experiments measures the quality of clustering and compares it to a ground truth. In the second series, we focus on the query log from the well-known SkyServer database. Here, a domain expert has interpreted various clusters by hand. We conclude that clusters obtained with our new measures of similarity seem to be good indicators of user interests.},
  keywords={Feature extraction;Relational databases;Extraterrestrial measurements;Companies;Clustering algorithms;SQL log analysis;SQL query representations;similarity measures},
  doi={10.1109/TKDE.2019.2913381},
  ISSN={1558-2191},
  month={Oct},}@ARTICLE{10659410,
  author={},
  journal={IEEE Std 2894-2024},
  title={IEEE Guide for an Architectural Framework for Explainable Artificial Intelligence},
  year={2024},
  volume={},
  number={},
  pages={1-55},
  abstract={A new wave of artificial intelligence applications that offer extensive benefits to our daily lives has been led to by dramatic success in machine learning. The loss of explainability during this transition, however, means vulnerability to vicious data, poor model structure design, and suspicion of stakeholders and the general public--all with a range of legal implications. The study of explainable AI (XAI), which is an active research field that aims to make AI systems results more understandable to humans, has been called for by this dilemma. This is a field with great hopes for improving the trust and transparency of AI-based systems and is considered a necessary route for AI to move forward. A technological blueprint for building, deploying, and managing machine learning models, while meeting the requirements of transparent and trustworthy AI by adopting a variety of XAI methodologies, is provided by this guide. It defines the architectural framework and application guidelines for explainable AI, including: description and definition of XAI; the types of XAI methods and the application scenarios to which each type applies; and performance evaluation of XAI.},
  keywords={IEEE Standards;Artificial intelligence;Explainable AI;Artificial intelligence;Machine learning;AI;architectural framework;artificial intelligence;explainable AI;explainable artificial intelligence;IEEE 2894™;machine learning;XAI},
  doi={10.1109/IEEESTD.2024.10659410},
  ISSN={},
  month={Aug},}@BOOK{8186976,
  author={Lupu, Mihai and Hanbury, Allan},
  title={Patent Retrieval},
  year={2013},
  volume={},
  number={},
  pages={},
  abstract={Intellectual property and the patent system in particular have garnered a lot of attention, even in the public media, over the last few years. This monograph is not concerned with any of the controversial issues regarding the patent system itself but it does examine a very real and growing problem: searching for innovation. The target collection for this task does not consist of patent documents only, but it is in these documents that the main difference is found compared to web or news information retrieval. In addition, the issue of patent search implies a particular user model and search process model. Patent Retrieval addresses the question of how research and technology in the field of Information Retrieval assists or even changes the processes of patent search. It is a survey of work done on patent data in relation to Information Retrieval in the last 20 to 25 years. It explains the sources of difficulty and the existing document processing and retrieval methods of the domain, and provides a motivation for further research in the area. Patent Retrieval is an ideal reference for Information Retrieval researchers interested in the patent domain and for patent information professionals.},
  keywords={Web Science: Search;Web Science: E-Government;Web Science: Databases on the web;Theoretical Computer Science: Information retrieval;Machine Learning: Classification and prediction;Information Retrieval: User modelling and user studies for IR;Informa},
  doi={10.1561/1500000027},
  ISSN={},
  publisher={now},
  isbn={9781601986498},
  url={https://ieeexplore.ieee.org/document/8186976},}@ARTICLE{10734711,
  author={},
  journal={IEEE P3110/D7, August 2024},
  title={IEEE Draft Standard for Computer Vision (CV) - Technical Requirements for Algorithms Application Programming Interfaces (APIs) of Deep Learning Framework},
  year={2024},
  volume={},
  number={},
  pages={1-58},
  abstract={Functional and technical requirements for the interfaces between algorithms and learning frameworks (including the interfaces provided by training frameworks), and between algorithms and data sets in the development of artificial intelligence (AI) computer vision algorithms were specified in this standard. The adaptation and invocation of the application programming interfaces (API) involved in the development of computer vision algorithms (CVA) based on deep learning (hereinafter referred to as “CVA”) were addressed in this standard.},
  keywords={IEEE Standards;Computer vision;Deep learning;Artificial intelligence;Technical requirements;Algorithm design and analysis;Application programming interfaces;algorithm;application programming interfaces;computer vision;functional requirements;technical requirements},
  doi={},
  ISSN={},
  month={Oct},}@INBOOK{9822325,
  author={Powell, Warren B.},
  booktitle={Reinforcement Learning and Stochastic Optimization: A Unified Framework for Sequential Decisions},
  title={Derivative‐Free Stochastic Search},
  year={2022},
  volume={},
  number={},
  pages={317-427},
  abstract={},
  keywords={Stochastic processes;Search problems;Operations research;Drugs;Computer simulation;Computational modeling;Supply chains},
  doi={10.1002/9781119815068.ch7},
  ISSN={},
  publisher={Wiley},
  isbn={9781119815044},
  url={https://ieeexplore.ieee.org/document/9822325},}@INPROCEEDINGS{9426755,
  author={Sinkala, Zipani Tom and Herold, Sebastian},
  booktitle={2021 IEEE 18th International Conference on Software Architecture (ICSA)},
  title={InMap: Automated Interactive Code-to-Architecture Mapping Recommendations},
  year={2021},
  volume={},
  number={},
  pages={173-183},
  abstract={Reflexion Modelling is a popular method used in industry for Software Architectural Consistency Checking (SACC). However, it involves a mapping step that is manual and tedious. There exist techniques and tools that attempt to automate mapping, yet they are either limited in their approach or they require an initial set of manually pre-mapped entities. This study proposes a novel technique, InMap, that improves the mapping process in reflexion modelling by both providing versatility and eliminating the constraint of needing a set of manually pre-mapped entities in order to automate mapping. Using a software’s architecture descriptions, InMap applies information retrieval concepts to the software’s source code to interactively provide mapping recommendations to an architect. For the six systems InMap was evaluated on, the recommendations it provided achieved an average recall of 0.97, and an average precision of 0.82. InMap also achieved higher, f1-scores in comparison to existing techniques that require premapping. This provides a basis for improving industry tools that use reflexion modelling or similar SACC methods.},
  keywords={Industries;Training;Machine learning algorithms;Software architecture;Natural languages;Computer architecture;Manuals;automating architecture conformance;software architecture descriptions;source code mapping;reflexion modelling;software architecture consistency checking;software maintenance;information retrieval},
  doi={10.1109/ICSA51549.2021.00024},
  ISSN={},
  month={March},}@ARTICLE{10084361,
  author={Gözükara, Furkan and Özel, Selma Ayşe},
  journal={The Computer Journal},
  title={An Incremental Hierarchical Clustering Based System For Record Linkage In E-Commerce Domain},
  year={2021},
  volume={66},
  number={3},
  pages={581-602},
  abstract={In this study, a novel record linkage system for E-commerce products is presented. Our system aims to cluster the same products that are crawled from different E-commerce websites into the same cluster. The proposed system achieves a very high success rate by combining both semi-supervised and unsupervised approaches. Unlike the previously proposed systems in the literature, neither a training set nor structured corpora are necessary. The core of the system is based on Hierarchical Agglomerative Clustering (HAC); however, the HAC algorithm is modified to be dynamic such that it can efficiently cluster a stream of incoming new data. Since the proposed system does not depend on any prior data, it can cluster new products. The system uses bag-of-words representation of the product titles, employs a single distance metric, exploits multiple domain-based attributes and does not depend on the characteristics of the natural language used in the product records. To our knowledge, there is no commonly used tool or technique to measure the quality of a clustering task. Therefore in this study, we use ELKI (Environment for Developing KDD-Applications Supported by Index-Structures), an open-source data mining software, for performance measurement of the clustering methods; and show how to use ELKI for this purpose. To evaluate our system, we collect our own dataset and make it publicly available to researchers who study E-commerce product clustering. Our proposed system achieves 96.25% F-Measure according to our experimental analysis. The other state-of-the-art clustering systems obtain the best 89.12% F-Measure.},
  keywords={clustering;data mining;product clustering;record linkage;comparison shopping},
  doi={10.1093/comjnl/bxab179},
  ISSN={1460-2067},
  month={Oct},}@ARTICLE{10473052,
  author={Siew, Emily Sing Kiang and Sze, San Nah and Goh, Say Leng and Kendall, Graham and Sabar, Nasser R. and Abdullah, Salwani},
  journal={IEEE Access},
  title={A Survey of Solution Methodologies for Exam Timetabling Problems},
  year={2024},
  volume={12},
  number={},
  pages={41479-41498},
  abstract={Exam timetabling is a prominent topic in academic administration management as it ensures the effective utilization of resources and satisfies the requirements and preferences of stakeholders, which leads to a productive academic environment, contributing to the institution’s overall success. Given the myriad of solution methodologies explored across diverse exam timetabling problems and constraints, both in studied benchmark datasets and real-life cases over the last decade, it is imperative to undertake a comprehensive survey. This survey paper aims to comprehensively describe the exam timetabling problem (ETP), including its variants, constraints, and benchmark dataset. We look at different methods to solve ETP problems from 2012 to 2023. These methods include mathematical optimization, heuristics, metaheuristics, hyper-heuristics, hybrid approaches, and matheuristics. Finally, we discuss the review findings and potential research directions. By doing so, we hope to facilitate a deeper understanding of ETP and offer valuable insights for future research.},
  keywords={Surveys;Benchmark testing;Linear programming;Symbols;Terminology;Programming;Metaheuristics;Education;Testing;Scheduling;Educational timetabling;exam scheduling;exam timetabling;solution methodologies},
  doi={10.1109/ACCESS.2024.3378054},
  ISSN={2169-3536},
  month={},}@ARTICLE{3883,
  author={Mili, H. and Rada, R.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Merging thesauri: principles and evaluation},
  year={1988},
  volume={10},
  number={2},
  pages={204-220},
  abstract={An investigation is reported of ways to take advantage of the semantics in thesauri to improve: (1) indexing by providing descriptions of documents as sets of terms from the thesaurus; and (2) retrieval by assessing the relevance of documents to a query. Thesauri need to be updated to account for the evolution of the field they cover. Accordingly, various augmentation algorithms and methods to assess the usefulness of these augmentations are being studied. The augmentations consist of merging two existing thesauri. By keeping a consistent level of complexity among the structure manipulated by the merging algorithm, the reasoning method, and the evaluation procedure, an improvement of the performance of the merged thesaurus on both document indexing and retrieval is demonstrated.<>},
  keywords={Merging;Thesauri;Indexing;Information retrieval;Artificial intelligence;Libraries;Databases;Testing;Machine learning algorithms;Learning systems},
  doi={10.1109/34.3883},
  ISSN={1939-3539},
  month={March},}@INPROCEEDINGS{6304150,
  author={Newman, Nils C. and Porter, Alan L. and Newman, David and Courseault, Cherie and Bolan, Stephanie D.},
  booktitle={2012 Proceedings of PICMET '12: Technology Management for Emerging Technologies},
  title={Comparing methods to extract technical content for technological intelligence},
  year={2012},
  volume={},
  number={},
  pages={1279-1285},
  abstract={We are developing indicators for the emergence of science and technology (S&T) topics. We are targeting various S&T information resources, including metadata (i.e., bibliographic information) and full text. We explore alternative text analysis approaches - principal components analysis (PCA) and topic modeling - to extract technical topic information. We analyze the topical content to pursue potential applications and innovation pathways. In this presentation we compare alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing (NLP) on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and tf-idf weighting. We compare PCA results to topic modeling results. Our key test set consists of 4104 Web of Science records on Dye-Sensitized Solar Cells (DSSCs). Results suggest good potential to enhance our technical intelligence payoffs from database searches on topics of interest.},
  keywords={Principal component analysis;Decision support systems;Abstracts;Electrodes;Clustering algorithms;Photovoltaic cells;Films},
  doi={},
  ISSN={2159-5100},
  month={July},}

@ARTICLE{9788568,
  author={},
  journal={IEEE P2941.1/D2, June 2022},
  title={IEEE Draft Standard for Operator Interfaces of Artificial Intelligence},
  year={2022},
  volume={},
  number={},
  pages={1-346},
  abstract={A set of operator interfaces frequently used in artificial intelligence (AI) applications is defined in this standard, where the AI operators refer to the standard building blocks and primitives for performing basic AI operations. The functionality and the specific input and output operands of an AI operator are discussed, as well as both generality and efficiency. Various types of operators, such as those related to basic mathematics, neural network, and machine learning, are highlighted.},
  keywords={IEEE Standards;Artificial intelligence;Neural networks;Machine learning;Mathematics;artificial intelligence;AI;basic mathematics;IEEE 2941.1;neural network;machine learning;operators},
  doi={},
  ISSN={},
  month={June},}@ARTICLE{9855479,
  author={},
  journal={P2941.1/D3, August 2022},
  title={IEEE Approved Draft Standard for Operator Interfaces of Artificial Intelligence},
  year={2022},
  volume={},
  number={},
  pages={1-357},
  abstract={A set of operator interfaces frequently used in artificial intelligence (AI) applications is defined in this standard, where the AI operators refer to the standard building blocks and primitives for performing basic AI operations. The functionality and the specific input and output operands of an AI operator are discussed, as well as both generality and efficiency. Various types of operators, such as those related to basic mathematics, neural network, and machine learning, are highlighted.},
  keywords={IEEE Standards;Artificial intelligence;Machine learning;Neural networks;Mathematics;User interfaces;artificial intelligence;AI;basic mathematics;IEEE 2941.1;neural network;machine learning;operators},
  doi={},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{6970153,
  author={Pereira, Denilson Alves and Braga da Silva, Eduardo Emanuel and Esmin, Ahmed A. A.},
  booktitle={IEEE/ACM Joint Conference on Digital Libraries},
  title={Disambiguating publication venue titles using association rules},
  year={2014},
  volume={},
  number={},
  pages={77-86},
  abstract={Research agencies in several countries evaluate the impact of scientific publications of researcher groups to define their investments, and one of the main used metrics is the quality of the publication venues where their works were published. Several bibliometric indexes have been formulated by measuring the quality of a publication venue. However, given a set of citations extracted, for example, from curricula vitae of a researcher group, to effectively use bibliometric indexes to evaluate their quality it is necessary to identify correctly the publication venue title of each citation. This task is not easy, since there are not unique identifiers for publication venues. Frequently, citations contain abbreviated forms and acronyms, publication venues share similar titles, sometimes they change their titles, divide or merge, creating new ones. Traditional digital libraries deal with this problem by creating Authority Files. In this work, we present a twofold contribution: (i) the creation of a Computer Science publication venue authority file and (ii) the proposal of a method that uses association rules to disambiguate publication venue titles originated from citations. The disambiguator is a supervised learning method that uses the authority file to train a classifier, whose generated model is a set of association rules to identify publication venues. Experiments show that our method obtains better results than three state of art baselines.},
  keywords={Association rules;Computer science;Indexes;Training;Libraries;Measurement;Bibliometrics;Citation;Publication Venue;Association Rules;Authority File;Entity Resolution},
  doi={10.1109/JCDL.2014.6970153},
  ISSN={},
  month={Sep.},}@ARTICLE{182763,
  author={},
  journal={IEEE Std 610},
  title={IEEE Standard Computer Dictionary: A Compilation of IEEE Standard Computer Glossaries},
  year={1991},
  volume={},
  number={},
  pages={1-217},
  abstract={Identifies terms currently in use in the computer field. Standard definitions for thoseterms are established. Compilation of IEEE Stds IEEE Std 1084, IEEE Std 610.2, IEEE Std 610.3, IEEE Std 610.4, IEEE Std 610.5 and IEEE Std 610.12},
  keywords={Terminology;terminology;computer;applications;glossary;definitions;dictionary;610},
  doi={10.1109/IEEESTD.1991.106963},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10456055,
  author={Roy, Ankita and Garg, Atul},
  booktitle={2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG)},
  title={Bibliometric Analysis of Application of Artificial Intelligence in Heart Disease: 2013 to 2023},
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={The use of cutting-edge technologies in healthcare evolves continuously. Artificial intelligence offers a wide range of applications in the diagnosis, detection, or early prediction of many diseases. This research work was conducted to outline the current research directions in the area of artificial intelligence applications for cardiac disease. The various bibliometric analysis approaches are used between 2013 and 2023 to evaluate the potential for research in the field of applications of artificial intelligence in heart disease. Initially, search strategy 1 was used to retrieve 1128845 papers, while search strategy 2 was used to retrieve 581739 papers. Finally, the AND operator was used to combine search strategies 1 and 2 to include 50196 publications that perfectly matched our objectives. This research work found that: (1) The number of publications in the field of application of Artificial Intelligence in Heart Disease increased exponentially; (2) there is 33440 groups or organisation involved out of it 242 has at least 5 research publications; (3) There is 16187 keywords available with this research topic.},
  keywords={Heart;Bibliometrics;Government;Medical services;Search problems;Artificial intelligence;Diseases;Disease;Heart Disease;Health;Healthy Lives;Artificial Intelligence;Bibliometric Analysis},
  doi={10.1109/ICTBIG59752.2023.10456055},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9545979,
  author={Ni, Yuan and Zhang, Teng and Xu, Lei and Han, Pengfei},
  booktitle={2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)},
  title={Research on the Evolution Path of Sentiment Analysis Technology Based on Bibliometrics},
  year={2021},
  volume={},
  number={},
  pages={150-157},
  abstract={Sentiment analysis is a natural language processing technology with functions such as mining sentiment tendency classification and extracting sentiment features in texts. It is widely used in government and enterprise public opinion monitoring or product market research tasks. In order to understand the evolution process of the field of sentiment analysis at home and abroad, it is convenient for scholars to locate the knowledge structure and the development trend of the prediction field as a whole. This article uses social network tools such as CiteSpace and Pajek to collect 2260 document data in the core of Web of Science, and uses statistical analysis, co-occurrence and cluster analysis to reveal the phased characteristics of the evolution of the field and promote the object and the topic structure, through the co-citation analysis of the literature, the detection analysis of inflated words, the calculation of network characteristics, and the literature review method, it explains the phased evolutionary power, knowledge base and frontier hot spots from both qualitative and quantitative aspects. The main results show that the field of sentiment analysis can be divided into three stages according to the law of evolution, and the realization method dimension in the domain dimension is the main line of evolution throughout all stages.},
  keywords={Sentiment analysis;Landline;Statistical analysis;Social networking (online);Knowledge based systems;Tools;Market research;sentiment analysis;bibliometrics;opinion mining;frontier identification;evolutionary path},
  doi={10.1109/CAIBDA53561.2021.00039},
  ISSN={},
  month={May},}@ARTICLE{10669276,
  author={},
  journal={IEEE Std 1808-2024 (Revision of IEEE Std 1808-2011)},
  title={IEEE Guide for Collecting and Managing Transmission Line Inspection and Maintenance Data},
  year={2024},
  volume={},
  number={},
  pages={1-69},
  abstract={Reference information to assist electric utilities and their contractors with the development of computer-based means for collecting and managing transmission line inspection and maintenance data and associated asset information is provided. A high-level overview is provided in this guide for key principles and considerations learned through experience that help ensure common pitfalls are avoided and enhance the usability of systems and collected data. It is not intended to provide an exhaustive discussion of the many details and specifics accounted for when designing and developing a system including data requirements for an individual utility’s application and needs.},
  keywords={IEEE Standards;Artificial intelligence;Asset management;Conformance testing;Defect detection;Electromagnetic fields;Geographic information systems;Machine learning;Risk management;Regulation;Data science;AI;analytics;artificial intelligence;asset management;automated;compliance;data;database;data science;defect reporting;electric;electronic;field/virtual inspection;geographic information systems;GIS;IEEE 1808™;lines;machine learning;maintenance;ML;regulations;risk;transmission},
  doi={10.1109/IEEESTD.2024.10669276},
  ISSN={},
  month={Sep.},}@ARTICLE{10608112,
  author={Brée, Tim and Karger, Erik and Ahlemann, Frederik},
  journal={IEEE Access},
  title={Shaping the Future of Data Ecosystem Research—What Is Still Missing?},
  year={2024},
  volume={12},
  number={},
  pages={103162-103175},
  abstract={To share data, more and more companies work together in interorganizational networks called data ecosystems (DEs). Research on DEs has grown significantly over the past ten years. Owing to the research field’s recent growth, interested scholars and practitioners might find it difficult to keep up to date with the diverse research field dealing with DEs. In this article, we conduct the first bibliometric study on DE research and collect and quantitatively analyze the research within this field. As a result, we come up with a synthesized overview of the most relevant research constituents. Furthermore, we analyze the thematic structure of DE research and show the most frequently addressed topics and key themes. We found that DE research can be divided into five different thematic clusters. Based on these results, we finally develop a future research agenda that can help advance research on DEs and guide interested scholars. As such, our article summarizes and consolidates the existing knowledge on DEs and provides suggestions for its further development.},
  keywords={Bibliometrics;Ecosystems;Reviews;Companies;Standards organizations;Data integrity;Systematics;Bibliometric review;data ecosystem;data sharing;interorganizational network;quantitative analysis},
  doi={10.1109/ACCESS.2024.3432969},
  ISSN={2169-3536},
  month={},}@ARTICLE{9767818,
  author={Fang, Yu-Shen and Fang, Li-Chun},
  journal={IEEE Access},
  title={A Review of Chinese E-Commerce Research: 2001–2020},
  year={2022},
  volume={10},
  number={},
  pages={49015-49027},
  abstract={Electronic commerce (EC) has become the most critical business activity worldwide. China has become the world’s second largest economy. This study reviewed EC studies in China because although research on EC has yielded numerous results, limited research have reviewed these papers. The data used in this study were obtained from the Web of Science database. A total of 1,982 journal articles published between 2001 and 2020 were collected. In addition to conducting an overall analysis on EC in China, this study referred to the Five-Year Plan for Economic and Social Development of the People’s Republic of China and divided the research period into 5-year stages. The BibExcel, UCINET, and SPSS software programs were used to conduct co-word analysis on the keywords in the papers for determining the knowledge structure and clustering of each stage and understanding the trend of leading research in the future. The results indicated that (a) Stages I–IV comprised 4, 3, 4, and 4 clusters, respectively. (b) Consumer’s personalized demands were considered in EC development activities ranging from the initial EC infrastructure construction to the integration of artificial-intelligence-related technology. (c) Topics regarding consumer behaviors were centered on Stages II and III, which indicated that the research on these topics was mature. (d) Stage IV explored the new research topic of integrating smart technology into the EC environment and indicated the characteristics of the e-market.},
  keywords={Market research;Databases;Electronic commerce;Education;Statistical analysis;Libraries;Consumer behavior;Topic evolution;co-word analysis;research focus;e-service;cluster analysis},
  doi={10.1109/ACCESS.2022.3172433},
  ISSN={2169-3536},
  month={},}@ARTICLE{7192685,
  author={Heimerl, Florian and Han, Qi and Koch, Steffen and Ertl, Thomas},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  title={CiteRivers: Visual Analytics of Citation Patterns},
  year={2016},
  volume={22},
  number={1},
  pages={190-199},
  abstract={The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.},
  keywords={Visualization;Market research;Metadata;Tag clouds;Data mining;Color;Joining processes;scientific literature;visual document analysis;visual citation analysis;streamgraph;clustering;scientific literature;visual document analysis;visual citation analysis;streamgraph;clustering},
  doi={10.1109/TVCG.2015.2467621},
  ISSN={1941-0506},
  month={Jan},}@INPROCEEDINGS{10303626,
  author={Secco, Cristian A. and Sina, Lennart B. and Blazevic, Midhad and Nazemi, Kawa},
  booktitle={2023 27th International Conference Information Visualisation (IV)},
  title={Visual Analytics for Forecasting Technological Trends from Text},
  year={2023},
  volume={},
  number={},
  pages={251-258},
  abstract={Knowledge of emerging and declining trends and their potential future course is highly relevant in many application domains, particularly in corporate strategy and foresight. The early awareness of trends allows reacting to market, political, and societal changes and challenges at an appropriate time. In our previous works, we presented approaches for the early identification and analysis of emerging trends. Although our previous approaches are detecting emerging trends appropriately, they lack the ability to predict the potential future course of a trend or technology. We present in this work a novel Visual Analytics approach for forecasting emerging trends that combines interactive visualizations with machine learning techniques and statistical approaches to detect, analyze, and predict trends from textual data. We extend our previous work on analyzing technological trends from text and propose an advanced approach that includes forecasting through hybrid techniques consisting of neural networks and established statistical methods. Our approach offers insights from enormous data sets and the potential future course of trends based on their occurrence in textual data. We contribute with a novel approach for identifying and forecasting trends, a hybrid forecasting method to predict trends from text, and interactive visualization techniques on macro level, micro level, and monitoring topics of interest.},
  keywords={Statistical analysis;Visual analytics;Neural networks;Data visualization;Machine learning;Market research;Forecasting;Visual Analytics;emerging trends;trend detection;trend forecasting;machine learning;neural network;hybrid prediction},
  doi={10.1109/IV60283.2023.00051},
  ISSN={2375-0138},
  month={July},}@INPROCEEDINGS{9809429,
  author={Shehata, Boulus and Tlili, Ahmed and Huang, Ronghuai},
  booktitle={2021 8th International Conference on ICT & Accessibility (ICTA)},
  title={An Analysis of International Conference Proceedings on Artificial General Intelligence (AGI) from 2008 to 2020: A Data-Mining Mapping Analysis},
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper analyzes the proceedings of 13 international conferences on Artificial General Intelligence AGI, that are part of the lecture notes in artificial intelligence. The motivation for this study is to explore the publication networks and areas where AGI could be useful to Information & Communication Technology ICT in education. SPSS and Microsoft Excel were used to enter, screen, clean and visualize the analysis of the data. Furthermore, all published proceedings were uploaded into Voyant Tools and VOSviewer for data mining and visualization. After obtaining the most frequently used keywords and terms, certain patterns lead to trends in the research on AGI. The analysis leads to discussions and suggestions of possible opportunities where AGI may be integrated with education to assist people with or without a disability. The contribution of the study is in the overall mapping analysis of the conference proceedings, which would inspire further exploration and collaboration with potential scholars, institutions, and countries.},
  keywords={Electronic learning;Education;Government;Collaboration;Data visualization;Market research;Information and communication technology;Artificial General Intelligence;International Conference;Proceedings;Data Mining Analysis},
  doi={10.1109/ICTA54582.2021.9809429},
  ISSN={2379-4402},
  month={Dec},}@ARTICLE{10146430,
  author={Nunes, Lucas dos Santos and Moreno, Edward David and Carneiro, Glauco de Figueiredo},
  journal={IEEE Transactions on Computational Social Systems},
  title={A Review of COVID-19 Data Usage in Correctional Institutions From Developing Countries},
  year={2024},
  volume={11},
  number={2},
  pages={2740-2754},
  abstract={COVID-19 is highly transmissible and pathogenic, representing a potential threat to health conditions in correctional environments worldwide. This threat could worsen in prison systems in developing countries due to high crowding levels, lack of individual hygiene materials, inadequate primary sanitary conditions, and difficulties in using health services. Attempts to provide a landscape of academic research findings on COVID-19 data usage from prison systems in developing countries have been limited. This scenario motivated us to carry out this systematic mapping study (SMS) to provide a comprehensive overview of the impacts that may prevent the effective use of COVID-19 data, critical issues, and gaps in the current academic research. Following the steps of a systematic mapping, we examined 79 studies returned from a search string and eight studies returned from the use of the snowballing technique and selected 37 from related journals and conferences published between 2020 and 2021. The results point to possible problems in the use of COVID-19, preventing its effective use in information systems.},
  keywords={COVID-19;Developing countries;Databases;Systematics;Public healthcare;Soft sensors;Presses;COVID-19;machine learning;open government data;systematic mapping (SM)},
  doi={10.1109/TCSS.2023.3280912},
  ISSN={2329-924X},
  month={April},}@ARTICLE{8811578,
  author={Sarmento, Róger M. and Vasconcelos, Francisco F. Ximenes and Filho, Pedro P. Rebouças and Wu, Wanqing and de Albuquerque, Victor Hugo C.},
  journal={IEEE Reviews in Biomedical Engineering},
  title={Automatic Neuroimage Processing and Analysis in Stroke—A Systematic Review},
  year={2020},
  volume={13},
  number={},
  pages={130-155},
  abstract={This article presents a systematic review of the current computational technologies applied to medical images for the detection, segmentation, and classification of strokes. Besides, analyzing and evaluating the technological advances, the challenges to be overcome and the future trends are discussed. The principal approaches make use of artificial intelligence, digital image processing and analysis, and various other technologies to develop computer-aided diagnosis (CAD) systems to improve the accuracy in the diagnostic process, as well as the interpretation consistency of medical images. However, there are some points that require greater attention such as low sensitivity, optimization of the algorithm, a reduction of false positives, and improvement in the identification and segmentation processes of different sizes and shapes. Also, there is a need to improve the classification steps of different stroke types and subtypes. Furthermore, there is an additional need for further research to improve the current techniques and develop new algorithms to overcome disadvantages identified here. The main focus of this research is to analyze the applied technologies for the development of CAD systems and verify how effective they are for stroke detection, segmentation, and classification. The main contributions of this review are that it analyzes only up-to-date studies, mainly from 2015 to 2018, as well as organizing the various studies in the area according to the research proposal, i.e., detection, segmentation, and classification of the types of stroke and the respective techniques used. Thus, the review has great relevance for future research, since it presents an ample comparison of the most recent works in the area, clearly showing the existing difficulties and the models that have been proposed to overcome such difficulties.},
  keywords={Stroke (medical condition);Medical diagnostic imaging;Neuroimaging;Magnetic resonance imaging;Computed tomography;Image segmentation;Artificial intelligence;CAD system;classification;detection;neuroimaging;segmentation;stroke},
  doi={10.1109/RBME.2019.2934500},
  ISSN={1941-1189},
  month={},}@INPROCEEDINGS{7425795,
  author={Dong, Zhaoan and Lu, Jiaheng and Ling, Tok Wang},
  booktitle={2016 International Conference on Big Data and Smart Computing (BigComp)},
  title={PANDA: A platform for academic knowledge discovery and acquisition},
  year={2016},
  volume={},
  number={},
  pages={10-17},
  abstract={Scientific literatures contain some academic knowledge which is interesting or valuable but previously unknown. For instance, an algorithm A proposed in one article might have association with algorithm B in another article, while algorithm B is designed based on the definition of C in a third article. Thus we can deduce the relationship A-C based on A-B and B-C. There are also other kinds of academic knowledge such as association between two research communities, historical evolvement of a research topics, etc. But with the exponential growth of research articles that usually published in Portable Document Format (PDF), to discover and acquire potential knowledge poses many practical challenges. Existing algorithmic methods can hardly extend to handle diverse journals and layouts, nor scale up to process massive documents. As crowdsourcing has become a powerful paradigm for problem-solving especially for tasks that are difficult for computer to resolve solely, we state the problem of academic knowledge discovery and acquisition using an hybrid framework, integrating the accuracy of human workers and the speed of automatic algorithms. We briefly introduce a Platform for Academic kNowledge Discovery and Acquisition (PANDA), our current system implementation, as well as some preliminary achievements and promising future directions.},
  keywords={Crowdsourcing;Portable document format;Computer architecture;Knowledge discovery;Layout;Context;Microprocessors},
  doi={10.1109/BIGCOMP.2016.7425795},
  ISSN={2375-9356},
  month={Jan},}@ARTICLE{8101457,
  author={Ki, Wanwook and Kim, Kwangsoo},
  journal={IEEE Access},
  title={Generating Information Relation Matrix Using Semantic Patent Mining for Technology Planning: A Case of Nano-Sensor},
  year={2017},
  volume={5},
  number={},
  pages={26783-26797},
  abstract={For the purposes of technology planning and research and development strategy development, we present a semi-automated method that extracts text information from patent data, uses natural language processing to extract the key technical information of the patent, and then visualizes this information in a matrix form. We tried to support qualitative analysis of patent contents by extracting functions, components, and contexts, which are the most important information about inventions. We validated the method by applying it to patent data related to nanosensors. The matrix can emphasize technical information that have not been exploited in patents, and thereby identify development opportunities.},
  keywords={Patents;Market research;Text mining;Semantics;Nanosensors;Tools;Information relation matrix;natural language processing;patent analysis;patent matrix;semantic patent mining;technical information;technology planning;text mining},
  doi={10.1109/ACCESS.2017.2771371},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{7354596,
  author={Abbasi, Alireza},
  booktitle={2015 International Conference on Information and Communication Technology Convergence (ICTC)},
  title={Reviewing academic social network mining applications},
  year={2015},
  volume={},
  number={},
  pages={503-508},
  abstract={Social network systems not only play important roles in peoples' social life but also help them in their professional life too. This study explores academic social network systems and particularly focuses on the data extraction methods to develop social relations. Several studies have used search engines to extract social networks from the Web and some systems get their data by description of their members' relation to others like Social Network Services or Friend-Of-A-Friend documents. Both types of data gathering have their own limitations such as data unreliability and scalability of queries to search engines. In this paper, a basic understanding of the issues in academic social network systems is provided. Different approaches to various problems of retrieving reliable data (relational information), as the major item of making these systems, are briefly described, and possible future research directions are discussed.},
  keywords={Social network services;Semantic Web;Search engines;Ontologies;Collaboration;Web pages;Social network services (SNS);web mining;social network systems;knowledge sharing},
  doi={10.1109/ICTC.2015.7354596},
  ISSN={},
  month={Oct},}@ARTICLE{250082,
  author={Conklin, D. and Fortier, S. and Glasgow, J.},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={Knowledge discovery in molecular databases},
  year={1993},
  volume={5},
  number={6},
  pages={985-987},
  abstract={An approach to knowledge discovery in complex molecular databases is described. The machine learning paradigm used is structured concept formation, in which object's described in terms of components and their interrelationships are clustered and organized in a knowledge base. Symbolic images are used to represent classes of structured objects. A discovered molecular knowledge base is successfully used in the interpretation of a high resolution electron density map.<>},
  keywords={Logic;Proteins;Image analysis;Image databases;Image reconstruction;Intelligent robots;Knowledge representation;Machine learning;Spatial resolution;Electrons},
  doi={10.1109/69.250082},
  ISSN={1558-2191},
  month={Dec},}@INPROCEEDINGS{10152943,
  author={Ayad, Manal and Siadat, Ali and Hamlich, Mohamed},
  booktitle={2023 3rd International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)},
  title={Joint consideration of Production, Quality and Maintenance: a Bibliometric Analysis},
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Dealing with the complexity of smart industries, and increasing their productivity has always been the research focus of researchers aiming at meeting the requirements of Production, Quality and Maintenance, as three fundamental pillars of industries. Given the direct link of this triplet, studies acting on these three functions are, generally, divided into two main types of optimization: Partial integration which considers jointly Production-Quality or Production-Maintenance, or Maintenance-Quality for optimizing productivity, and Full integration that consider them all. To understand the publications trends using the joint consideration of Production, Quality and Maintenance of the existing studies and to provide a holistic view, this paper performs a Systematic Literature Network Analysis SLNA that undertakes a bibliometric analysis from 1977 by the mean of: Performance Analysis and Science mapping using VosViewer. The main contribution of this paper is the classification of the investigated papers into thematics and the study of the relationships among the most frequent terms in theses papers. Analyses of 2306 original publications demonstrated that the joint consideration of Production and Quality is the oldest and the Joint consideration of Production-Maintenance is the most widely used, whereas the full consideration of Production, Quality and Maintenance is recent and related to advanced smart tools.},
  keywords={Productivity;Industries;Systematics;Bibliometrics;Network analyzers;Maintenance engineering;Market research;Production;Quality;Maintenance;Bibliometric Analysis},
  doi={10.1109/IRASET57153.2023.10152943},
  ISSN={},
  month={May},}@INPROCEEDINGS{9357075,
  author={Fortiş, Teodor-Florin and Fortiş, Alexandra-Emilia},
  booktitle={2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)},
  title={A bibliometric overview of the International Symposium on Symbolic and Numeric Algorithms for Scientific Computing between 2005 and 2018},
  year={2020},
  volume={},
  number={},
  pages={278-285},
  abstract={Current research offers a bibliometric overview of the International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, from 2005 to 2018, from different perspectives, in order to highlight the generated impact, the dimensions and strength of international collaborations, as well as a statistical study of conference papers, typical structure of collaboration groups, evolution of research trends, and others. Associated findings are presented either as raw data, or processed via VOSViewer.},
  keywords={Couplings;Scientific computing;Bibliometrics;Collaboration;Market research;International collaboration;Portals;bibliometrics;conference analysis;VOSViewer;Web of Science;Scopus},
  doi={10.1109/SYNASC51798.2020.00052},
  ISSN={},
  month={Sep.},}@ARTICLE{9738599,
  author={Gurcan, Fatih and Cagiltay, Nergiz Ercil},
  journal={IEEE Access},
  title={Exploratory Analysis of Topic Interests and Their Evolution in Bioinformatics Research Using Semantic Text Mining and Probabilistic Topic Modeling},
  year={2022},
  volume={10},
  number={},
  pages={31480-31493},
  abstract={Bioinformatics, which has developed rapidly in recent years with the collaborative contributions of the fields of biology and informatics, provides a deeper perspective on the analysis and understanding of complex biological data. In this regard, bioinformatics has an interdisciplinary background and a rich literature in terms of domain-specific studies. Providing a holistic picture of bioinformatics research by analyzing the major topics and their trends and developmental stages is critical for an understanding of the field. From this perspective, this study aimed to analyze the last 50 years of bioinformatics studies (a total of 71,490 articles) by using an automated text-mining methodology based on probabilistic topic modeling to reveal the main topics, trends, and the evolution of the field. As a result, 24 major topics that reflect the focuses and trends of the field were identified. Based on the discovered topics and their temporal tendencies from 1970 until 2020, the developmental periods of the field were divided into seven phases, from the “newborn” to the “wisdom” stages. Moreover, the findings indicated a recent increase in the popularity of the topics “Statistical Estimation”, “Data Analysis Tools”, “Genomic Data”, “Gene Expression”, and “Prediction”. The results of the study revealed that, in bioinformatics studies, interest in innovative computing and data analysis methods based on artificial intelligence and machine learning has gradually increased, thereby marking a significant improvement in contemporary analysis tools and techniques based on prediction.},
  keywords={Bioinformatics;Market research;Biology;Analytical models;Genomics;Proteins;Computational modeling;Bioinformatics corpus;probabilistic topic modeling;textual content analysis;scientometric analysis;bioinformatics topics and trends},
  doi={10.1109/ACCESS.2022.3160795},
  ISSN={2169-3536},
  month={},}@BOOK{9453327,
  author={Hand, Kevin},
  title={Alien Oceans: The Search for Life in the Depths of Space},
  year={2019},
  volume={},
  number={},
  pages={},
  abstract={Inside the epic quest to find life on the water-rich moons at the outer reaches of the solar systemWhere is the best place to find life beyond Earth? We often look to Mars as the most promising site in our solar system, but recent scientific missions have revealed that some of the most habitable real estate may actually lie farther away. Beneath the frozen crusts of several of the small, ice-covered moons of Jupiter and Saturn lurk vast oceans that may have existed for as long as Earth, and together may contain more than fifty times its total volume of liquid water. Could there be organisms living in their depths? Alien Oceans reveals the science behind the thrilling quest to find out.Kevin Peter Hand is one of today's leading NASA scientists, and his pioneering research has taken him on expeditions around the world. In this captivating account of scientific discovery, he brings together insights from planetary science, biology, and the adventures of scientists like himself to explain how we know that oceans exist within moons of the outer solar system, like Europa, Titan, and Enceladus. He shows how the exploration of Earth's oceans is informing our understanding of the potential habitability of these icy moons, and draws lessons from what we have learned about the origins of life on our own planet to consider how life could arise on these distant worlds.Alien Oceans describes what lies ahead in our search for life in our solar system and beyond, setting the stage for the transformative discoveries that may await us.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691189642},
  url={https://ieeexplore.ieee.org/document/9453327},}@ARTICLE{9745089,
  author={Gomez, Manuel J. and Ruipérez-Valiente, José A. and García Clemente, Félix J.},
  journal={IEEE Access},
  title={Analyzing Trends and Patterns Across the Educational Technology Communities Using Fontana Framework},
  year={2022},
  volume={10},
  number={},
  pages={35336-35351},
  abstract={Nowadays, the use of technology in continuously increasing, making a significant impact in almost every area, including education. New areas have gained much popularity in the last years in educational technology (EdTech), such as Massive Open Online Courses (MOOCs) or computer-supported collaborative learning. In addition, research and interest in this area have also been growing over the years. The quantity of research and scientific publications in EdTech is constantly increasing, and trying to analyze and extract information from a set of research papers is often a very time-consuming task. To make this process easier and solve these limitations, we present Fontana, a framework that can quickly perform trend and social network analysis using any corpus of documents and its metadata. Specifically, the framework can: 1) Discover the latest trends given any corpus of documents, using Natural Language Processing (NLP) analysis and keywords (bibliometric approach); 2) Discover the evolution of the trends previously identified over the years; 3) Discover the primary authors and papers, along with hidden relationships between existing communities. To test its functionality, we evaluated the framework using a corpus of papers from the EdTech research field. We also followed an open science methodology making the entire framework available in Open Science Framework (OSF) easy to access and use. The case study successfully proved the capabilities of the framework, revealing some of the most frequent topics in the area, such as “EDM,” “learning analytics,” or “collaborative learning.” We expect our work to help identifying trends and patterns in the EdTech area, using natural language processing and social network analysis to objectively process large amounts of research.},
  keywords={Bibliometrics;Market research;Task analysis;Metadata;Social networking (online);Databases;Network analyzers;EdTech;data mining;bibliometrics;NLP;network analysis;topic modeling},
  doi={10.1109/ACCESS.2022.3163253},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10020918,
  author={Belz, Andrea and Graddy-Reed, Alexandra and Shweta, Fnu and Giga, Aleksandar and Murali, Shivesh Meenakshi},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)},
  title={Patentopia: A multi-stage patent extraction platform with disambiguation for certain semantic challenges},
  year={2022},
  volume={},
  number={},
  pages={3478-3485},
  abstract={Bibliographic name disambiguation is an major semantic challenge, but critical to social sciences studies of important intellectual assets. Here we contribute to innovation research in several ways. We show a significant synonym problem in author names and discuss how a pre-processing heuristic step standardizing name variants helps, but homonyms generated with Chinese names are particularly difficult to resolve and manifest in an associated location list. Here we identify a new phenomenon of "onomastic profusion," the frequent use of certain words in firm names for semantic reasons that can confound disambiguation clustering algorithms. We illustrate these concerns with Patentopia, our customized platform accessing the PatentsView portal for the United States Patent and Trademark Office database and available for free academic use. This multi-stage system uses heuristics in concert with the PatentsView clustering process and reports meta-data to further assist analysis. As highly relevant use cases, we illustrate system performance with data derived from two important public innovation programs, I-Corps and Small Business Innovation Research (SBIR), and we close with implications for bibliometric analysis of current patent data.},
  keywords={Technological innovation;Patents;Databases;System performance;Semantics;Bibliometrics;Social sciences;disambiguation;patents;NLP;bibliometric;SBIR;I-Corps},
  doi={10.1109/BigData55660.2022.10020918},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9744877,
  author={Luling, Xie and Zixi, Wang and Yeqiu, Chen and Yichao, Wen and Di, Zhao},
  booktitle={2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)},
  title={COVID-19 Literature Mining and Analysis Research},
  year={2022},
  volume={},
  number={},
  pages={1045-1052},
  abstract={By 2019 COVID-19, since the epidemic, the number of relevant documents exponentially level rise. Faced with a large amount of literature, this research provides convenience for exploring the connection between research topics and fields and quickly understanding relevant literature information. We pass on the data set after data cleansing using the LDA(Latent Dirichlet allocation) methods, and Berts and K-means modeling method extracting topic keywords. Use knowledge graph tools to output relevant visual graphics and systematically extract adequate information. Through text mining of biomedical research papers related to COVID-19, the improved model is used to analyze and make recommendations to respond to and prevent the COVID-19 pandemic. This research can support the rapid and in-depth analysis of a large number of relevant documents and can be used in future research to support real-time scientific disease research.},
  keywords={COVID-19;Text mining;Electrical engineering;Analytical models;Visualization;Pandemics;Conferences;literature topic modeling;COVID-19;bibliometrics;machine learning},
  doi={10.1109/EEBDA53927.2022.9744877},
  ISSN={},
  month={Feb},}@ARTICLE{10557630,
  author={Denden, Mohsen and Jemmali, Mahdi and Boulila, Wadii and Soni, Mukesh and Khan, Faheem and Ahmad, Jawad},
  journal={IEEE Transactions on Consumer Electronics},
  title={Clustering-Based Resource Management for Consumer Cost Optimization in IoT Edge Computing Environments},
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={Edge computing emerges as a pivotal model in the era of next-generation consumer electronics and the emerging challenges of multimodal data-driven decision-making. Specifically, edge computing offers an open computing architecture for the vast and diverse consumer multimodal data generated by mobile computing and Internet of Things (IoT) technologies. While edge computing is instrumental in optimizing latency and bandwidth control in processing consumer multimodal data, the viability of employing edge resources is complicated by high service costs and the complexities of managing multimodal data diversity. This study introduces an innovative optimization method for distributing multimodal data on edge storage, considering both the I/O (input/output) speed and the overall distribution cost. The core part of our approach is the deployment of intelligent algorithms that ensure equitable data distribution across storage servers, thus eliminating unused space and reducing extra costs. Given the complexity of this NP-hard (non-deterministic polynomial-time) challenge, our study reveals a unique model incorporating an edge-broker component in combination with novel algorithms. The proposed algorithms aim to harmonize data distribution and reduce resource allocation expenses in a multimodal edge environment. Our proposed approach achieves excellent results, highlighting the efficacy of the proposed algorithms in several parameters such as makespan, cost, multimodal data security, and total processing time. Empirical tests reveal that the BCA (Best Clustering Algorithm) performs best, achieving a minimum load balancing rate of 92.2%, an average variance of 0.04, and an average run time of 0.56 seconds.},
  keywords={Costs;Internet of Things;Servers;Clustering algorithms;Scheduling;Edge computing;Task analysis;Consumer Cost;edge workflow;clustering;randomization;load balancing},
  doi={10.1109/TCE.2024.3414929},
  ISSN={1558-4127},
  month={},}@ARTICLE{9319154,
  author={Roman, Muhammad and Shahid, Abdul and Khan, Shafiullah and Koubaa, Anis and Yu, Lisu},
  journal={IEEE Access},
  title={Citation Intent Classification Using Word Embedding},
  year={2021},
  volume={9},
  number={},
  pages={9982-9995},
  abstract={Citation analysis is an active area of research for various reasons. So far, statistical approaches are mainly used for citation analysis, which does not look into the internal context of the citations. Deep analysis of citation may reveal interesting findings by utilizing deep neural network algorithms. The existing scholarly datasets are best suited for statistical approaches but lack citation context, intent, and section information. Furthermore, the datasets are too small to be used with deep learning approaches. For citation intent analysis, the datasets must have a citation context labeled with different citation intent classes. Most of the datasets either do not have labeled context sentences, or the sample is too small to be generalized. In this study, we critically investigated the available datasets for citation intent and proposed an automated citation intent technique to label the citation context with citation intent. Furthermore, we annotated ten million citation contexts with citation intent from Citation Context Dataset (C2D) dataset with the help of our proposed method. We applied Global Vectors (GloVe), Infersent, and Bidirectional Encoder Representations from Transformers (BERT) word embedding methods and compared their Precision, Recall, and F1 measures. It was found that BERT embedding performs significantly better, having an 89% Precision score. The labeled dataset, which is freely available for research purposes, will enhance the study of citation context analysis. Finally, It can be used as a benchmark dataset for finding the citation motivation and function from in-text citations.},
  keywords={Metadata;Citation analysis;Computational modeling;Context modeling;Task analysis;Semantics;Citation intent;citation analysis;citation context;citation motivation;citation function classification;word embedding;scholarly dataset},
  doi={10.1109/ACCESS.2021.3050547},
  ISSN={2169-3536},
  month={},}@ARTICLE{8636502,
  author={Asim, Muhammad Nabeel and Wasim, Muhammad and Ghani Khan, Muhammad Usman and Mahmood, Nasir and Mahmood, Waqar},
  journal={IEEE Access},
  title={The Use of Ontology in Retrieval: A Study on Textual, Multilingual, and Multimedia Retrieval},
  year={2019},
  volume={7},
  number={},
  pages={21662-21686},
  abstract={Web contains a vast amount of data, which are accumulated, studied, and utilized by a huge number of users on a daily basis. A substantial amount of data on the Web is available in an unstructured format, such as Web pages, books, journals, and files. Acquiring appropriate information from such humongous data has become quite challenging and a time-consuming task. Trivial keyword-based information retrieval systems highly depend on the statistics of data, thus facing word mismatch problem due to inevitable semantic and context variations of a certain word. Therefore, this marks the desperate need to organize such massive data into a structured format so that information can be easily processed in a large context by taking data semantics into account. Ontologies are not only being extensively employed in the semantic Web to store unstructured information in an organized and structured way but it has also raised the performance of diverse information retrieval approaches to a great extent. Ontological information retrieval systems retrieve data based on the similarity of semantics between the user query and the indexed data. This paper reviews modern ontology-based information retrieval methods for textual, multimedia, and cross-lingual data types. Furthermore, we compare and categorize the most recent approaches used in the above-mentioned information retrieval methods along with their major drawbacks and advantages.},
  keywords={Semantics;Ontologies;Semantic Web;Task analysis;Search engines;Natural language processing;Ontology;text retrieval;multimedia retrieval;cross lingual retrieval},
  doi={10.1109/ACCESS.2019.2897849},
  ISSN={2169-3536},
  month={},}@ARTICLE{10731951,
  author={Xiong, Zhitong and Zhang, Fahong and Wang, Yi and Shi, Yilei and Zhu, Xiao Xiang},
  journal={IEEE Geoscience and Remote Sensing Magazine},
  title={EarthNets: Empowering artificial intelligence for Earth observation},
  year={2024},
  volume={},
  number={},
  pages={2-36},
  abstract={Earth observation (EO), aiming at monitoring the state of planet Earth using remote sensing data, is critical for improving our daily lives and living environment. With a growing number of satellites in orbit, an increasing number of datasets with diverse sensors and research domains are being published to facilitate the research of the remote sensing community. This paper presents a comprehensive review of more than 500 publicly published datasets, including research domains like agriculture, land use and land cover, disaster monitoring, scene understanding, vision-language models, foundation models, climate change, and weather forecasting. We systematically analyze these EO datasets from four aspects: volume, resolution distributions, research domains, and the correlation between datasets. Based on the dataset attributes, we propose to measure, rank, and select datasets to build a new benchmark for model evaluation. Furthermore, a new platform for EO, termed EarthNets, is released to achieve a fair and consistent evaluation of deep learning methods on remote sensing data. EarthNets supports standard dataset libraries and cutting-edge deep learning models to bridge the gap between the remote sensing and machine learning communities. Based on this platform, extensive deep-learning methods are evaluated on the new benchmark. The insightful results are beneficial to future research. The platform and dataset collections are publicly available at https://earthnets.github.io/.},
  keywords={Artificial intelligence;Climate change;Satellite images;Earth Observing System;Environmental monitoring;Remote sensing;Deep learning;Benchmark testing},
  doi={10.1109/MGRS.2024.3466998},
  ISSN={2168-6831},
  month={},}@INPROCEEDINGS{10126745,
  author={Glory, K.B. and Venkatesan, D. and Devi, G. Naga Rama and Kiran, Chappeli Sai and Khairnar, Prerana Nilesh and Priya, S.},
  booktitle={2022 IEEE North Karnataka Subsection Flagship International Conference (NKCon)},
  title={An Effective Storage Management for University Library using Weighted K-Nearest Neighbor Algorithm},
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={The most fascinating topic in economic geography is the storage location-allocation problem. The storage serves as a transition point to lower the cost of transmission. To produce an accurate and approximate response, two a model based hybrid of k-means –Particle Swarm Optimization(KPSO) proposed in this work. When compared to the existing model, the proposed model formulation is simpler and easier to understand. The testing findings show that the proposed model makes better use of the computer's Random Access Memory (RAM), allowing us to solve medium-sized tasks. This approach cannot outperform the MIP model in terms of run time. The multi-assignment facility location queries are included in the extension of the CP formulations. Initial PSO solutions are produced using the well-known data clustering technique K-means. The experimental results demonstrate that in terms of time, objective value, and reliability of performance metrics, the KPSO method is superior to the PSO.},
  keywords={Runtime;Computational modeling;Storage management;Random access memory;Clustering algorithms;Transportation;Time factors;Transportation model;SCM;Material management;K-means algorithm},
  doi={10.1109/NKCon56289.2022.10126745},
  ISSN={},
  month={Nov},}@ARTICLE{10599101,
  author={Olu-Ajayi, Razak and Alaka, Hafiz and Sunmola, Funlade and Ajayi, Saheed and Mporas, Iosif},
  journal={IEEE Transactions on Engineering Management},
  title={Statistical and Artificial Intelligence-Based Tools for Building Energy Prediction: A Systematic Literature Review},
  year={2024},
  volume={71},
  number={},
  pages={14733-14753},
  abstract={The application of statistical and artificial intelligence (AI) tools in building energy prediction (BEP) is considered one of the most effective advances toward improving energy efficiency. Thus, researchers are constantly propagating the energy prediction field with many prediction models using diverse statistical and AI tools. However, many of these tools are employed in unsuitable data conditions or for wrong situations. Using the Institute of Electrical and Electronics Engineers and Scopus databases, 92 journal articles on statistical and AI tools in BEP were systematically analyzed. Furthermore, a quantitative bibliometric analysis was conducted to pinpoint the trends and examine knowledge gaps. This research reviews the performance of nine popular and promising statistical and AI tools with a primary focus on seven pertinent criteria within the building energy research domain. Although it was concluded that no one tool is best in all criteria, a diagrammatic framework is provided to serve as a guide for appropriate tool selection in various situations. This study contributes to appropriate tool selection in the development of BEP models and their related drawbacks. In addition, this study also evaluated the performance of the high-performing tools on a standard dataset.},
  keywords={Reviews;Buildings;Systematics;Support vector machines;Predictive models;Energy consumption;Bibliographies;Artificial intelligence (AI);building energy consumption;energy efficiency;energy prediction;machine learning;statistical tools;systematic literature review},
  doi={10.1109/TEM.2024.3422821},
  ISSN={1558-0040},
  month={},}@ARTICLE{9093868,
  author={Ang, Kenneth Li-Minn and Ge, Feng Lu and Seng, Kah Phooi},
  journal={IEEE Access},
  title={Big Educational Data & Analytics: Survey, Architecture and Challenges},
  year={2020},
  volume={8},
  number={},
  pages={116392-116414},
  abstract={The proliferation of mobile devices and the rapid development of information and communication technologies (ICT) have seen increasingly large volume and variety of data being generated at an unprecedented pace. Big data have started to demonstrate significant values in higher education. This paper gives several contributions to the state-of-the-art for Big data in higher education and learning technologies research. Currently, there is no comprehensive survey or literature review for Big educational data. Most literature reviews from a few authors have focused on one of these fields: educational mining, learning analytics with discussions on one or two aspects such as Big data technologies without educational focus, social media data in education, etc. Most of these literature reviews are short and insufficient to provide more inclusive reviews for Big educational data. In this paper, we present a comprehensive literature review of the current and emerging paradigms for Big educational data. The survey is presented in five parts: (1) The first part presents an overview and classification of Big education research to show the full landscape in this field, which also gives a concise summary of the overall scope of this paper; (2) The second part presents a discussion for the various data sources from education platforms or systems including learning management systems (LMS), massive open online courses (MOOC), learning object repository (LOR), OpenCourseWare (OCW), open educational resources (OER), social media, linked data and mobile learning contributing to Big education data; (3) The third part presents the data collection, data mining and databases in Big education data; (4) The fourth part presents the technological aspects including Big data platforms and architectures such as Hadoop, Spark, Samza and Big data tools for Big education data; and (5) The fifth part presents different approaches of data analytics for Big education data. This part provides a more inclusive discussion on data analytics which is beyond traditional forms of learning analysis in higher education. This includes predictive analytics, learning analytics including collaborative, behavior, personal learnings and assessment, followed by recommendation systems, graph analytics, visual analytics, immersive learning and analytics, etc. The final part of the paper discusses social (e.g. privacy and ethical issues) and technological challenges for Big data in education. This part also illustrates the technological challenges faced by giving an example for utilizing graph-based analytics for a cross-institution learning analytics scenario.},
  keywords={Education;Big Data;Bibliographies;Data mining;Data analysis;Computer architecture;Social network services;Big data;learning technologies;educational data;learning analytics},
  doi={10.1109/ACCESS.2020.2994561},
  ISSN={2169-3536},
  month={},}@ARTICLE{9197624,
  author={Wang, Hui and Le, Zichun and Gong, Xuan},
  journal={IEEE Access},
  title={Recommendation System Based on Heterogeneous Feature: A Survey},
  year={2020},
  volume={8},
  number={},
  pages={170779-170793},
  abstract={Recommendation systems have become an important field of research in computer science and physics. In recent years, breakthroughs have been achieved in social, biological, and research cooperation networks. With the popularization of big data and deep learning technology development, graph structures are increasingly being used to represent large-scale and complex data in the real world. In this paper, we reviewed the progress made in recommendation systems research in the past 20 years and comprehensively classified recommender systems based on the heterogeneous input features. We introduced layering in the classification of recommendation systems. Furthermore, we proposed a new hierarchical classification model of recommendation systems divided into three layers: feature input, feature learning, and output layers. In the feature learning layer, existing recommendation systems were divided into graph-based, text-based, behavior-based, spatiotemporal-based, and hybrid recommendation systems. Additionally, we provided evaluation index, open-source implementation, experimental comparison and the relative merits for each recommendation method. Subsequently, future development directions of recommendation systems are discussed.},
  keywords={Spatiotemporal phenomena;Recurrent neural networks;Semisupervised learning;Convolution;Computer science;Behavior features;feature;graph features;recommendation systems;text features},
  doi={10.1109/ACCESS.2020.3024154},
  ISSN={2169-3536},
  month={},}@INBOOK{8290848,
  author={Mackenzie, Adrian},
  booktitle={Machine Learners: Archaeology of a Data Practice},
  title={Machine Learners},
  year={2017},
  volume={},
  number={},
  pages={iii-xvi},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262342551},
  url={https://ieeexplore.ieee.org/document/8290848},}@INPROCEEDINGS{10633654,
  author={Kumar, Amit and Hrishikesh, Ethari and Deasi, Yugandhar and Agarwal, Sonali},
  booktitle={2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)},
  title={Prevalence and Prediction of Unseen Co-Changes: A Graph-Based Approach},
  year={2024},
  volume={},
  number={},
  pages={1157-1167},
  abstract={Co-changes refer to the phenomenon wherein two or more software entities are modified together within the same commit or when they are changed to accomplish a specific task or functionality. The key to accurate co-change prediction lies in effectively predicting unseen co-changes-those occurring between entities that have not been co-changed before. However, despite considerable research on co-change patterns and prediction, there remains a significant gap in understanding unseen co-changes, including their prevalence, complexity, and predictability. We model co-changes as a graph, treating unseen co-change prediction as a link prediction task. Our method leverages file proximity measures derived from both homogeneous and heterogeneous networks, alongside other similarity measures, to predict these unseen co-changes. Analysis of 14 Apache Software Foundation projects revealed a significantly higher prevalence of unseen co-changes (up to 23x more frequent in specific projects and 7x on average) compared to recurrent co-changes. Interestingly, comparisons of co-change complexity based on file distance in the directory structure revealed no decisive differences between the two types. Our graph-based approach achieved good accuracy in predicting unseen co-changes (average AUROC of 0.84, with some projects reaching up to 0.98). Our method achieved significantly better performance than the baseline approach, demonstrating an average recall of 90% and a precision of 38%. While the precision value might seem modest, our approach achieves very high precision@k values (near 100% for 13 out of 14 projects up to $k=100$), underlining its effectiveness in real-world applications.},
  keywords={Accuracy;Shape;Shape measurement;Source coding;Predictive models;Software systems;Heterogeneous networks;Link Prediction;Unseen Co-changes;Co-change Prediction;Heterogeneous Network},
  doi={10.1109/COMPSAC61105.2024.00155},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{6845518,
  author={Suvarna Vani, K. and Om Swaroopa, M. and Sravani, T.D. and Praveen Kumar, K.},
  booktitle={2014 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology},
  title={Frequent substructures and fold classification from protein contact maps},
  year={2014},
  volume={},
  number={},
  pages={1-8},
  abstract={The three dimensional structure of proteins is useful to carry out the biophysical and biochemical functions in a cell. Approaches to protein structure/fold classification typically extract amino acid sequence features, and machine-learning approaches are applied to classification problem. Protein contact maps are two-dimensional representations of contacts among the amino acid residues in the folded protein structure. Many researchers make note of the way secondary structures are clearly visible in the contact maps where alpha-helices are seen as thick bands and the beta-sheets as orthogonal to the diagonal. Some patterns in off-diagonal contact maps correspond to configurations of protein secondary structures. This paper explores the idea of extracting rules from contact maps to represent fold information. Contact maps for proteins of any length are generated. An efficient way to extract Secondary Structure Elements from contact maps is adopted. This method achieves appreciable performance, when compared to the original Secondary Structure Elements. Frequent substructures are extracted using a graph based pattern learning system, SUBDUE, to six folds in All-Alpha structural class. Extracted substructures are mapped to three-dimensional structure that proves the performance of the work. To extract additional features from off-diagonal contact map, Triangle Sub Division Method is implemented and feature set is enhanced to 20 regions of interest. An accuracy of 70% is achieved by the J48 decision tree classifier. The decision tree classifier results, gain understanding of rules generated for each structural class. The differences in regions of interest are distinguished for All-Alpha structural class. This method needs to be validated on other SCOP classes.},
  keywords={Proteins;Accuracy;Feature extraction;Decision trees;Amino acids;Data mining;Vectors;Mining protein contact maps;Protein fold prediction;Association rule mining;Frequent patterns},
  doi={10.1109/CIBCB.2014.6845518},
  ISSN={},
  month={May},}@INPROCEEDINGS{6921206,
  author={Ranaei, Samira and Karvonen, Matti and Suominen, Arho and Kässi, Tuomo},
  booktitle={Proceedings of PICMET '14 Conference: Portland International Center for Management of Engineering and Technology; Infrastructure and Service Integration},
  title={Forecasting emerging technologies of low emission vehicle},
  year={2014},
  volume={},
  number={},
  pages={2924-2937},
  abstract={The aim of this paper is to propose a patent search strategy in the case of emerging technology fields and to study the development patterns of the Low Emission Vehicle (LEV) technologies. An Automatic Patent Classification (APC) system has been developed based on text mining techniques to facilitate the patent retrieval process. The data was collected from Global Patent Index (GPI) database and interviews were conducted to involve expert's opinion. Technology forecasting method utilized the collected patent data to define the technological life cycles of LEV technology. The growth curves estimates steady growth in LEV technologies including hybrid and battery electronic vehicles, and apparently reaching to saturation point in few decades is inevitable. Plus, patenting activity of hydrogen fuel cell vehicle technology was experiencing the infancy period so far, and further it is anticipated to reach higher growth rate in line with other energy alternatives. The proposed method can help patent researchers in terms of retrieving accurate patents based on their technology target. Moreover, the technology forecasting techniques provide an insight to investors assisting them to allocate their resources properly. The results can benefit car industry stakeholders to anticipate the most promising technology areas in an uncertain dynamic market.},
  keywords={Patents;Vehicles;Market research;Industries;Technology forecasting;Search problems;Hydrogen},
  doi={},
  ISSN={2159-5100},
  month={July},}@INPROCEEDINGS{323329,
  author={Chen and She},
  booktitle={1994 Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences},
  title={Inductive query by examples (IQBE): a machine learning approach},
  year={1994},
  volume={3},
  number={},
  pages={428-437},
  abstract={This paper presents an incremental, inductive learning approach to query-by-examples for information retrieval (IR) and database management systems (DBMS). After briefly reviewing conventional information retrieval techniques and the prevailing database query paradigms, we introduce the ID5R algorithm, previously developed by Utgoff (1989), for "intelligent" and system-supported query processing.<>},
  keywords={Learning systems;Database systems, query processing;Information retrieval},
  doi={10.1109/HICSS.1994.323329},
  ISSN={},
  month={Jan},}@ARTICLE{9994688,
  author={Yadav, Divakar and Katna, Rishabh and Yadav, Arun Kumar and Morato, Jorge},
  journal={IEEE Access},
  title={Feature Based Automatic Text Summarization Methods: A Comprehensive State-of-the-Art Survey},
  year={2022},
  volume={10},
  number={},
  pages={133981-134003},
  abstract={With the advent of the World Wide Web, there are numerous online platforms that generate huge amounts of textual material, including social networks, online blogs, magazines, etc. This textual content contains useful information that can be used to advance humanity. Text summarization has been a significant area of research in natural language processing (NLP). With the expansion of the internet, the amount of data in the world has exploded. Large volumes of data make locating the required and best information time-consuming. It is impractical to manually summarize petabytes of data; hence, computerized text summarization is rising in popularity. This study presents a comprehensive overview of the current status of text summarizing approaches, techniques, standard datasets, assessment criteria, and future research directions. The summarizing approaches are assessed based on several characteristics, including approach-based, document-number-based, Summarization domain-based, document-language-based, output summary nature, etc. This study concludes with a discussion of many obstacles and research opportunities linked to text summarizing research that may be relevant for future researchers in this field.},
  keywords={Measurement;Data mining;Web sites;Market research;Deep learning;Neural networks;Computer science;Abstractive summarization;cosine-similarity;deep learning;extractive summarization;graph-based algorithm;neural networks},
  doi={10.1109/ACCESS.2022.3231016},
  ISSN={2169-3536},
  month={},}@INBOOK{9968181,
  author={Tuffery, Stephane},
  booktitle={Deep Learning: From Big Data to Artificial Intelligence with R},
  title={Deep Learning for Natural Language Processing},
  year={2023},
  volume={},
  number={},
  pages={431-478},
  abstract={This chapter describes the application of deep learning methods to natural language processing, with examples of text classification and text generation. It focuses on recurrent neural networks but also on transformer models, in particular, the BERT model, and compares the classification of texts by a recurrent network LSTM and by a transformer model DistilBERT. The chapter shows some examples of the implementation of deep neural networks, and, in particular, recurrent networks. If the source text is very long or if its vocabulary is simple, the ratio &#x201c;total number of words / number of different words&#x201d; will be high and it will be possible to generate the new text word&#x2010;by&#x2010;word. The words of a sentence can be represented in a way that allows convolutional neural networks to be naturally applied to them.},
  keywords={Solid modeling;Deep learning;Task analysis;Vocabulary;Transformers;Training;Tensors},
  doi={10.1002/9781119845041.ch9},
  ISSN={},
  publisher={Wiley},
  isbn={9781119845027},
  url={https://ieeexplore.ieee.org/document/9968181},}@INPROCEEDINGS{8620815,
  author={Gürcan, Fatih},
  booktitle={2018 International Conference on Artificial Intelligence and Data Processing (IDAP)},
  title={Major Research Topics in Big Data: A Literature Analysis from 2013 to 2017 Using Probabilistic Topic Models},
  year={2018},
  volume={},
  number={},
  pages={1-4},
  abstract={Big data is a popular phenomenon among practitioners as well as scholars. Due to its multidisciplinary background, big data research literature includes a wide spectrum of scientific publications in various research areas. With the aim of identification of research trends in big data literature, an empirical analysis based on probabilistic topic models was performed on peer reviewed articles between 2013 and 2017. As a result of the analysis, the 24 topics demonstrating research trends was discovered. This study maps the research landscape of the big data literature over the past five years. The findings of this study may provide valuable insights to big data research communities.},
  keywords={Big Data;Analytical models;Data models;Market research;Distributed databases;Semantics;Data mining;Big data;research topics;literature analysis;topic models;text mining},
  doi={10.1109/IDAP.2018.8620815},
  ISSN={},
  month={Sep.},}@ARTICLE{10716649,
  author={Müller, Nils and Reermann, Jens and Meisen, Tobias},
  journal={IEEE Access},
  title={Navigating the Depths: A Comprehensive Survey of Deep Learning for Passive Underwater Acoustic Target Recognition},
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={The field of deep learning is a rapidly developing research area with numerous applications across multiple domains. Sonar (SOund Navigation And Ranging) processing has traditionally been a field of statistical analysis. However, in the past ten to fifteen years, the rapid growth of deep learning has challenged classical approaches with modern deep learning-based methods. This survey provides a systematic overview of the Underwater Acoustic Target Recognition (UATR) domain within the area of deep learning. The objective is to highlight popular design choices and evaluate the commonalities and differences of the investigated techniques in relation to the selected architectures and pre-processing methods. Furthermore, this survey examines the state of UATR literature through the identification of prominent conferences and journals which points new researchers in directions where to allocate UATR related publications. Additionally, popular datasets and available benchmarks are identified and analysed for complexity coverage. This work targets researchers new to the field as well as experienced researchers that want to get a broader overview. Nonetheless, experienced sonar engineers with a strong background within classical analysis also benefit from this survey.},
  keywords={Deep learning;Surveys;Underwater acoustics;Target recognition;Sonar;Databases;Sensors;Reproducibility of results;Sonar navigation;Reviews;Deep Learning;Passive Sonar Classification;Underwater Acoustic Target Recognition},
  doi={10.1109/ACCESS.2024.3480788},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{4035746,
  author={Bilgic, Mustafa and Licamele, Louis and Getoor, Lise and Shneiderman, Ben},
  booktitle={2006 IEEE Symposium On Visual Analytics Science And Technology},
  title={D-Dupe: An Interactive Tool for Entity Resolution in Social Networks},
  year={2006},
  volume={},
  number={},
  pages={43-50},
  abstract={Visualizing and analyzing social networks is a challenging problem that has been receiving growing attention. An important first step, before analysis can begin, is ensuring that the data is accurate. A common data quality problem is that the data may inadvertently contain several distinct references to the same underlying entity; the process of reconciling these references is called entity-resolution. D-Dupe is an interactive tool that combines data mining algorithms for entity resolution with a task-specific network visualization. Users cope with complexity of cleaning large networks by focusing on a small subnetwork containing a potential duplicate pair. The subnetwork highlights relationships in the social network, making the common relationships easy to visually identify. D-Dupe users resolve ambiguities either by merging nodes or by marking them distinct. The entity resolution process is iterative: as pairs of nodes are resolved, additional duplicates may be revealed; therefore, resolution decisions are often chained together. We give examples of how users can flexibly apply sequences of actions to produce a high quality entity resolution result. We illustrate and evaluate the benefits of D-Dupe on three bibliographic collections. Two of the datasets had already been cleaned, and therefore should not have contained duplicates; despite this fact, many duplicates were rapidly identified using D-Dupe's unique combination of entity resolution algorithms within a task-specific visual interface},
  keywords={Collaboration;Visualization;Task analysis;Layout;Tools;Substrates;Social networking (online);Data cleaning and integration;user interfaces;visual analytics;visual data mining;H.2.8 [Information Systems]: Database Applications¿Data mining;H.5.2 [Information Interfaces and Presentation]: User Interfaces¿User-centered design},
  doi={10.1109/VAST.2006.261429},
  ISSN={},
  month={Oct},}@ARTICLE{10499978,
  author={Al Qassem, Lamees M. and Stouraitis, Thanos and Damiani, Ernesto and Elfadel, Ibrahim M.},
  journal={IEEE Transactions on Network and Service Management},
  title={Containerized Microservices: A Survey of Resource Management Frameworks},
  year={2024},
  volume={21},
  number={4},
  pages={3775-3796},
  abstract={The growing adoption of microservice architectures (MSAs) has led to major research and development efforts to address their challenges and improve their performance, reliability, and robustness. Important aspects of MSA that are not sufficiently covered in the open literature include efficient cloud resource allocation and optimal power management. Other aspects of MSA remain widely scattered in the literature, including cost analysis, service level agreements (SLAs), and demand-driven scaling. In this article, we examine recent cloud frameworks for containerized microservices with a focus on efficient resource utilization using auto-scaling. We classify these frameworks on the basis of their resource allocation models and underlying hardware resources. We highlight current MSA trends and identify workload-driven resource sharing within microservice meshes and SLA streamlining as two key areas for future microservice research.},
  keywords={Microservice architectures;Resource management;Cloud computing;Containers;Service level agreements;Computer architecture;Surveys;Microservices;containers;resource management;container orchestration;machine learning;workload forecasting;reactive allocation;predictive allocation},
  doi={10.1109/TNSM.2024.3388633},
  ISSN={1932-4537},
  month={Aug},}@ARTICLE{484439,
  author={Chung-Hsin Lin and Hsinchun Chen},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  title={An automatic indexing and neural network approach to concept retrieval and classification of multilingual (Chinese-English) documents},
  year={1996},
  volume={26},
  number={1},
  pages={75-88},
  abstract={An automatic indexing and concept classification approach to a multilingual (Chinese and English) bibliographic database is presented. We introduced a multi-linear term-phrasing technique to extract concept descriptors (terms or keywords) from a Chinese-English bibliographic database. A concept space of related descriptors was then generated using a co-occurrence analysis technique. Like a man-made thesaurus, the system-generated concept space can be used to generate additional semantically-relevant terms for search. For concept classification and clustering, a variant of a Hopfield neural network was developed to cluster similar concept descriptors and to generate a small number of concept groups to represent (summarize) the subject matter of the database. The concept space approach to information classification and retrieval has been adopted by the authors in other scientific databases and business applications, but multilingual information retrieval presents a unique challenge. This research reports our experiment on multilingual databases. Our system was initially developed in the MS-DOS environment, running ETEN Chinese operating system. For performance reasons, it was then tested on a UNIX-based system. Due to the unique ideographic nature of the Chinese language, a Chinese term-phrase indexing paradigm considering the ideographic characteristics of Chinese was developed as a multilingual information classification model. By applying the neural network based concept classification technique, the model presents a novel way of organizing unstructured multilingual information.},
  keywords={Machine assisted indexing;Neural networks;Databases;Information retrieval;Thesauri;Hopfield neural networks;Operating systems;System testing;Natural languages;Organizing},
  doi={10.1109/3477.484439},
  ISSN={1941-0492},
  month={Feb},}@INPROCEEDINGS{10170541,
  author={Bangdiwala, Malhar and Mahadik, Sakshi and Mehta, Yashvi and Salunke, Abhijeet and Das, Rita},
  booktitle={2023 4th International Conference for Emerging Technology (INCET)},
  title={Automated Library Management System using Face Recognition and OCR},
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Traditional methods of managing a library have their disadvantages like long waiting queues and expensive scanner machines. This paper aims to overcome these drawbacks by introducing a new application that makes use of face and text recognition to issue books from the library. The main idea is that a picture of the book with the issuer will be taken from which the required data will be extracted. The account of the user will be created by the library admin by uploading a picture of the face of the user. The application consists of two types of logins - user and admin, where administrators can keep track of user transactions and history. The face recognition algorithm and book recognition algorithm gave an overall accuracy of 99.43% and 82.5% respectively. The user can also see the information related to books and this information is extracted from a third-party API. Further, easy payment and reissue options for the books on the application have also been implemented.},
  keywords={Automation;Text recognition;Face recognition;Optical character recognition;Libraries;Data mining;Character recognition;Face Recognition;Optical Character Recognition;Library Automation},
  doi={10.1109/INCET57972.2023.10170541},
  ISSN={},
  month={May},}@INBOOK{8040770,
  author={},
  booktitle={Information Retrieval: Searching in the 21st Century},
  title={Front Matter},
  year={2009},
  volume={},
  number={},
  pages={i-xxiii},
  abstract={The prelims comprise: <list style="bulleted" xml:id="l1"> <listItem> Half&#x2010;Title Page </listItem> <listItem> Title Page </listItem> <listItem> Copyright Page </listItem> <listItem> Dedication Page </listItem> <listItem> Table of Contents </listItem> <listItem> Foreword </listItem> <listItem> Preface </listItem> <listItem> About the Editors </listItem> <listItem> List of Contributors </listItem> <listItem> Introduction </listItem> </list>},
  keywords={},
  doi={10.1002/9780470033647.fmatter},
  ISSN={},
  publisher={Wiley},
  isbn={9780470033630},
  url={https://ieeexplore.ieee.org/document/8040770},}@INPROCEEDINGS{4272026,
  author={Kang, Hyunmo and Sehgal, Vivek and Getoor, Lise},
  booktitle={2007 11th International Conference Information Visualization (IV '07)},
  title={GeoDDupe: A Novel Interface for Interactive Entity Resolution in Geospatial Data},
  year={2007},
  volume={},
  number={},
  pages={489-496},
  abstract={Due to the growing interest in geospatial data mining and analysis, data cleaning and integration in geospatial data is becoming an important issue. Geospatial entity resolution is the process of reconciling multiple location references to the same real world location within a single data source (deduplication) or across multiple data sources (integration). In this paper, we introduce an interactive tool called GeoDDupe which effectively combines automatic data mining algorithms for geospatial entity resolution with a novel network visualization supporting users' resolution analysis and decisions. We illustrate the GeoDDupe interface with an example geospatial dataset and show how users can efficiently and accurately resolve location entities. Finally, the case study with two real-world geospatial datasets demonstrates the potential of GeoDDupe.},
  keywords={Spatial resolution;Data mining;Data visualization;Databases;Data analysis;Merging;Computer science;Computer interfaces;Educational institutions;Cleaning},
  doi={10.1109/IV.2007.55},
  ISSN={1550-6037},
  month={July},}@INPROCEEDINGS{10307301,
  author={Palanisamy, Deva and Narayanan, Jayasree},
  booktitle={2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
  title={Network Analysis of Research Base Papers: Metrics and Potential use},
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={This paper describes a common challenge faced by researchers who struggle to find good references for their current domain of study due to the exponential increase in research papers being published daily. The proposed solution involves using deep learning techniques with the BERT Model to obtain a summary of the published paper and its citation papers. Science mapping approaches are then applied to ascertain the similarity between the reference papers and the original paper and rank the citations based on their similarity. Various bibliometric analyses such as citation analysis, co-citation analysis, bibliometric coupling, and network metrics are used to further rank the citations and identify unique citations with citation count. Additionally, author suggestions are provided. The end goal is to develop a visualization tool in the form of a dashboard to help researchers focus on relevant papers.},
  keywords={Measurement;Deep learning;Couplings;Time-frequency analysis;Visualization;Citation analysis;Network analyzers;Bibliometrics;Bert;Citation Analysis;Science mapping;Cosine similarity;Term frequency;Inverse Term frequency;Citation count},
  doi={10.1109/ICCCNT56998.2023.10307301},
  ISSN={2473-7674},
  month={July},}@INPROCEEDINGS{9700356,
  author={Meyer, Maria Laura Brzezinski},
  booktitle={2021 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  title={TSAI - Test Selection using Artificial Intelligence for the Support of Continuous Integration},
  year={2021},
  volume={},
  number={},
  pages={306-309},
  abstract={The agile methodology has been increasingly deployed in the industry world, breaking the process into cycles of planning, executing, and evaluating. In the software development domain, an agile method named continuous integration is widely used to automatically integrate code changes from different developers into the same software. Then, each new build can be tested to make sure that the modifications did not interfere with the rest of the already verified code. Despite being very important, regression tests are usually the costliest part of a project. It is laborious to retest all tests of each new software version due to the time it takes to perform and often, before all tests are finished, a new software version is ready to be tested. To improve regression tests results, a selection can be done. By selecting the right tests at the right moment, the use of all test catalogs can be avoided to find faults in the software tested. The aim of this work is to develop a method to select tests to be executed for each version using artificial intelligence algorithms. Learning algorithms can find patterns and similarities between test cases to help knowing which one has a higher probability to expose a fault.},
  keywords={Industries;Software testing;Codes;Conferences;Software algorithms;Software;Software reliability;Continuous Integration;testing;regression tests;artificial intelligence;industrial case study},
  doi={10.1109/ISSREW53611.2021.00092},
  ISSN={},
  month={Oct},}@ARTICLE{10693348,
  author={Yan, Caihong and Lu, Xiaofeng and Lio, Pietro and Hui, Pan and He, Daojing},
  journal={IEEE Internet of Things Journal},
  title={Self-simulation and Meta-Model Aggregation Based Heterogeneous Graph Coupled Federated Learning},
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={A heterogeneous information network (heterogeneous graph) federated learning plays a crucial role in enabling multi-party collaboration in the IoT system. However, due to differences in business and data, the local models of each participant are heterogeneous and unable to achieve federated aggregation. Furthermore, the non-independent and identically distributed (non-IID) coupling topology structure among participants severely impacts the performance of federated learning. Given the lack of appropriate solutions to these issues, this study proposes a novel heterogeneous graph federated learning framework (HGFL+) based on self-simulation and meta-model aggregation, which includes the following two innovative techniques: (1) The missing coupling supplement module simulates new neighbor nodes on its original heterogeneous graph, and constructs associated edges using multiple encoder-decoder structures, thereby achieving the supplement of missing neighbors with better results than external generative methods. (2) The heterogeneous model aggregation algorithm realizes the fusion of multi-party heterogeneous graph information through mapping, splitting, aggregating, and recombining multiple stages based on the meta-model (the largest basic model unit among participants). We theoretically analyzed the applicability and effectiveness of HGFL+, demonstrating the generalization boundary of HGFL+. Meanwhile, multi-dimensional empirical verification of classification performance, convergence effect, time overhead, model size, and application extension (model, task, domain) validates the effectiveness of the proposed method.},
  keywords={Federated learning;Couplings;Data models;Graph neural networks;Analytical models;Semantics;Internet of Things;Federated Learning;Coupled Heterogeneous Graphs;Missing Graph Completion;Heterogeneous Model Aggregation},
  doi={10.1109/JIOT.2024.3462724},
  ISSN={2327-4662},
  month={},}@ARTICLE{9932606,
  author={Vergallo, Roberto and Mainetti, Luca},
  journal={IEEE Access},
  title={The Role of Technology in Improving the Customer Experience in the Banking Sector: A Systematic Mapping Study},
  year={2022},
  volume={10},
  number={},
  pages={118024-118042},
  abstract={Information Technology (IT) has revolutionized the way we manage our money. The adoption of innovative technologies in banking scenarios allows to access old and new financial services but in a faster and more secure, comfortable, rewarding and engaging way. The number, the performances and the seamless integration of these innovations is a driver for banks to retain their customers and avoid costly change of hearts. The literature is rich in works reporting on the use of technology with direct or indirect impact on the experience of banking customers. Some mapping studies about the adoption of technologies in the field exist, but they are specific to particular technologies (e.g., only Artificial Intelligence), or vice versa too generic (e.g., reviewing the adoption of technologies to support any kind of banking process). So a specific research effort on the crossed domain of technology and Customer Experience (CX) is missing. This paper aims to overcome the following gaps: the lack of a comprehensive map of the research made in the field in the past decade; a discussion on the current research trends of top publications and journals is missing; the next research challenges are yet to be identified. To face these limitations, we designed and submitted 7 different queries to pull papers out of 4 popular scientific databases. From an initial set of 6,756 results, we identified a set of 89 primary studies that we thoroughly analyzed. A selection of the top 20% works allowed us to seek the most performant technologies as well as other promising ones that have not been experimented yet in the field. Main results prove that the combined study of technology and CX in the banking sector is not approached systematically and thus the development of a new specific research line is needed.},
  keywords={Banking;Systematics;Blockchains;Market research;Machine learning;Product design;Bibliometrics;Financial industry;Information and communication technology;Artificial intelligence;Disruptive technologies;Banking;financial services;user experience;information and communication technology;disruptive innovation;internet of things;artificial intelligence;blockchains;product customization;computer security},
  doi={10.1109/ACCESS.2022.3218010},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{6031483,
  author={Szymanski, Julian and Szymański, Julian},
  booktitle={2011 International Conference on Communications, Computing and Control Applications (CCCA)},
  title={Creating categories for Wikipedia articles using Self-Organizing Maps},
  year={2011},
  volume={},
  number={},
  pages={1-5},
  abstract={The article presents the results of the experiments performed on selected sub-set of Wikipedia which we categorized automaticly. We analyze two methods of text representation: based on references and word content. Using them we introduced joint representation that has been used to build groups of similar articles based on Kohonen Self-Organizing Maps. To fulfill efficiency of the data processing, we performed dimensionality reduction of raw data using Principal Component Analysis performed on similarity matrix. Changing the granularity of SOM network allows to build hierarchical categories and find significant relations between articles in documents repository.},
  keywords={Ethics;Internet;Computers;Color;Information services;Electronic publishing;text representation;documents clustering;Self Organizing Maps;Principal Component Analysis;text processing text representation;documents clustering;Self Organizing Maps;Principal Component Analysis;text processing},
  doi={10.1109/CCCA.2011.6031483},
  ISSN={},
  month={March},}@ARTICLE{9745269,
  author={Latino, Maria Elena and Menegoli, Marta and Corallo, Angelo},
  journal={IEEE Transactions on Engineering Management},
  title={Agriculture Digitalization: A Global Examination Based on Bibliometric Analysis},
  year={2024},
  volume={71},
  number={},
  pages={1330-1345},
  abstract={Several research studies discuss the process of agriculture digitalization proposing technologies able to face the current agri-food challenges. However, the extant bibliometric studies do not fully exploit the complementarity of different modern bibliometric tools, such as performance analysis and science mapping and do not consider the entire research field timespan. Therefore, the aim of this study is to complement and update the previous works and provide a broad quantitative and qualitative view of Agriculture 4.0 research by using synergistically performance analysis and science mapping. The analysis was realized on a sample of 2334 papers adopting statistical frequency analysis and VOSviewer. Performance analysis provided key findings about the following indicators: research subject areas, publications trend, most productive journals, document types, authors productivity, authors' and index keywords, most cited papers, most productive and influent institutions, country map collaboration, the documents by funding sponsor. Science mapping provided key findings about the main thematic research field’ clusters and their evolution overtime: technology application in agricultural industry, data model for analysis and prediction, experimentation and applicative studies of smart agriculture, decision support systems for crop monitoring. This study could benefit: food companies, supporting in technologies identification; governments, suggesting policies to stimulate agricultural digitalization process; academics, incentivizing more consciousness about the topic and research agenda; those who approach this topic for the first time, facilitating bibliographical referencing.},
  keywords={Bibliometrics;Performance analysis;Monitoring;Digital agriculture;Technological innovation;Indexes;Productivity;Agriculture 4.0;bibliometric analysis;precision agriculture;smart agriculture;smart farming;systematic literature review},
  doi={10.1109/TEM.2022.3154841},
  ISSN={1558-0040},
  month={},}@BOOK{8186825,
  author={Hazan, Elad},
  title={Introduction to Online Convex Optimization},
  year={2016},
  volume={},
  number={},
  pages={},
  abstract={Introduction to Online Convex Optimization portrays optimization as a process. In many practical applications the environment is so complex that it is infeasible to lay out a comprehensive theoretical model and use classical algorithmic theory and mathematical optimization. It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed. This view of optimization as a process has become prominent in varied fields and has led to some spectacular success in modeling and systems that are now part of our daily lives. Introduction to Online Convex Optimization is intended to serve as a reference for a self-contained course on online convex optimization and the convex optimization approach to machine learning for the educated graduate student in computer science/electrical engineering/ operations research/statistics and related fields. It is also an ideal reference for the researcher diving into this fascinating world at the intersection of optimization and machine learning.},
  keywords={Online learning;Optimization},
  doi={10.1561/2400000013},
  ISSN={},
  publisher={now},
  isbn={9781680831719},
  url={https://ieeexplore.ieee.org/document/8186825},}@INPROCEEDINGS{4470253,
  author={Tang, Jie and Zhang, Duo and Yao, Limin},
  booktitle={Seventh IEEE International Conference on Data Mining (ICDM 2007)},
  title={Social Network Extraction of Academic Researchers},
  year={2007},
  volume={},
  number={},
  pages={292-301},
  abstract={This paper addresses the issue of extraction of an academic researcher social network. By researcher social network extraction, we are aimed at finding, extracting, and fusing the 'semantic '-based profiling information of a researcher from the Web. Previously, social network extraction was often undertaken separately in an ad-hoc fashion. This paper first gives a formalization of the entire problem. Specifically, it identifies the 'relevant documents' from the Web by a classifier. It then proposes a unified approach to perform the researcher profiling using conditional random fields (CRF). It integrates publications from the existing bibliography datasets. In the integration, it proposes a constraints-based probabilistic model to name disambiguation. Experimental results on an online system show that the unified approach to researcher profiling significantly outperforms the baseline methods of using rule learning or classification. Experimental results also indicate that our method to name disambiguation performs better than the baseline method using unsupervised learning. The methods have been applied to expert finding. Experiments show that the accuracy of expert finding can be significantly improved by using the proposed methods.},
  keywords={Social network services;Data mining;USA Councils;Computer science;Computer vision;Image databases;Visual databases;Indexing;Biometrics;Application software},
  doi={10.1109/ICDM.2007.30},
  ISSN={2374-8486},
  month={Oct},}@ARTICLE{8314667,
  author={Liu, Jiaying and Tang, Tao and Wang, Wei and Xu, Bo and Kong, Xiangjie and Xia, Feng},
  journal={IEEE Access},
  title={A Survey of Scholarly Data Visualization},
  year={2018},
  volume={6},
  number={},
  pages={19205-19221},
  abstract={Scholarly information usually contains millions of raw data, such as authors, papers, citations, as well as scholarly networks. With the rapid growth of the digital publishing and harvesting, how to visually present the data efficiently becomes challenging. Nowadays, various visualization techniques can be easily applied on scholarly data visualization and visual analysis, which enables scientists to have a better way to represent the structure of scholarly data sets and reveal hidden patterns in the data. In this paper, we first introduce the basic concepts and the collection of scholarly data. Then, we provide a comprehensive overview of related data visualization tools, existing techniques, as well as systems for the analyzing volumes of diverse scholarly data. Finally, open issues are discussed to pursue new solutions for abundant and complicated scholarly data visualization, as well as techniques, that support a multitude of facets.},
  keywords={Data visualization;Data mining;Tools;Data analysis;Libraries;Visual analytics;Big Data;Scholarly data;scholarly data analysis;scholarly data visualization;visual analysis},
  doi={10.1109/ACCESS.2018.2815030},
  ISSN={2169-3536},
  month={},}@ARTICLE{5645623,
  author={de Carvalho, Moises G. and Laender, Alberto H. F. and Goncalves, Marcos Andre and da Silva, Altigran S.},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={A Genetic Programming Approach to Record Deduplication},
  year={2012},
  volume={24},
  number={3},
  pages={399-412},
  abstract={Several systems that rely on consistent data to offer high-quality services, such as digital libraries and e-commerce brokers, may be affected by the existence of duplicates, quasi replicas, or near-duplicate entries in their repositories. Because of that, there have been significant investments from private and government organizations for developing methods for removing replicas from its data repositories. This is due to the fact that clean and replica-free repositories not only allow the retrieval of higher quality information but also lead to more concise data and to potential savings in computational time and resources to process this data. In this paper, we propose a genetic programming approach to record deduplication that combines several different pieces of evidence extracted from the data content to find a deduplication function that is able to identify whether two entries in a repository are replicas or not. As shown by our experiments, our approach outperforms an existing state-of-the-art method found in the literature. Moreover, the suggested functions are computationally less demanding since they use fewer evidence. In addition, our genetic programming approach is capable of automatically adapting these functions to a given fixed replica identification boundary, freeing the user from the burden of having to choose and tune this parameter.},
  keywords={Training;Machine learning;Genetic programming;Probabilistic logic;Databases;Data mining;Database administration;evolutionary computing and genetic algorithms;database integration.},
  doi={10.1109/TKDE.2010.234},
  ISSN={1558-2191},
  month={March},}@INPROCEEDINGS{10353295,
  author={Menon, Remya R.K. and Rahul, R and Bhadrakrishnan, V},
  booktitle={2023 4th IEEE Global Conference for Advancement in Technology (GCAT)},
  title={Graph Auto Encoders for Content-based Document Recommendation System},
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={In the modern world,recommendation system have a good potential to improve business outcomes and to improve user experience with better accuracy combined with more generalisation to new unseen query.We propose a content-based document recommendation system which retrieves the most similar documents to the query given by the user.The embedded documents are fed into Graph Auto Encoder (GAE) and query through sparse coder to do latent representation which are finally used to give the user with perfect recommendations.The proposed method got evaluated with an average accuracy value of 93.91%.},
  keywords={User experience;Encoding;Recommender systems;Business;GAE;content based;document recommendation system;sparse coding},
  doi={10.1109/GCAT59970.2023.10353295},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{7193065,
  author={Sruthika S and Tajunisha, N.},
  booktitle={2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)},
  title={A study on evolution of data analytics to big data analytics and its research scope},
  year={2015},
  volume={},
  number={},
  pages={1-6},
  abstract={The volume of data that enterprise acquires every day is increasing rapidly. The enterprises do not know what to do with the data and how to extract information from this data. Analytics is the process of collecting, organizing and analysing large set of data that is important for the business. The process of analysing and processing this huge amount of data is called bigdata analytics. The volume, variety and velocity of big data cause performance problems when processed using traditional data processing techniques. It is now possible to store and process these vast amounts of data on low cost platforms such as Hadoop. The major aspire of this paper is to make a study on data analytics, big data and its applications.},
  keywords={Computational modeling;Engines;Analytics;Bigdata;Hadoop},
  doi={10.1109/ICIIECS.2015.7193065},
  ISSN={},
  month={March},}@ARTICLE{10214236,
  author={Anirudh, Rushil and Archibald, Rick and Asif, M. Salman and Becker, Markus M. and Benkadda, Sadruddin and Bremer, Peer-Timo and Budé, Rick H. S. and Chang, C. S. and Chen, Lei and Churchill, R. M. and Citrin, Jonathan and Gaffney, Jim A. and Gainaru, Ana and Gekelman, Walter and Gibbs, Tom and Hamaguchi, Satoshi and Hill, Christian and Humbird, Kelli and Jalas, Sören and Kawaguchi, Satoru and Kim, Gon-Ho and Kirchen, Manuel and Klasky, Scott and Kline, John L. and Krushelnick, Karl and Kustowski, Bogdan and Lapenta, Giovanni and Li, Wenting and Ma, Tammy and Mason, Nigel J. and Mesbah, Ali and Michoski, Craig and Munson, Todd and Murakami, Izumi and Najm, Habib N. and Olofsson, K. Erik J. and Park, Seolhye and Peterson, J. Luc and Probst, Michael and Pugmire, David and Sammuli, Brian and Sawlani, Kapil and Scheinker, Alexander and Schissel, David P. and Shalloo, Rob J. and Shinagawa, Jun and Seong, Jaegu and Spears, Brian K. and Tennyson, Jonathan and Thiagarajan, Jayaraman and Ticoş, Catalin M. and Trieschmann, Jan and Dijk, Jan van and Essen, Brian Van and Ventzek, Peter and Wang, Haimin and Wang, Jason T. L. and Wang, Zhehui and Wende, Kristian and Xu, Xueqiao and Yamada, Hiroshi and Yokoyama, Tatsuya and Zhang, Xinhua},
  journal={IEEE Transactions on Plasma Science},
  title={2022 Review of Data-Driven Plasma Science},
  year={2023},
  volume={51},
  number={7},
  pages={1750-1838},
  abstract={Data-driven science and technology offer transformative tools and methods to science. This review article highlights the latest development and progress in the interdisciplinary field of data-driven plasma science (DDPS), i.e., plasma science whose progress is driven strongly by data and data analyses. Plasma is considered to be the most ubiquitous form of observable matter in the universe. Data associated with plasmas can, therefore, cover extremely large spatial and temporal scales, and often provide essential information for other scientific disciplines. Thanks to the latest technological developments, plasma experiments, observations, and computation now produce a large amount of data that can no longer be analyzed or interpreted manually. This trend now necessitates a highly sophisticated use of high-performance computers for data analyses, making artificial intelligence and machine learning vital components of DDPS. This article contains seven primary sections, in addition to the introduction and summary. Following an overview of fundamental data-driven science, five other sections cover widely studied topics of plasma science and technologies, i.e., basic plasma physics and laboratory experiments, magnetic confinement fusion, inertial confinement fusion and high-energy-density physics, space and astronomical plasmas, and plasma technologies for industrial and other applications. The final Section before the summary discusses plasma-related databases that could significantly contribute to DDPS. Each primary Section starts with a brief introduction to the topic, discusses the state-of-the-art developments in the use of data and/or data-scientific approaches, and presents the summary and outlook. Despite the recent impressive signs of progress, the DDPS is still in its infancy. This article attempts to offer a broad perspective on the development of this field and identify where further innovations are required.},
  keywords={Plasmas;Physics;Scientific computing;Research and development;Data science;Contracts;Technological innovation;Artificial intelligence;data-driven plasma science;machine learning;nuclear fusion;plasma control;plasma diagnostics;plasma processing;plasma simulation},
  doi={10.1109/TPS.2023.3268170},
  ISSN={1939-9375},
  month={July},}@INPROCEEDINGS{848380,
  author={Popescul, A. and Flake, G.W. and Lawrence, S. and Ungar, L.H. and Giles, C.L.},
  booktitle={Proceedings IEEE Advances in Digital Libraries 2000},
  title={Clustering and identifying temporal trends in document databases},
  year={2000},
  volume={},
  number={},
  pages={173-182},
  abstract={We introduce a simple and efficient method for clustering and identifying temporal trends in hyper-linked document databases. Our method can scale to large datasets because it exploits the underlying regularity often found in hyper-linked document databases. Because of this scalability, we can use our method to study the temporal trends of individual clusters in a statistically meaningful manner. As an example of our approach, we give a summary of the temporal trends found in a scientific literature database with thousands of documents.},
  keywords={Databases;Clustering algorithms;National electric code;Scalability;Web sites;Publishing;Delay;Merging;Citation analysis},
  doi={10.1109/ADL.2000.848380},
  ISSN={},
  month={May},}@INPROCEEDINGS{375347,
  author={Ripoche, H. and Sallantin, J.},
  booktitle={Proceedings of the Twenty-Eighth Annual Hawaii International Conference on System Sciences},
  title={Knowledge discovery in a genetic database: the MINOS system},
  year={1995},
  volume={5},
  number={},
  pages={91-98 vol.5},
  abstract={This paper concerns the management of genetic sequences in an object-oriented database and the extraction of knowledge from these sequences. In our case knowledge discovery consists in finding functions capable of predicting properties about genetic sequences. This problem is also known as functional inference. The paper is divided in two parts: the first one shows the interest of using an object-oriented query language to build and use prediction functions. In the second part, we propose to use prediction functions as descriptors of sequences in order to index them. The indexation is performed with concept lattices.<>},
  keywords={Genetics;Object oriented databases;Lattices;Machine learning;Database languages;Database systems;Visual databases;Knowledge management;Humans;Algorithm design and analysis},
  doi={10.1109/HICSS.1995.375347},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{4221907,
  author={Eldakar, Youssef and Adly, Noha and Nagi, Magdy},
  booktitle={2006 1st International Conference on Digital Information Management},
  title={A Framework for the Encoding of Multilayered Documents},
  year={2007},
  volume={},
  number={},
  pages={306-313},
  abstract={Electronic publishing of material digitized using imaging and OCR calls for a special delivery format capable of reconstructing original documents in a well-usable electronic form. We present a framework for the universal encoding of multilingual image-on-text documents, enabling retrieval systems to text-search and highlight hits on original page images. A generalized format for representation of image-on-text allows for integration of different OCR engines and target format encoders. This framework's current implementation encodes multilingual content into DjVu and PDF. Performance has been evaluated with focus on file size and shown that overhead of adding text layers is small compared to advantages and that output is comparable to other systems.},
  keywords={Encoding;Optical character recognition software;Image reconstruction;Image coding;Image retrieval;Software libraries;Electronic publishing;Humans;Books;XML},
  doi={10.1109/ICDIM.2007.369215},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{5228034,
  author={Anand, T. and Padmapriya, S. and Kirubakaran, E.},
  booktitle={2009 International Conference on Intelligent Agent & Multi-Agent Systems},
  title={Terror tracking using advanced web mining perspective},
  year={2009},
  volume={},
  number={},
  pages={1-4},
  abstract={Web mining is a rapidly growing research area. It consists of Web usage mining, Web structure mining, and Web content mining. Web usage mining refers to the discovery of user access patterns from Web usage logs. Web structure mining tries to discover useful knowledge from the structure of hyperlinks. Web content mining aims to extract/mine useful information or knowledge from Web page contents. Web mining techniques can be used for detecting and avoiding terror threats caused by terrorists all over the world.},
  keywords={Web mining;Data mining;Web pages;Text mining;Terrorism;Blogs;Counting circuits;Computer science;Information technology;Machine learning;usage mining;structure mining;content mining and patterns},
  doi={10.1109/IAMA.2009.5228034},
  ISSN={},
  month={July},}@BOOK{10614710,
  author={Presner, Todd and Bonazzi, Anna and Deblinger, Rachel and Fan, Lizhou and Lee, Michelle and Rosen, Kyle and Yamane, Campbell},
  title={Ethics of the Algorithm: Digital Humanities and Holocaust Memory},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={How computational methods can expand how we see, read, and listen to Holocaust testimonyThe Holocaust is one of the most documented—and now digitized—events in human history. Institutions and archives hold hundreds of thousands of hours of audio and video testimony, composed of more than a billion words in dozens of languages, with millions of pieces of descriptive metadata. It would take several lifetimes to engage with these testimonies one at a time. Computational methods could be used to analyze an entire archive—but what are the ethical implications of “listening” to Holocaust testimonies by means of an algorithm? In this book, Todd Presner explores how the digital humanities can provide both new insights and humanizing perspectives for Holocaust memory and history.Presner suggests that it is possible to develop an “ethics of the algorithm” that mediates between the ethical demands of listening to individual testimonies and the interpretative possibilities of computational methods. He delves into thousands of testimonies and witness accounts, focusing on the analysis of trauma, language, voice, genre, and the archive itself. Tracing the affordances of digital tools that range from early, proto-computational approaches to more recent uses of automatic speech recognition and natural language processing, Presner introduces readers to what may be the ultimate expression of these methods: AI-driven testimonies that use machine learning to process responses to questions, offering a user experience that seems to replicate an actual conversation with a Holocaust survivor.With Ethics of the Algorithm, Presner presents a digital humanities argument for how big data models and computational methods can be used to preserve and perpetuate cultural memory.},
  keywords={AI;algorithmic fabulation;archive;big data;cultural analytics;datafication;digital culture;digital media;digital technologies;ethical computation;Ethics of the Algorithm: Digital Humanities and Holocaust Memory;humanistic data science;natural language processing;survivor;Testimony;Todd Presner;virtual;witness;Data;Survivors;Narrative;Algorithm;Holocaust;Jewish;Human;Triplets;USC Shoah foundation;Mala;Jews;Visualization;Digital;Ethical;Semantic;Algorithmic;Memory;Ethics;Nazi;Database;Kimmelmann;Auschwitz;Corpus;War;Death;Trauma;Dimensions in Testimony (DiT);Network;Semantic triplets;History;Holocaust testimony;Judgment;Distant;Technologies;Cultural;Segments;Processes;Violence;Victims;Child;Population;Dutch;Fortunoff;Bomba;Ghetto;Silence;Media;Machine learning;Clusters;Police;Labor;Technology},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691258980},
  url={https://ieeexplore.ieee.org/document/10614710},}@INPROCEEDINGS{6103313,
  author={Costa, Gianni and Ortale, Riccardo and Ritacco, Ettore},
  booktitle={2011 IEEE 23rd International Conference on Tools with Artificial Intelligence},
  title={Effective XML Classification Using Content and Structural Information via Rule Learning},
  year={2011},
  volume={},
  number={},
  pages={102-109},
  abstract={We propose a new approach to XML classification, that uses a particular rule-learning technique for the induction of interpretable classification models. These separate the individual classes of XML documents by looking at the presence within the XML documents themselves of certain features, that provide information on their content and structure. The devised approach induces classifiers with outperforming effectiveness in comparison to several established competitors.},
  keywords={XML;Vegetation;Training data;Databases;Data models;Context;Predictive models},
  doi={10.1109/ICTAI.2011.24},
  ISSN={2375-0197},
  month={Nov},}@ARTICLE{9467368,
  author={Das, Badhan Chandra and Anwar, Md. Musfique and Bhuiyan, Md. Al-Amin and Sarker, Iqbal H. and Alyami, Salem A. and Moni, Mohammad Ali},
  journal={IEEE Access},
  title={Attribute Driven Temporal Active Online Community Search},
  year={2021},
  volume={9},
  number={},
  pages={93976-93989},
  abstract={Almost all of the existing approaches to determining online local community are typically deliberated like-minded users who have similar topical interests. However, such methodologies overlook the prospective temporality of users' interests as well as users' degree of topical activeness. As a result, the consequential communities might have extremely lower active users. This research investigates how online social users' behaviors and topical activeness vary over time and how these parameters can be employed in order to improve the quality of the detected local community. For a given input query, consisting a query node (user) and a set of attributes, this research intends to find densely-connected community in which community members are temporally similar in terms of their activities related to the query attributes. To address the proposed problem, we develop a temporal activity biased weight model which gives higher weight to users' recent activities and develop an algorithm to search an effective community. The effectiveness of the proposed methodology is justified using four benchmark datasets and compared with four other baseline methods. Experimental results demonstrate that our proposed framework yields better outcomes than the baseline methods for all four benchmark datasets.},
  keywords={Social networking (online);Sports;Blogs;Frequency measurement;Computer science;Benchmark testing;Time measurement;Online local community;query attributes;temporal topical activeness},
  doi={10.1109/ACCESS.2021.3093368},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10593600,
  author={Soni, Tanishq and Gupta, Deepali and Dutta, Monica},
  booktitle={2024 5th International Conference for Emerging Technology (INCET)},
  title={Understanding the Influence of Deep Learning on Mitigating Leaf Diseases: A Bibliometric Analysis},
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Plant diseases that affect the leaves represent a substantial risk to the production of agriculture all over the globe. In the topic of leaf disease management, the purpose of this bibliometric study is to offer a thorough picture of research trends, citation patterns, important keywords, and crucial papers. In order to construct a complete collection of publications relating to leaf diseases that were published over the course of the last several decades, a methodical methodology that included exploring the SCOPUS database was used. The study consists of both co-occurrence analysis of keywords and citation networks, which are used to discover prominent publications and authors. Additionally, the analysis highlights developing trends and main areas of research. The geographical distribution of research is also investigated in this study, with the goal of identifying regional concentrations and collaborative efforts in the fight against leaf diseases. This bibliometric study presents significant insights into the development of research, crucial contributions, and future directions in the fight against leaf diseases. This provides researchers, practitioners, and policymakers in the agricultural sector with vital assistance.},
  keywords={Visualization;Plant diseases;Face recognition;Bibliometrics;Collaboration;Production;Market research;Agriculture;Co-occurrence analysis;Leaf Disease;Bibliometric Analysis},
  doi={10.1109/INCET61516.2024.10593600},
  ISSN={},
  month={May},}@ARTICLE{10534228,
  author={},
  journal={IEEE Std 1547.2‐2023 (Revision of IEEE Std 1547.2‐2008)},
  title={IEEE Application Guide for IEEE Std 1547™‐2018, IEEE Standard for Interconnection and Interoperability of Distributed Energy Resources with Associated Electric Power Systems Interfaces},
  year={2024},
  volume={},
  number={},
  pages={1-291},
  abstract={Technical background and application details to support understanding of IEEE Std 1547™- 2018 are provided. The guide facilitates the use of IEEE Std 1547-2018 by characterizing various forms of distributed energy resource (DER) technologies and their associated interconnection issues. It provides background and rationale of the technical requirements of IEEE Std 1547-2018. It also provides tips, techniques, and common practices to address issues related to DER project implementation. This guide is intended for use by engineers, engineering consultants, and knowledgeable individuals in the field of DERs. The IEEE 1547 series of standards is cited in the Federal Energy Policy Act of 2005, and this guide is one document in the IEEE 1547 series.},
  keywords={IEEE Standards;Codes;Generators;Diesel engines;Induction machines;Fuel cells;Power grids;Fault diagnosis;Energy consumption;Power systems;Harmonic analysis;Energy storage;amendment;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547™;IEEE 1547.2™;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={10.1109/IEEESTD.2024.10534228},
  ISSN={},
  month={May},}@INPROCEEDINGS{560024,
  author={Cunningham, S.J. and Connaway, L.S.},
  booktitle={Proceedings Sixth Australian Conference on Computer-Human Interaction},
  title={Information searching preferences and practices of computer science researchers},
  year={1996},
  volume={},
  number={},
  pages={294-299},
  abstract={We present preliminary findings of an ongoing study of the ways that computer scientists seek, use, and store information when conducting research. Their preferred methods of information foraging has implications for the design of information retrieval systems for these researchers. Traditional indexing schemes based on controlled vocabularies see little use. Researchers rely heavily on browsing and citation searches-information gathering techniques that are not well supported by existing indexes and retrieval systems. Not surprisingly, resources that can be immediately accessed from the user's office (particularly via the Internet) are preferred to those requiring a special trip to another location (such as a university library).},
  keywords={Computer science;Information retrieval;Electronic mail;Indexing;Vocabulary;Internet;Science - general;Bibliographies;Software libraries;Writing},
  doi={10.1109/OZCHI.1996.560024},
  ISSN={},
  month={Nov},}@ARTICLE{10345586,
  author={Adel, Kareem and Elhakeem, Ahmed and Marzouk, Mohamed},
  journal={IEEE Access},
  title={Blockchain and Artificial Intelligence: Scientometric Analysis and Visualization},
  year={2023},
  volume={11},
  number={},
  pages={137911-137928},
  abstract={Integrating Artificial Intelligence (AI) with Blockchain Technology (BT) is deemed the fourth generation of BT applications (Blockchain 4.0). This generation has gained considerable attention from the research community. Such attention has led to a vast amount of scientific literature. However, a comprehensive quantitative analysis of this literature is still missing. The present study conducts a scientometric analysis to explore and characterize the development track and trends of BT-AI research. Using the Web of Science (WoS) Core Collection database, a total of 2615 peer-reviewed journal articles were identified between 2017–2023 and extracted for analysis, while employing VOSviewer and Biblioshiny as software tools. First, the publication trend was analyzed, and the pivotal articles were identified. Second, the scientific collaboration networks were analyzed and mapped to identify the key researchers, countries, and organizations. Third, the sources’ productivity and citation were analyzed and mapped to identify the dependable sources of information and the best-fit sources for publishing the BT-AI studies. Fourth, the conceptual structure for the BT-AI literature was analyzed and visualized using keywords co-occurrence and keywords thematic evolution to explore and identify the research hotspots and emerging themes. The findings of this study can help in further familiarizing new researchers with BT-AI literature and assist practitioners, policy-makers, and editors to focus on the promising and arising BT-AI trends for further development.},
  keywords={Blockchains;Peer-to-peer computing;Organizations;Databases;Data visualization;Software tools;Market research;Artificial intelligence;Bibliometrics;Data visualization;Artificial intelligence;bibliometrics;blockchains;data visualization;reviews},
  doi={10.1109/ACCESS.2023.3339752},
  ISSN={2169-3536},
  month={},}@INBOOK{10138158,
  author={Tari, Zahir and Sohrabi, Nasrin and Samadi, Yasaman and Suaboot, Jakapan},
  booktitle={Data Exfiltration Threats and Prevention Techniques: Machine Learning and Memory-Based Data Security},
  title={Memory&#x2010;Based Data Exfiltration Detection Methods},
  year={2023},
  volume={},
  number={},
  pages={181-219},
  abstract={This chapter looks at efficient methods that monitor sensitive data in the random access memory (RAM). The method described in this chapter is called Fast lookup Bag&#x2010;of&#x2010;Words (FBoW), and it is an approximate multipattern matching method for text documents. FBoW addresses several aspects in matching the RAM's textual sensitive data, such as scalability (i.e. when the database of sensitive data contains many documents) and noise (i.e. the noise from decoding the nontextual elements in the memory to extra characters or reordering the content as per memory paging). FBoW can be summarized as follows: (i) an innovative pattern&#x2010;matching algorithm for multiple long text corpus that is memory and run&#x2010;time efficient and (ii) a customizable approximate search algorithm that allows a user to fine&#x2010;tune a trade&#x2010;off between scalability (i.e. memory footprint and processing time) and the detection accuracy.},
  keywords={Pattern matching;Memory management;Random access memory;Behavioral sciences;Runtime;Monitoring;Malware},
  doi={10.1002/9781119898900.ch7},
  ISSN={},
  publisher={IEEE},
  isbn={9781119898887},
  url={https://ieeexplore.ieee.org/document/10138158},}@ARTICLE{10313264,
  author={Moraitis, Michail},
  journal={IEEE Access},
  title={FPGA Bitstream Modification: Attacks and Countermeasures},
  year={2023},
  volume={11},
  number={},
  pages={127931-127955},
  abstract={Advances in Field-Programmable Gate Array (FPGA) technology in recent years have resulted in an expansion of its usage in a very wide spectrum of applications. Apart from serving the traditional prototyping purposes, FPGAs are currently regarded as an integral part of embedded systems used in many industries, including communication, medical, aerospace, automotive, and military. Moreover, the emerging trend of AI has found FPGAs to be at the technological forefront with their use as deep learning acceleration platforms. The demand for FPGAs has grown to the point that major companies (e.g. Amazon) are offering cloud-based access to FPGAs, known as FPGA-as-a-Service. In many applications, FPGAs handle sensitive data and/or host cryptographic algorithm implementations. These FPGAs are not always located in a tamper-resistant environment, which makes their security a major concern, especially in light of the ever-growing number of publications demonstrating effective attacks specifically tailored to exploit the physical traits of FPGA implementations. In this survey, we cover the subset of those attacks that involve tampering with the FPGA configuration bitstream. We start by discussing how the FPGA vendors attempt to protect their products and how malicious parties try to overcome this protection. We then proceed to present the different bitstream modification attacks that can be found in the literature organized according to their targets. Finally, we present various countermeasures that can be deployed, drawing on bibliographic references from works specifically focused on FPGA bitstream protection, as well as those initially proposed for different purposes or devices that can be adapted for bitstream protection.},
  keywords={Field programmable gate arrays;Surveys;Cryptography;Random access memory;Routing;Performance evaluation;Microcontrollers;Physical security;reverse engineering;cryptographic implementation;FPGA;bitstream encryption;bitstream modification},
  doi={10.1109/ACCESS.2023.3331507},
  ISSN={2169-3536},
  month={},}@ARTICLE{8719895,
  author={Ain, Qurat Ul and Butt, Wasi Haider and Anwar, Muhammad Waseem and Azam, Farooque and Maqbool, Bilal},
  journal={IEEE Access},
  title={A Systematic Review on Code Clone Detection},
  year={2019},
  volume={7},
  number={},
  pages={86121-86144},
  abstract={Code cloning refers to the duplication of source code. It is the most common way of reusing source code in software development. If a bug is identified in one segment of code, all the similar segments need to be checked for the same bug. Consequently, this cloning process may lead to bug propagation that significantly affects the maintenance cost. By considering this problem, code clone detection (CCD) appears as an active area of research. Consequently, there is a strong need to investigate the latest techniques, trends, and tools in the domain of CCD. Therefore, in this paper, we comprehensively inspect the latest tools and techniques utilized for the detection of code clones. Particularly, a systematic literature review (SLR) is performed to select and investigate 54 studies pertaining to CCD. Consequently, six categories are defined to incorporate the selected studies as per relevance, i.e., textual approaches (12), lexical approaches (8), tree-based approaches (3), metric-based approaches (7), semantic approaches (7), and hybrid approaches (17). We identified and analyzed 26 CCD tools, i.e., 13 existing and 13 proposed/developed. Moreover, 62 open-source subject systems whose source code is utilized for the CCD are presented. It is concluded that there exist several studies to detect type1, type2, type3, and type4 clones individually. However, there is a need to develop novel approaches with complete tool support in order to detect all four types of clones collectively. Furthermore, it is also required to introduce more approaches to simplify the development of a program dependency graph (PDG) while dealing with the detection of the type4 clones.},
  keywords={Cloning;Charge coupled devices;Tools;Semantics;Software;Computer bugs;Databases;CCD;SLR;code clone detection;CCD tools;code clone types},
  doi={10.1109/ACCESS.2019.2918202},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9422595,
  author={Almuhanna, Abrar A. and Yafooz, Wael M. S.},
  booktitle={2021 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)},
  title={Expert Finding In Scholarly Data: An Overview},
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={In the era of digital transformation, when scholarly literature is rapidly growing, there are hundreds of papers published online daily in different fields, especially in the academic field. The huge volume of research papers published makes it difficult to find an expert/scholar to collaborate within a specific research area. This considers one of the most challenging factors in academia. Many researchers have proposed several methods to rank the authors based on their expertise in a specific area, focusing on co-citation, using keywords and their principal areas of research. The significant relationship or collaborations between scholars are credible. This paper explores the existing methods and tools in related studies to obtain author expertise among the scholarly network in determining the expertise in a specific area. Also, the semantic relations in the heterogeneous networks are presented with an overview of building a visual scholarly network that is beneficial in many pieces of research in data mining and related areas.},
  keywords={Visualization;Systematics;Mechatronics;Digital transformation;Semantics;Focusing;Data visualization;Expert Finding;Co-colleaperation;Semantic Similarity;Expert Scholar},
  doi={10.1109/IEMTRONICS52119.2021.9422595},
  ISSN={},
  month={April},}@INBOOK{9968178,
  author={Tuffery, Stephane},
  booktitle={Deep Learning: From Big Data to Artificial Intelligence with R},
  title={Social Network Analysis},
  year={2023},
  volume={},
  number={},
  pages={187-235},
  abstract={Social networks are at the heart of big data, with their huge quantities of data of all kinds, text, images, video, and audio. Graphs are used to represent social networks in particular and all networks in general. They are sets of vertices linked together by edges. In many applications of social networks, it is important to identify the most influential individuals. In a graph, the importance of a vertex can be expressed in several ways, the main ones being the degree centrality, the closeness centrality, the betweenness centrality, and prestige. A clique is a graph in which all vertices are connected and a quasi&#x2010;clique is a group of vertices that are highly connected. A community is a subgraph that is both a quasi&#x2010;clique and a quasi&#x2010;connected component. The business model of social networks is the provision of free services in exchange for personal data.},
  keywords={Social networking (online);Search engines;Organizations;Oral communication;Level measurement;Irrigation;Hypertext systems},
  doi={10.1002/9781119845041.ch5},
  ISSN={},
  publisher={Wiley},
  isbn={9781119845027},
  url={https://ieeexplore.ieee.org/document/9968178},}@INPROCEEDINGS{7184861,
  author={Elshawi, Radwa and Batarfi, Omar and Fayoumi, Ayman and Barnawi, Ahmed and Sakr, Sherif},
  booktitle={2015 IEEE First International Conference on Big Data Computing Service and Applications},
  title={Big Graph Processing Systems: State-of-the-Art and Open Challenges},
  year={2015},
  volume={},
  number={},
  pages={24-33},
  abstract={Graph is a fundamental data structure that captures relationships between different data entities. In practice, graphs are widely used for modeling complicated data in different application domains such as social networks, protein networks, transportation networks, bibliographical networks, knowledge bases and many more. Currently, graphs with millions and billions of nodes and edges have become very common. In principle, graph analytics is an important big data discovery technique. Therefore, with the increasing abundance of large graphs, designing scalable systems for processing and analyzing large scale graphs has become one of the most timely problems facing the big data research community. In general, distributed processing of big graphs is a challenging task due to their size and the inherent irregular structure of graph computations. Thus, in recent years, we have witnessed an unprecedented interest in building big graph processing systems that attempted to tackle these challenges. To better understand the challenges of developing scalable graph processing systems, in this paper, we provide a comprehensive overview of the state-of-the art. In addition, we identify a set of the current open research challenges and discuss some promising directions for future research.},
  keywords={Big Graph;Big Data;Graph Processing},
  doi={10.1109/BigDataService.2015.11},
  ISSN={},
  month={March},}@BOOK{10172355,
  author={Coveney, Peter and Highfield, Roger and Ramakrishnan, Venki},
  title={Virtual You: How Building Your Digital Twin Will Revolutionize Medicine and Change Your Life},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={The visionary science behind the digital human twins that will enhance our health and our futureVirtual You is a panoramic account of efforts by scientists around the world to build digital twins of human beings, from cells and tissues to organs and whole bodies. These virtual copies will usher in a new era of personalized medicine, one in which your digital twin can help predict your risk of disease, participate in virtual drug trials, shed light on the diet and lifestyle changes that are best for you, and help identify therapies to enhance your well-being and extend your lifespan—but thorny challenges remain.In this deeply illuminating book, Peter Coveney and Roger Highfield reveal what it will take to build a virtual, functional copy of a person in five steps. Along the way, they take you on a fantastic voyage through the complexity of the human body, describing the latest scientific and technological advances—from multiscale modeling to extraordinary new forms of computing—that will make “virtual you” a reality, while also considering the ethical questions inherent to realizing truly predictive medicine.With an incisive foreword by Nobel Prize–winning biologist Venki Ramakrishnan, Virtual You is science at its most astounding, showing how our virtual twins and even whole populations of virtual humans promise to transform our health and our lives in the coming decades.},
  keywords={Protein;Molecule;Machine learning;Supercomputer;Simulation;Digital Twins;Prediction;Biology;Calculation;Metabolism;Result;Computer;Technology;Organism;Qubit;Mathematics;Bacteria;Big data;Enzyme;Disease;Parameter;Scientist;Thought;Virtual Cell;Liver;Sovereignty;Theory;Instance (computer science);Photon;Computer simulation;Differential equation;Quantity;Artificial neural network;Global commons;Action potential;Circulatory system;Gene;Human rights;Genomics;Mycoplasma;Stephen Wolfram;Heart;Metamaterial;Emergence;Logic;Cadaver;Digestion;Clinical trial;Antigen;Quantum mechanics;Aneurysm;Activation;Experimental data;Chemist;Mathematician;Measurement;Ordinary differential equation;Email;Chemical process;Receptor (biochemistry);Chemical reaction;Millimetre;Molecular machine;Desertification;Cloud;Cognitive test;Free parameter;Electron microscope;Victor Veselago;Time evolution},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691223407},
  url={https://ieeexplore.ieee.org/document/10172355},}@ARTICLE{9641817,
  author={Atanasiu, Vlad},
  journal={IEEE Access},
  title={The Structural Information Potential and its Application to Document Triage},
  year={2022},
  volume={10},
  number={},
  pages={13103-13138},
  abstract={This article introduces structural information potential (SIP), a measure of information based on the potential of structures to be informative about their content. An example of this concept is the clustered appearance that typically characterizes the first page of scientific articles, which summarizes the article’s contents and provides additional data, yielding potentially the largest and most diverse amount of information from a single page in the shortest time with the least effort. This characteristic makes SIP particularly well-adapted to triage tasks (i.e., rapid decision-making under conditions of uncertainty and limited resources), an application illustrated by means of a case study on classifying document images. The SIP method consists in unifying the Shannon entropy, the Fourier transform, the fractal dimension, and the golden ratio into a single equation and several algorithmic components. While the application domain is document images, the concept has generic character. The method results in a mathematically and perceptually coherent pattern space, characterized by continuous transition between uniform, clustered, and regular configurations, and corresponding to a structural information potential with a well-defined maximum. The maximum SIP leads to the identification of shapes and patterns with minimal structural redundancy, termed “fluorescent objects” as a complement to regular graphs and the Platonic solids.},
  keywords={Entropy;Fractals;Task analysis;Redundancy;Uncertainty;Text analysis;Spectral analysis;Information theory;structural entropy;spectral entropy;Fourier transform;fractals;graph theory;golden ratio;pattern analysis;image classification;document analysis;layout analysis;document triage;digital libraries},
  doi={10.1109/ACCESS.2021.3133654},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{7555994,
  author={Hussein, Ashraf S.},
  booktitle={2016 SAI Computing Conference (SAI)},
  title={Visualizing document similarity using n-grams and latent semantic analysis},
  year={2016},
  volume={},
  number={},
  pages={269-279},
  abstract={As the number of information resources and document quantity explodes, efficient tools with intuitive visualization capabilities desperately needed to assist users in conducting document similarity analysis and/or plagiarism detection tasks by discovering hidden relations among documents. This paper proposes a content-based method for document similarity analysis and visualization. The proposed method is based on modeling the relationship between documents and their n-gram phrases, which are generated from the normalized text, exploiting morphology analysis and lexical lookup. Resolving possible morphological ambiguities is carried out by tagging the words within the examined documents. Text indexing and stop-words removal are performed, employing a new technique that is efficient in dealing with multiple long documents. The examined documents' TF-IDF model is constructed using heuristic based pair-wise matching algorithm, considering lexical and syntactic changes. Then, the hidden associations between the documents and their unique n-gram phrases are investigated using Latent Semantic Analysis (LSA). Next, the pairwise document subset and similarity measures are derived from the Singular Value Decomposition (SVD) computations. Different visualization techniques are then applied on the SVD results to expose the hidden relations among the documents under consideration. As Arabic is one of the most morphological and complicated languages, this paper emphasizes Arabic documents similarity analysis and visualization. Various experiments were carried out revealing the strong capabilities of the proposed method in analyzing and visualizing literal and some types of intelligent similarities.},
  keywords={Plagiarism;Text analysis;Data visualization;Semantics;Visualization;Natural language processing;document visualization;text-reuse;text mining;similarity estimation;plagiarism check;natural language processing;Latent Semantic Analysis;Singular Value Decomposition},
  doi={10.1109/SAI.2016.7555994},
  ISSN={},
  month={July},}@INPROCEEDINGS{7975929,
  author={de Oliveira, Francisco Kelsen and de Oliveira, Max Brandão and Gomes, Alex Sandro and Queiros, Leandro Marques},
  booktitle={2017 12th Iberian Conference on Information Systems and Technologies (CISTI)},
  title={RECREIO: Floss as SAAS for sharing of educational resources},
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={The objective of this study was to verify if a development model of Units of Learning (UoLs), supported by UoLs authoring tools, that meet the usability criteria and abstract advanced knowledge in programming language by the developers, effectively provides the use and implementation of such resources by teachers in all areas of high school (in this first phase of the research) with few skills with technology. This is also due to the fact that many teachers consume much more resources and also the difficulties faced by teachers and developers in reusing their resources in other environments because they were initially designed for a specific environment. The method was based on the paradigm of Design Science Research (DSR), which allowed us to understand the problem and solve it creatively from useful artifacts. The three cycles of the research used questionnaires, interviews and documentary collection as instruments of data collection of the first cycle of survey and survey of the problem under study, while the non-participant observation and questionnaires were used in the second and third cycles at the time of evaluations of the system interfaces by users. Suggestions for improvements were implemented in the Recreio, while integrations to the Learning Management Systems (LMS) are being developed. Preliminary results showed the importance of Recreio incorporate tools of authorship of UoLs, preferably, free and online. In addition, users' reports made explicit the need for a space for sharing and dissemination of the resources developed directly in LMS, as well as another space destined for exchanging experiences and learning with courses among users.},
  keywords={Tools;Unified modeling language;Computational modeling;Education;Software as a service;Internet;Repository;authoring tool;exchanges of experiences;resource sharing},
  doi={10.23919/CISTI.2017.7975929},
  ISSN={},
  month={June},}@ARTICLE{10494051,
  author={Li, Yang and Liu, Kangbo and Satapathy, Ranjan and Wang, Suhang and Cambria, Erik},
  journal={IEEE Computational Intelligence Magazine},
  title={Recent Developments in Recommender Systems: A Survey [Review Article]},
  year={2024},
  volume={19},
  number={2},
  pages={78-95},
  abstract={In this technical survey, the latest advancements in the field of recommender systems are comprehensively summarized. The objective of this study is to provide an overview of the current state-of-the-art in the field and highlight the latest trends in the development of recommender systems. It starts with a comprehensive summary of the main taxonomy of recommender systems, including personalized and group recommender systems. In addition, the survey analyzes the robustness, data bias, and fairness issues in recommender systems, summarizing the evaluation metrics used to assess the performance of these systems. Finally, it provides insights into the latest trends in the development of recommender systems and highlights the new directions for future research in the field.},
  keywords={Collaborative software;Recommender systems;Surveys;Taxonomy;Performance evaluation;Group recommendation;personalized recommendation;recommender system},
  doi={10.1109/MCI.2024.3363984},
  ISSN={1556-6048},
  month={May},}@ARTICLE{10552262,
  author={Shevchuk, Ruslan and Martsenyuk, Vasyl},
  journal={IEEE Access},
  title={Neural Networks Toward Cybersecurity: Domain Map Analysis of State-of-the-Art Challenges},
  year={2024},
  volume={12},
  number={},
  pages={81265-81280},
  abstract={The growing interest in applying neural networks for cybersecurity has prompted a substantial increase in related research. This paper presents a comprehensive bibliometric analysis of research on cybersecurity towards neural networks published in the Web of Science over the past two decades (2003–2023) using bibliometric methods and CiteSpace software. The analysis encompasses yearly publication trends, types of publications, and trends across various dimensions such as publishing sources, organizations, researchers, countries, and keywords. Additionally, timeline and burst detection analyses were conducted to identify significant topic trends and citations in the last two decades. It also outlines the latest trends, under-explored topics, and open challenges.},
  keywords={Neural networks;Computer security;Market research;Bibliometrics;Visualization;Knowledge engineering;Databases;Neural network;cybersecurity;scientometric database;domaine map analysis;CiteSpace},
  doi={10.1109/ACCESS.2024.3411632},
  ISSN={2169-3536},
  month={},}@ARTICLE{9354144,
  author={},
  journal={IEEE P2410-2021/D10, February 2021},
  title={IEEE Draft Standard for Biometric Privacy},
  year={2021},
  volume={},
  number={},
  pages={1-52},
  abstract={The Standard for Biometric Privacy (SBP) provides private identity assertion. SBP supersedes the prior IEEE Std 2410(TM)-2019 by including a formal specification for privacy and biometrics such that a conforming SBP system does not incur GDPR, CCPA, BIPA or HIPAA privacy obligations. Homomorphic encryption ensures the biometric payload is always one-way encrypted with no need for key management and provides full privacy by ensuring plaintext biometrics are never received by the SBP server. The SBP implementation includes software running on a client device and on the SPB server. Pluggable components are used to replace legacy functionality to allow rapid integration into existing operating environments. The SBP implementation allows the systems to meet security needs by using the application programming interface, whether the underlying system is a relational database management system or a search engine. The SBP implementation functionality offers a “point-and-cut” mechanism to add the appropriate security to the production systems as well as to the systems in development. The architecture is language neutral, allowing Representational State Transfer (REST ), JavaScript Object Notation (JSON), and Transport Layer Security (TLS) to provide the communication interface. This document describes the essential methodology to SBP.},
  keywords={IEEE Standards;Biometrics;Privacy;Servers;admin console;application;biometric-driven device;biometrics;privacy;client device IDS;IDS cluster;IEEE Std 2410(TM);Jena Rules;liveness;original site admin;SBP admin;SBP cluster;SBP IDS;SBP server;site admin;trusted adjudicated data;user},
  doi={},
  ISSN={},
  month={Feb},}@ARTICLE{9299470,
  author={},
  journal={IEEE P2410-2020/D5, December 2020},
  title={IEEE Draft Standard for Biometric Privacy},
  year={2020},
  volume={},
  number={},
  pages={1-50},
  abstract={The Standard for Biometric Privacy (SBP) provides private identity assertion. SBP supersedes the prior IEEE Std 2410(TM)-2019 by including a formal specification for privacy and biometrics such that a conforming SBP system does not incur GDPR, CCPA, BIPA or HIPAA privacy obligations. Homomorphic encryption ensures the biometric payload is always one-way encrypted with no need for key management and provides full privacy by ensuring plaintext biometrics are never received by the SBP server. The SBP implementation includes software running on a client device and on the SPB server. Pluggable components are used to replace legacy functionality to allow rapid integration into existing operating environments. The SBP implementation allows the systems to meet security needs by using the application programming interface, whether the underlying system is a relational database management system or a search engine. The SBP implementation functionality offers a “point-and-cut” mechanism to add the appropriate security to the production systems as well as to the systems in development. The architecture is language neutral, allowing Representational State Transfer (REST ), JavaScript Object Notation (JSON), and Transport Layer Security (TLS) to provide the communication interface. This document describes the essential methodology to SBP.},
  keywords={IEEE Standards;Privacy;Biometrics (access control);Servers;Encryption;Security;admin console;application;biometric-driven device;biometrics;privacy;client device IDS;IDS cluster;IEEE Std 2410(TM);Jena Rules;liveness;original site admin;SBP admin;SBP cluster;SBP IDS;SBP server;site admin;trusted adjudicated data;user},
  doi={},
  ISSN={},
  month={Dec},}@INBOOK{10138160,
  author={Tari, Zahir and Sohrabi, Nasrin and Samadi, Yasaman and Suaboot, Jakapan},
  booktitle={Data Exfiltration Threats and Prevention Techniques: Machine Learning and Memory-Based Data Security},
  title={Introduction},
  year={2023},
  volume={},
  number={},
  pages={1-18},
  abstract={This chapter introduces the topics to be covered in this book. A brief overview of existing data exfiltration methods is provided (e.g. malicious behavior detection methods, RAM&#x2010;based data exfiltration detection methods, and temporal data exfiltration methods). A summary of the main research questions covered by the book is also detailed, and the scope of the book is provided with a detailed summary of the various contributions, such as data security threats, use&#x2010;cases, sub&#x2010;curve HMM, Fast lookup Bag&#x2010;of&#x2010;Words (FBoW), and Temporal Memory Bag&#x2010;of&#x2010;Words (TMBoW).},
  keywords={Malware;Monitoring;Behavioral sciences;Random access memory;SCADA systems;Military aircraft;Hidden Markov models},
  doi={10.1002/9781119898900.ch1},
  ISSN={},
  publisher={IEEE},
  isbn={9781119898887},
  url={https://ieeexplore.ieee.org/document/10138160},}@INPROCEEDINGS{6920451,
  author={Blaiech, Hayfa and Neji, Mohamed and Wali, Ali and Alimi, Adel M.},
  booktitle={13th International Conference on Hybrid Intelligent Systems (HIS 2013)},
  title={Emotion recognition by analysis of EEG signals},
  year={2013},
  volume={},
  number={},
  pages={312-318},
  abstract={We propose in this paper an emotional recognition system based on physiological signals. We adopt the seven basic emotions that are: neutrality, joy, sadness, fear, anger, disgust and surprise. An experiment has been conducted to verify the feasibility of the proposed system. This experience has allowed us to acquire EEG signals and to create an emotional database. For this, we have used the Emotiv EPOC headset. Thereafter, we have chosen the fuzzy logic techniques to classify the EEG signals and to analyze the results.},
  keywords={Electroencephalography;Databases;Standards;emotion recognition system;EEG;fuzzy logic},
  doi={10.1109/HIS.2013.6920451},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{1263251,
  author={Mao, S. and Kim, J.W. and Thoma, G.R.},
  booktitle={First International Workshop on Document Image Analysis for Libraries, 2004. Proceedings.},
  title={A dynamic feature generation system for automated metadata extraction in preservation of digital materials},
  year={2004},
  volume={},
  number={},
  pages={225-232},
  abstract={Obsolescence in storage media and the hardware and software for access and use can render old electronic files inaccessible and unusable. Therefore, the long-term preservation of digital materials has become an active area of research. At the U.S. National Library of Medicine (NLM), we are investigating the preservation of scanned and online medical journal articles, though other data types (e.g., video sequences) are also of interest. Metadata of different types have been proposed to save the information needed to preserve digital materials. Given the ever-increasing volume of medical journals and high labor cost of manual data entry, automated metadata extraction is crucial. A system has been developed at NLM to automatically generate descriptive metadata that includes title, author, affiliation, and abstract from scanned medical journals. A module called ZoneMatch is used to generate geometric and contextual features from a set of issues of each journal. A rule-based labeling module (called ZoneCzar) then uses these features to perform labeling independent of journal layout styles. However, if there are significant style variations among the issues of a same journal, the features generated from one set of journal issues may not be very useful to label a different set. We describe a dynamic feature updating system in which the features used for labeling a current journal issue are generated from previous issues with similar layout style. This new system can adapt to possible style variations among different issues of the same journal. Experimental results presented show that the new system delivers improved labeling performance accuracy.},
  keywords={Hardware;Material storage;Labeling;Software libraries;Costs;Conducting materials;Data mining;Emulation;Storage automation;Video sequences},
  doi={10.1109/DIAL.2004.1263251},
  ISSN={},
  month={Jan},}@ARTICLE{9930960,
  author={},
  journal={ISO/IEC/IEEE 8802-11:2022(E)},
  title={ISO/IEC/IEEE - International Standard - Telecommunications and information exchange between systems--Specific requirements for local and metropolitan area networks--Part 11: Wireless LAN medium access control (MAC) and physical layer (PHY) specifications},
  year={2022},
  volume={},
  number={},
  pages={1-4382},
  abstract={Technical corrections and clarifications to IEEE Std 802.11 for wireless local area networks (WLANs) as well as enhancements to the existing medium access control (MAC) and physical layer (PHY) functions are specified in this revision. Amendments 1 to 5 published in 2016 and 2018 have also been incorporated into this revision.},
  keywords={IEEE Standards;IEC Standards;ISO Standards;Encryption;Array signal processing;Collision avoidance;Resource management;Authentication;Channel estimation;Clustering methods;2.4 GHz;256-QAM;3650 MHz;4.9 GHz;5 GHz;5.9 GHz;60 GHz;advanced encryption standard;AES;audio;beamforming;carrier sense multiple access/collision avoidance;CCMP;channel switching;clustering;contention based access period;Counter mode with Cipherblock chaining Message authentication code Protocol;confidentiality;CSMA/CA;DFS;direct link;directional multi-gigabit;dynamic allocation of service period;dynamic extension of service period;dynamic frequency selection;dynamic truncation of service period;E911;EDCA;emergency alert system;emergency services;fast session transfer;forwarding;GCMP;generic advertisement service;high throughput;IEEE 802.11;international roaming;interworking;interworking with external networks;LAN;local area network;MAC;management;measurement;medium access control;media-independent handover;medium access controller;mesh;MIS;millimeter-wave;MIMO;MIMO-OFDM;multi-band operation;multi-hop;multi-user MIMO;multiple input multiple output;network advertisement;network discovery;network management;network selection;noncontiguous frequency segments;OCB;path-selection;personal basic service set;PHY;physical layer;power saving;QoS;quality of service;quality-of-service management frame;radio;radio frequency;RF;radio resource;radio management;relay operation;spatial sharing;SSPN;subscriber service provider;television white spaces;TPC;transmit power control;video;wireless access in vehicular environments;wireless LAN;wireless local area network;WLAN;wireless network management;zero-knowledge proof},
  doi={10.1109/IEEESTD.2022.9930960},
  ISSN={},
  month={Oct},}@ARTICLE{10287332,
  author={Jiménez-Ocaña, Alvaro A. and Pantoja, Andrés and Valderrama, Mario Andrés and Giraldo, Luis Felipe},
  journal={IEEE Access},
  title={A Systematic Review of Technology-Aided Stress Management Systems: Automatic Measurement, Detection and Control},
  year={2023},
  volume={11},
  number={},
  pages={116109-116126},
  abstract={Even though stress response is a defense mechanism of the body to deal with adverse daily situations, prolonged exposure to these effects can trigger significant detriments to physical and mental health. The aim of this systematic review is to identify the use of technological tools in stress management, with a special focus on feedback control systems that include detection, control, and intervention phases. The databases selected for this systematic review, which applies the PRISMA protocol, are Scopus, IEEE Xplore, Web of Science, and Science Direct. We include research works that have experiments involving automated physiological data collection through non-invasive methods and an intervention technique to manage stress. Applying these criteria, a total of 75 articles are included in the final analysis. The quality of the included articles was assessed in the search strategy, the selection process and the data collection process, following the eligibility criteria. Summarizing some results, almost half of the studies included fifty or fewer participants in the experiments and twelve physiological variables were identified, being HR and ECG the most important ones. The most used technique of stress management was breathing and 16 articles used some type of feedback control, mainly biofeedback. Several promising physiological variables and intervention techniques are identified for implementing stress management systems. Although using machine learning in stress detection is common, its application to develop feedback control systems is limited. Moreover, it was found that the theory of control in dynamical systems has not been applied yet to design automatic stress management systems.},
  keywords={Human factors;Physiology;Systematics;Databases;Stress;Frequency measurement;Biological control systems;Signal processing;Physiology;Automatic measurement;feedback systems;intervention techniques;physiological signals;stress management;systematic review},
  doi={10.1109/ACCESS.2023.3325763},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{5606467,
  author={Halalai, Raluca and Lemnaru, Camelia and Potolea, Rodica},
  booktitle={Proceedings of the 2010 IEEE 6th International Conference on Intelligent Computer Communication and Processing},
  title={Distributed community detection in social networks with genetic algorithms},
  year={2010},
  volume={},
  number={},
  pages={35-41},
  abstract={Community detection in social networks is a hot research topic that has received great interest in the recent years due to its wide applicability. This paper proposes a scalable approach for community structure identification using a genetic algorithm. Two existing fitness functions are analyzed and genetic parameters are tuned on thoroughly studied networks with known community structures. Experiments on a large data set show how the amount of time necessary to determine meaningful communities in a network is significantly reduced by running the algorithm distributed. This enables the analysis of larger, real-world networks. We then propose a new fitness function that offers a good tradeoff between efficiency and speed.},
  keywords={Communities;Social network services;Genetics;Clustering algorithms;Educational institutions;Scalability;Libraries},
  doi={10.1109/ICCP.2010.5606467},
  ISSN={},
  month={Aug},}@ARTICLE{10078891,
  author={Chemnad, Khansa and Othman, Achraf},
  journal={IEEE Access},
  title={Advancements in Arabic Text-to-Speech Systems: A 22-Year Literature Review},
  year={2023},
  volume={11},
  number={},
  pages={30929-30954},
  abstract={Although there are several speech synthesis models available for different languages tailored to specific domain requirements and applications, there is currently no readily available information on the latest trends in Arabic language speech synthesis. This can make it challenging for beginners to research and develop text-to-speech (TTS) systems for Arabic. To address this issue, this article provides a comprehensive overview of several scholars’ contributions to the field of Arabic TTS, along with an examination of the unique features of the Arabic language and the corresponding challenges in creating TTS systems. Reporting only on papers discussing Arabic TTS, this systematic review evaluated available literature published between 2000 and 2022. We conducted a systematic review of six databases using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to identify studies that addressed Arabic Text-to-Speech systems. Of the 3719 articles identified, only 36 (0.96%) met our search criteria. Bibliometric analyses of these studies were conducted and reported. The results highlight the main types of speech synthesis techniques used in TTS systems: concatenative, formant, deep neural network (DNN), hybrid models, and multiagent. The corpora used to develop these systems, as well as the diacritization techniques incorporated, evaluation techniques, and the results of the performance of the systems are reported. Subjective evaluation using the mean opinion score is the most commonly applied method to measure the accuracy of systems. This study also identifies gaps in the literature and makes recommendations for future research directions.},
  keywords={Speech synthesis;Text analysis;Natural language processing;Systematics;Syntactics;Smart phones;Morphology;Text-to-speech;speech synthesis;Arabic;low-resource languages;natural language processing},
  doi={10.1109/ACCESS.2023.3260844},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{655242,
  author={Stohr, E.A. and Yongbeom Kim},
  booktitle={Proceedings of the Thirty-First Hawaii International Conference on System Sciences},
  title={A model for performance evaluation of interactive systems},
  year={1998},
  volume={4},
  number={},
  pages={2-11 vol.4},
  abstract={We describe a quantitative model for the performance evaluation of interactive computer systems. The approach involves the development of an "interaction graph" or state transition diagram to describe the user-machine interaction. Given numerical data on transition times and probabilities, the model can be used to perform sensitivity analyses of changes in system parameters and user behavior. To illustrate the model, we use empirical data from field and laboratory experiments designed to compare a prototype natural language query system with a formal (relational) query system. The general approach is applicable in a broad range of other contexts including bibliographic retrieval and the analysis of web-log data. It should be of interest to both system developers and potential users of these systems.},
  keywords={Interactive systems;World Wide Web;System testing;Search engines;Web sites;Sensitivity analysis;Information retrieval;Performance evaluation;Laboratories;Prototypes},
  doi={10.1109/HICSS.1998.655242},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{6861068,
  author={Gutiérrez-Soto, Claudio and Hubert, Gilles},
  booktitle={2014 IEEE Eighth International Conference on Research Challenges in Information Science (RCIS)},
  title={Randomized algorithm for Information Retrieval using past search results},
  year={2014},
  volume={},
  number={},
  pages={1-9},
  abstract={In Information Retrieval, past searches are a source of useful information for new searches. This paper presents an approach for reusing past queries submitted to an information retrieval system and their returned results to build the result list for a new submitted query. This approach is based on a Monte Carlo algorithm to select past search results to answer the new query. The proposed algorithm is easy to implement and does not require learning. First experiments were carried out to evaluate the proposed algorithm. These experiments used a simulated dataset (i.e., document collections, queries and judgments of users are simulated). The proposed approach was compared with a traditional approach of information retrieval, showing better precision for our proposed approach.},
  keywords={Information retrieval;Arrays;Approximation methods;Approximation algorithms;Genetic algorithms;Context;Monte Carlo methods;Reusing past queries;probabilistic algorithm;IR},
  doi={10.1109/RCIS.2014.6861068},
  ISSN={2151-1357},
  month={May},}@ARTICLE{7954067,
  author={},
  journal={IEEE P2410/D3, May 2017},
  title={IEEE Draft Standard for Biometric Open Protocol},
  year={2017},
  volume={},
  number={},
  pages={1-153},
  abstract={Identity assertion, role gathering, multilevel access control, assurance, and auditing are provided by the Biometric Open Protocol Standard (BOPS). The BOPS implementation includes software running on a client device, a trusted BOPS server, and an intrusion detection system. The BOPS implementation allows pluggable components to replace existing components’ functionality, accepting integration into current operating environments in a short period of time. The BOPS implementation provides continuous protection to the resources and assurance of the placement and viability of adjudication and other key features. Accountability is the mechanism that proves a service-level guarantee of security. The BOPS implementation allows the systems to meet security needs by using the application programming interface. The BOPS implementation need not know whether the underlying system is a relational database management system or a search engine. The BOPS implementation functionality offers a “point-and-cut” mechanism to add the appropriate security to the production systems as well as to the systems in development. The architecture is language neutral, allowing Representational State Transfer (REST), JavaScript Object Notation (JSON), and Secure Sockets Layer (SSL) or Transport Layer Security (TLS) to provide the communication interface. The architecture is built on the servlet specification, open SSLs, Java, JSON, REST, and an open persistent store. All tools adhere to open standards, allowing maximum interoperability.},
  keywords={IEEE Standards;Biometrics;Open systems;Protocols;Trust management;Authentication;Computer architecture;Mobile handsets;Cryptography;admin console;application;BOPS admin;BOPS cluster;BOPS IDS;BOPS server;client device IDS;IDS cluster;IEEE 2410™;Jena Rules;liveness;original site admin;site admin;trusted adjudicated data;user;user device},
  doi={},
  ISSN={},
  month={June},}@ARTICLE{7903561,
  author={},
  journal={IEEE P2410/D2, April 2017},
  title={IEEE Draft Standard for Biometric Open Protocol},
  year={2017},
  volume={},
  number={},
  pages={1-125},
  abstract={Identity assertion, role gathering, multilevel access control, assurance, and auditing are provided by the Biometric Open Protocol Standard (BOPS). The BOPS implementation includes software running on a client device, a trusted BOPS server, and an intrusion detection system. The BOPS implementation allows pluggable components to replace existing components functionality, accepting integration into current operating environments in a short period of time. The BOPS implementation provides continuous protection to the resources and assurance of the placement and viability of adjudication and other key features. Accountability is the mechanism that proves a service-level guarantee of security. The BOPS implementation allows the systems to meet security needs by using the application programming interface. The BOPS implementation need not know whether the underlying system is a relational database management system or a search engine. The BOPS implementation functionality offers a point-and-cut mechanism to add the appropriate security to the production systems as well as to the systems in development. The architecture is language neutral, allowing Representational State Transfer (REST), JavaScript Object Notation (JSON), and Secure Sockets Layer (SSL) or Transport Layer Security (TLS) to provide the communication interface. The architecture is built on the servlet specification, open SSLs, Java, JSON, REST, and an open persistent store. All tools adhere to open standards, allowing maximum interoperability.},
  keywords={IEEE Standards;Trust management;Open systems;Protocols;Biometrics;Access control;Computer architecture;Computer security;admin console;application;BOPS admin;BOPS cluster;BOPS IDS;BOPS server;client device IDS;IDS cluster;IEEE 2410(TM);Jena Rules;liveness;original site admin;site admin;trusted adjudicated data;user;user device},
  doi={},
  ISSN={},
  month={Jan},}@BOOK{8187124,
  author={Adjeroh, Don and Bell, Tim and Mukherjee, Amar},
  title={Pattern Matching in Compressed Texts and Images},
  year={2013},
  volume={},
  number={},
  pages={},
  abstract={Pattern Matching in Compressed Texts and Images surveys and appraises techniques for pattern matching in compressed text and images. Normally compressed data needs to be decompressed before it is processed. If however the compression has been done in the right way, it is often possible to search the data without having to decompress it, or, at least, only partially decompress it. The problem can be divided into lossless and lossy compression methods, and then in each of these cases the pattern matching can be either exact or inexact. Much work has been reported in the literature on techniques for all of these cases. It includes algorithms that are suitable for pattern matching for various compression methods, and compression methods designed specifically for pattern matching. This monograph provides a survey of this work while also identifying the important relationship between pattern matching and compression, and proposing some performance measures for compressed pattern matching algorithms. Pattern Matching in Compressed Texts and Images is an excellent reference text for anyone who has an interest in the problem of searching compressed text and images. It concludes with a particularly insightful section on the ideas and research directions that are likely to occupy researchers in this field in the short and long term.},
  keywords={Search;data compression;text mining;Theoretical Computer Science: information retrieval;image and video retrieval},
  doi={10.1561/2000000038},
  ISSN={},
  publisher={now},
  isbn={9781601986856},
  url={https://ieeexplore.ieee.org/document/8187124},}@INPROCEEDINGS{9321944,
  author={Kunanets, Nataliia and Filippova, Natalia and Dobrovolska, Viktoriya and Kazimi, Parviz},
  booktitle={2020 IEEE 15th International Conference on Computer Sciences and Information Technologies (CSIT)},
  title={Biobibliographic Data Repository of Documentary Cultural Heritage},
  year={2020},
  volume={2},
  number={},
  pages={221-225},
  abstract={Goal of the article is to study the background of establishment, define advanced tasks and baseline of the biobibliographic data repository exemplified by the Repository of personal bibliographic indicators as part of the «Ukrainian National Biographic Archive» in the context of documentary cultural heritage. The requirements for the project implementation team formation and professional competencies, which contribute to establishment of effective communication ties between its members are substantiated. Conclusions. The defined advanced tasks and baseline of the biobibliographic data repository will enable the biobibliographic data repository project implementation as part of the «Ukrainian National Biographic Archive», which, in compliance with the requirements of logical structuring and its architectonic flexibility, by means of the measured search tooling will enable access to full text versions of bibliographic indicators devoted to prominent representatives of Ukrainian scientific idea and culture. Involvement of bibliographic edition compilers to the repository establishment process will ensure not only representational comprehensiveness of the mentioned scope of editions presented in e-repository, but also facilitate to compliance with regulations of the national law on copyright and related rights.},
  keywords={Libraries;Cultural differences;Global communication;Task analysis;Europe;Bioinformatics;Law;project management;documentary cultural heritage;consolidated information resources;biobibliographic data repository},
  doi={10.1109/CSIT49958.2020.9321944},
  ISSN={2766-3639},
  month={Sep.},}@INPROCEEDINGS{4119099,
  author={Mimno, David and McCallum, Andrew and Mann, Gideon S.},
  booktitle={Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)},
  title={Bibliometric impact measures leveraging topic analysis},
  year={2006},
  volume={},
  number={},
  pages={65-74},
  abstract={Measurements of the impact and history of research literature provide a useful complement to scientific digital library collections. Bibliometric indicators have been extensively studied, mostly in the context of journals. However, journal-based metrics poorly capture topical distinctions in fast-moving fields, and are increasingly problematic with the rise of open-access publishing. Recent developments in latent topic models have produced promising results for automatic sub-field discovery. The fine-grained, faceted topics produced by such models provide a clearer view of the topical divisions of a body of research literature and the interactions between those divisions. We demonstrate the usefulness of topic models in measuring impact by applying a new phrase-based topic discovery model to a collection of 300,000 Computer Science publications, collected by the Rexa automatic citation indexing system.},
  keywords={Bibliometrics;Software libraries;Computer science;Publishing;Permission;Context awareness;History;Indexing;Information retrieval;Text processing;data curation;harvesting;meta-search;search and retrieval protocols;subject portals;web services},
  doi={10.1145/1141753.1141765},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9559070,
  author={Tomat, Luka},
  booktitle={2021 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)},
  title={Current Trends in IoT Research},
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Recently, IoT has gained a lot of attention by the academia and the industry, and many research papers have been published in this area in the last years. While several papers offer thorough literature reviews on IoT studies, there exist a lack of literature that would show the research trends by applying bibliometric approach. Thus, to reveal the current knowledge structure and study areas in IoT research, presented study first defines the IoT concept and provides a literature review on published papers in this field. Next, it extracts the relevant recent IoT publications, indexed in the Web of Science database, and implements the co-occurrence analysis, which is a type of co-word bibliometric analysis, to determine the terms that co-occur in the selected papers. A special bibliometric software was used to identify the relationships among the main terms. The results revealed three cluster that are graphically presented and explained to reflect the current research nexus in the IoT field. Being a starting point of a broader research, this study enhances the existing body of knowledge in tacked area by applying bibliometric analysis and provides theoretical underpinnings for a better understanding of the current state and trends in IoT research as well as it offers methodological support for using bibliometric methods in similar future studies.},
  keywords={Industries;Couplings;Databases;Bibliographies;Bibliometrics;Market research;Data processing;IoT;Bibliometrics;Co-occurrence analysis;Text mining;Visualization},
  doi={10.23919/SoftCOM52868.2021.9559070},
  ISSN={1847-358X},
  month={Sep.},}@ARTICLE{10695061,
  author={Ghosh, Ananya and Deepa, K.},
  journal={IEEE Access},
  title={QueryMintAI: Multipurpose Multimodal Large Language Models for Personal Data},
  year={2024},
  volume={12},
  number={},
  pages={144631-144651},
  abstract={QueryMintAI, a versatile multimodal Language Learning Model (LLM) designed to address the complex challenges associated with processing various types of user inputs and generating corresponding outputs across different modalities. The proliferation of diverse data formats, including text, images, videos, documents, URLs, and audio recordings, necessitates an intelligent system capable of understanding and responding to user queries effectively. Existing models often exhibit limitations in handling multimodal inputs and generating coherent outputs across different modalities. The proposed QueryMintAI framework leverages state-of-the-art language models such as GPT-3.5 Turbo, DALL-E-2, TTS-1 and Whisper v2 among others, to enable seamless interaction with users across multiple modalities. By integrating advanced natural language processing (NLP) techniques with domain-specific models, QueryMintAI offers a comprehensive solution for text-to-text, text-to-image, text-to-video, and text-to-audio conversions. Additionally, the system supports document processing, URL analysis, image description, video summarization, audio transcription, and database querying, catering to diverse user needs and preferences. The proposed model addresses several limitations observed in existing approaches, including restricted modality support, lack of adaptability to various data formats, and limited response generation capabilities. QueryMintAI overcomes these challenges by employing a combination of advanced NLP algorithms, deep learning architectures, and multimodal fusion techniques.},
  keywords={Context modeling;Accuracy;Videos;Natural language processing;Computational modeling;Adaptation models;Deep learning;Large language models;Generative AI;Open source software;Multimodal large language models;generative AI;private database;Langchain;OpenAI},
  doi={10.1109/ACCESS.2024.3468996},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{6481051,
  author={Bancu, Cristian and Dagadita, Monica and Dascalu, Mihai and Dobre, Ciprian and Trausan-Matu, Stefan and Florea, Adina Magda},
  booktitle={2012 14th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
  title={ARSYS -- Article Recommender System},
  year={2012},
  volume={},
  number={},
  pages={349-355},
  abstract={In a world flooded by information, a filtering mechanism is compulsory. Recommender systems have taken a huge leap towards this goal, significantly improving the user experience in the online environment. There are two main approaches, content-based and collaborative filtering, both with advantages and drawbacks. We propose an article recommender system that integrates content based, collaborative and metadata recommendations, allowing users to select the method that best suits their needs. The first approach uses keywords in order to find similar articles, given a query or an entire document. Collaborative filtering is implemented using a P2P network in which data is distributed evenly across all peers. The last technique uses data from a semantic repository containing information about articles (e.g. title, author, domain), which can be interrogated using natural language-like queries. In addition, we present in detail the results obtained from employing the P2P network in terms of providing timely responses to the collaborative filtering technique and of ensuring reliability through data replication.},
  keywords={Collaboration;Peer-to-peer computing;Semantics;Portfolios;Recommender systems;Databases;article recommendation system;collaborative filtering;Peer-to-Peer;semantic repository},
  doi={10.1109/SYNASC.2012.38},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9754210,
  author={Kpoze, Aurelie' and Degila, Jules and Soude, Henoc and Wamba, Samuel Fosso},
  booktitle={2022 2nd International Conference on Innovative Practices in Technology and Management (ICIPTM)},
  title={Smart Grid Architecture: A Bibliometric Analysis and Future Research Directions},
  year={2022},
  volume={2},
  number={},
  pages={441-448},
  abstract={Smart Grids (SGs) are one of the most critical technological challenges nowadays and have attracted the interest of many researchers. An important research objective is to review the architectures proposed to design a robust research program to address the SG's future challenges. This paper presents a bibliometric study to obtain a structural overview of the Smart Grid architecture. A search for articles dealing with Smart Grid architecture was conducted in the core collection of the Web of Science database, which provides exhaustive citation data for global academic disciplines. As a result, 3993 documents were identified between 2000 and 2022, and the collected data were analyzed using bibliometric tools. The study results are presented and discussed in the following, and recommendations for future research efforts are provided.},
  keywords={Databases;Bibliometrics;Smart grids;Service-oriented architecture;Smart Grid architecture;Bibliometric study;Bibliometrix},
  doi={10.1109/ICIPTM54933.2022.9754210},
  ISSN={},
  month={Feb},}@ARTICLE{10689548,
  author={},
  journal={IEEE P1159.3/D6.2, September 2024},
  title={IEEE Draft Recommended Practice for Power Quality Data Interchange Format (PQDIF)},
  year={2024},
  volume={},
  number={},
  pages={1-102},
  abstract={A file format suitable for exchanging power quality related measurement and simulation data in a vendor independent manner is defined in this recommended practice. The format is designed to represent all power quality phenomena identified in IEEE Std 1159-2009, IEEE Recommended Practice on Monitoring Electric Power Quality, other power related measurement data, and is extensible to other data types as well. The recommended file format utilizes a highly compressed storage scheme to help reduce disk space and transmission times. The utilization of Globally Unique Identifiers (GUID) to represent each element in the file permits the format to be extensible without the need for a central registration authority},
  keywords={IEEE Standards;Power quality;Information exchange;Monitoring;Power transmission;Power system reliability;Power system measurements;data interchange;file format;IEEE 1159.3;measurement;monitoring;power quality;10 PQDIF11},
  doi={},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{1517833,
  author={Lampropoulos, A.S. and Lampropoulou, P.S. and Tsihrintzis, G.A.},
  booktitle={The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)},
  title={A middleware system for Web-based digital music libraries},
  year={2005},
  volume={},
  number={},
  pages={136-142},
  abstract={We present a middleware system that facilitates Internet users' access to Web-based digital music libraries and allows them to manipulate audio meta-information taking into consideration content and semantic information of music data. Useful relations in the data are automatically extracted through semantic networks (constructed and maintained in the library). Our system is complemented with a query-by-example retrieval subsystem, user relevance feedback facilities, and a new approach for musical genre classification based on the features extracted from signals that correspond to distinct musical instrument sources, as these sources have been identified by a source separation process. The system operation is illustrated in detail.},
  keywords={Middleware;Software libraries;Data mining;Internet;Music information retrieval;Feedback;Feature extraction;Signal processing;Instruments;Source separation},
  doi={10.1109/WI.2005.8},
  ISSN={},
  month={Sep.},}@ARTICLE{9805473,
  author={Zennayi, Yahya and Bourzeix, François and Guennoun, Zouhair},
  journal={IEEE Access},
  title={Analyzing the Scientific Evolution of Face Recognition Research and Its Prominent Subfields},
  year={2022},
  volume={10},
  number={},
  pages={68175-68201},
  abstract={This paper presents a science mapping approach to analyze thematic evolution of face recognition research. For this reason, different bibliometric tools are combined (performance analysis, science mapping and Co-word analysis) in order to identify the most important, productive and the highest-impact subfields. Moreover, different visualization tools are used to display a graphical vision of face recognition field to determine the thematic domains and their evolutionary behavior. Finally, this study proposes the most relevant lines of research for the face recognition field. Findings indicate a huge increase in face recognition research since 2014. Mixed approaches revealed a great interest compared to local and global approaches. In terms of algorithms, the use of deep learning methods is the new trend. On the other hand, the illumination variation impact on face recognition algorithms performances is nowadays, the most important and impacting challenge for the face recognition field.},
  keywords={Face recognition;Performance analysis;Databases;Lighting;Indexes;Feature extraction;Statistical analysis;Bibliometric studies;co-word analysis;face recognition;performance analysis;science mapping;thematic evolution},
  doi={10.1109/ACCESS.2022.3185137},
  ISSN={2169-3536},
  month={},}@ARTICLE{9402739,
  author={Baviskar, Dipali and Ahirrao, Swati and Potdar, Vidyasagar and Kotecha, Ketan},
  journal={IEEE Access},
  title={Efficient Automated Processing of the Unstructured Documents Using Artificial Intelligence: A Systematic Literature Review and Future Directions},
  year={2021},
  volume={9},
  number={},
  pages={72894-72936},
  abstract={The unstructured data impacts 95% of the organizations and costs them millions of dollars annually. If managed well, it can significantly improve business productivity. The traditional information extraction techniques are limited in their functionality, but AI-based techniques can provide a better solution. A thorough investigation of AI-based techniques for automatic information extraction from unstructured documents is missing in the literature. The purpose of this Systematic Literature Review (SLR) is to recognize, and analyze research on the techniques used for automatic information extraction from unstructured documents and to provide directions for future research. The SLR guidelines proposed by Kitchenham and Charters were adhered to conduct a literature search on various databases between 2010 and 2020. We found that: 1. The existing information extraction techniques are template-based or rule-based, 2. The existing methods lack the capability to tackle complex document layouts in real-time situations such as invoices and purchase orders, 3. The datasets available publicly are task-specific and of low quality. Hence, there is a need to develop a new dataset that reflects real-world problems. Our SLR discovered that AI-based approaches have a strong potential to extract useful information from unstructured documents automatically. However, they face certain challenges in processing multiple layouts of the unstructured documents. Our SLR brings out conceptualization of a framework for construction of high-quality unstructured documents dataset with strong data validation techniques for automated information extraction. Our SLR also reveals a need for a close association between the businesses and researchers to handle various challenges of the unstructured data analysis.},
  keywords={Organizations;Information retrieval;Optical character recognition software;Data mining;Automation;Standards organizations;Bibliographies;Artificial Intelligence (AI);document analysis;information extraction;named entity recognition (NER);optical character recognition (OCR);robotics process automation (RPA);unstructured data},
  doi={10.1109/ACCESS.2021.3072900},
  ISSN={2169-3536},
  month={},}@ARTICLE{9069219,
  author={Guamán, Danny S. and Alamo, Jose M. Del and Caiza, Julio C.},
  journal={IEEE Access},
  title={A Systematic Mapping Study on Software Quality Control Techniques for Assessing Privacy in Information Systems},
  year={2020},
  volume={8},
  number={},
  pages={74808-74833},
  abstract={Software Quality Control (SQC) techniques are widely used throughout the software development process with the objective of assessing and detecting anomalies that affect the quality of an information system. Privacy is one quality attribute of software systems for which several SQC techniques have been proposed in recent years. However, research has been carried out from different perspectives and, consequently, it has led to a growing body of knowledge scattered across different domains. To bridge this gap, we have carried out a systematic mapping study to provide practitioners and researchers with an overview of the state-of-the-art techniques to carry out software quality control of information systems focusing on aspects of privacy. Our results show a steady growth in the research efforts in this field. The European General Data Protection Regulation seems to have a significant influence on this growth, since 37% of techniques that focus on assessing compliance derive their assessment criteria from this legal framework. The maturity of the techniques varies between the type of technique: Formal verification techniques exhibit the lowest level of maturity while the combination of techniques has demonstrated its successful application in real-world scenarios. The latter seems a promising avenue of research as it provides better results in terms of coverage, precision and effectiveness than the application of individual, isolated techniques. In this paper, we describe the existing SQC techniques focusing on privacy and provide a suitable basis for identifying future research directions.},
  keywords={Privacy;Software quality;Software systems;Data privacy;Monitoring;Systematics;Data protection;information systems;mapping;privacy;software quality control;software engineering;systematic study},
  doi={10.1109/ACCESS.2020.2988408},
  ISSN={2169-3536},
  month={},}@ARTICLE{6916497,
  author={Kawamura, Takahiro and Ohsuga, Akihiko},
  journal={IEEE Intelligent Systems},
  title={Applying Linked Open Data to Green Design},
  year={2015},
  volume={30},
  number={1},
  pages={28-35},
  abstract={Increasing levels of environmental consciousness have spurred interest in urban agriculture and greening. However, plant cultivation in a limited urban space isn't necessarily a simple matter. On one hand, a plant that's not a hardy species might die; on the other, a plant that overgrows could disrupt the balance of vegetation in the surrounding environment. The authors propose an Android application called Green-Thumb Camera, which allows users to search for plants that fit particular environmental conditions from the linked open data (LOD) cloud. Queries are made using smartphone sensor information, and the application then uses augmented reality to overlay an image of the adult plant in the specified space. The authors describe the LOD content generation method and details of the application and then evaluate the accuracy of the LOD data and the application's usability.},
  keywords={Semantic Web;Green computing;Semantics;Sensors;Web services;Web pages;Augmented reality;intelligent Web services;Semantic Web;linked data;green computing;augmented reality;sensor Web;intelligent systems},
  doi={10.1109/MIS.2014.74},
  ISSN={1941-1294},
  month={Jan},}@ARTICLE{9238399,
  author={Caillou, Philippe and Renault, Jonas and Fekete, Jean-Daniel and Letournel, Anne-Catherine and Sebag, Michèle},
  journal={IEEE Computer Graphics and Applications},
  title={Cartolabe: A Web-Based Scalable Visualization of Large Document Collections},
  year={2021},
  volume={41},
  number={2},
  pages={76-88},
  abstract={We describe Cartolabe, a web-based multiscale system for visualizing and exploring large textual corpora based on topics, introducing a novel mechanism for the progressive visualization of filtering queries. Initially designed to represent and navigate through scientific publications in different disciplines, Cartolabe has evolved to become a generic framework and accommodate various corpora, ranging from Wikipedia (4.5M entries) to the French National Debate (4.3M entries). Cartolabe is made of two modules: The first relies on natural language processing methods, converting a corpus and its entities (documents, authors, and concepts) into high-dimensional vectors, computing their projection on the two-dimensional plane, and extracting meaningful labels for regions of the plane. The second module is a web-based visualization, displaying tiles computed from the multidimensional projection of the corpus using the Umap projection method. This visualization module aims at enabling users with no expertise in visualization and data analysis to get an overview of their corpus, and to interact with it: exploring, querying, filtering, panning, and zooming on regions of semantic interest. Three use cases are discussed to illustrate Cartolabe’s versatility and ability to bring large-scale textual corpus visualization and exploration to a wide audience.},
  keywords={Data visualization;Two dimensional displays;Organizations;Task analysis;Filtering;Visualization;Natural language processing;Web services},
  doi={10.1109/MCG.2020.3033401},
  ISSN={1558-1756},
  month={March},}@ARTICLE{745720,
  author={Schatz, B. and Mischo, W. and Cole, T. and Bishop, A. and Harum, S. and Johnson, E. and Neumann, L. and Hsinchun Chen and Dorbin Ng},
  journal={Computer},
  title={Federated search of scientific literature},
  year={1999},
  volume={32},
  number={2},
  pages={51-59},
  abstract={The Internet of the 21st Century will radically transform how we interact with knowledge. The rise of the World Wide Web and the information infrastructure have rapidly developed the technologies of collections for independent communities. In the future, online information will be dominated by small collections. The information infrastructure must similarly be radically different to support indexing of community collections and searching across such small collections. Users will consider themselves to be navigating in the Interspace, across logical spaces of semantic indexes, rather than in the Internet, across physical networks of computer servers. The digital libraries initiative (DLI) project at the University of Illinois at Urbana-Champaign (UIUC) was one of six sponsored by the NSF, DARPA, and NASA from 1994 through 1998. The goal was to develop widely usable Web technology to effectively search technical documents on the Internet. This article details their efforts.},
  keywords={Metasearch;Space technology;Internet;Web sites;Indexing;Navigation;IP networks;Computer networks;Physics computing;Network servers},
  doi={10.1109/2.745720},
  ISSN={1558-0814},
  month={Feb},}@INPROCEEDINGS{6867547,
  author={Jacob, Ferosh and Javed, Faizan and Zhao, Meng and Mcnair, Matt},
  booktitle={2014 International Conference on Collaboration Technologies and Systems (CTS)},
  title={sCooL: A system for academic institution name normalization},
  year={2014},
  volume={},
  number={},
  pages={86-93},
  abstract={Named Entity Normalization involves normalizing recognized entities to a concrete, unambiguous real world entity. Within the purview of the online job posting domain, academic institution name normalization provides a beneficial opportunity for CareerBuilder (CB). Accurate and detailed normalization of academic institutions are important to perform sophisticated labor market dynamics analysis. In this paper we present and discuss the design and the implementation of sCooL, an academic institution name normalization system designed to supplant the existing manually maintained mapping system at CB. We also discuss the specific challenges that led to the design of sCooL. sCooL leverages Wikipedia to create academic institution name mappings from a school database which is created from job applicant resumes posted on our website. The mappings created are utilized to build a database which is then used for normalization. sCooL provides the flexibility to integrate mappings collected from different curated and non-curated sources. The system is able to identify malformed data and K-12 schools from universities and colleges. We conduct an extensive comparative evaluation of the semi-automated sCooL system against the existing manual mapping implementation and show that sCooL provides better coverage with improved accuracy.},
  keywords={Educational institutions;Databases;Encyclopedias;Electronic publishing;Internet;Cities and towns;School Normalization;Name Entity Recognition;Lucene;Wikipedia},
  doi={10.1109/CTS.2014.6867547},
  ISSN={},
  month={May},}@ARTICLE{9741836,
  author={Maphosa, Mfowabo and Doorsamy, Wesley and Paul, Babu Sena},
  journal={IEEE Transactions on Education},
  title={Factors Influencing Students’ Choice of and Success in STEM: A Bibliometric Analysis and Topic Modeling Approach},
  year={2022},
  volume={65},
  number={4},
  pages={657-669},
  abstract={Contribution: This article lends empirical evidence to this research area of factors influencing students’ choice of and success in science, technology, engineering, and mathematics (STEM). Background: Understanding these factors is crucial as it informs recruitment and support interventions provided to students and constitutes a premise to improving graduation rates. The social cognitive career theory (SCCT) was used as a theoretical framework to provide insight regarding factors influencing students’ choice of qualifications. Research Questions: What is the state of research on the factors influencing students’ choice of and success in STEM programmes? Which of these factors have interested most researchers? What research themes are covered in articles investigating these factors? Methodology: This study followed the general bibliometric analysis workflow—study design, data collection, data analysis, data visualization, and interpretation. Data collection followed the preferred reporting items for systematic review and metaanalysis (PRISMA) guidelines. From an initial set of 408 articles, 179 related to the theme and were published in the Web of Science between 2004 and 2020. These articles were analyzed using the standard bibliometric metrics. Findings: Findings indicate that this research field is still growing. Thirty-two factors were identified and rated based using an objective assessment criterion. In addition, a classification of the factors is presented based on the SCCT. This study provides a theoretical reference for improving success rates for STEM qualifications and better understanding the theme. The study proposes a research agenda of what future research in the field should focus on, based on current gaps.},
  keywords={STEM;Engineering profession;Education;Analytical models;Citation analysis;Data visualization;Systematics;Choice of qualifications;social cognitive career theory (SCCT);science;technology;engineering;and mathematics (STEM) students;student retention},
  doi={10.1109/TE.2022.3160935},
  ISSN={1557-9638},
  month={Nov},}@INPROCEEDINGS{6689560,
  author={Benzarti, Sabrine and Ben Abdessalem Karaa, Wahiba},
  booktitle={2013 International Conference on Control, Decision and Information Technologies (CoDIT)},
  title={AnnoPharma: Detection of substances responsible of ADR by annotating and extracting information from MEDLINE abstracts},
  year={2013},
  volume={},
  number={},
  pages={294-299},
  abstract={Several studies have been conducted in different areas in order to annotate the medical data and extract knowledge, related to diseases, amino acid, genes, proteins, etc. Our research concerns the field of pharmacology which is wealthy but rarely studied. However it is a very interesting field and has great value as it is directly connected to human life. The aim of our research is to annotate MEDLINE abstracts belonging to the domain of pharmacology in order to extract the substances responsible of Adverse Reactions (ADRs) on the human body organs. To validate our approach, we implemented a prototype of a system (AnnoPharma). For this purpose we designed a new approach, based on semantic annotation, showing heartening performance.},
  keywords={Drugs;Ontologies;Abstracts;Semantics;Biological systems;Data mining;Information retrieval;Annotation;MEDLINE;pharmacology;ADR;UMLS;OWL;RDF;Information Extraction},
  doi={10.1109/CoDIT.2013.6689560},
  ISSN={},
  month={May},}@INBOOK{9968187,
  author={Tuffery, Stephane},
  booktitle={Deep Learning: From Big Data to Artificial Intelligence with R},
  title={Handwriting Recognition},
  year={2023},
  volume={},
  number={},
  pages={237-268},
  abstract={In this chapter, the authors implement the machine learning methods such as penalized multinomial logistic regression, Extra&#x2010;Trees, linear and quadratic discriminant analysis. They apply these methods to the Modified National Institute of Standards and Technology dataset and compare their performance in handwritten digit recognition. The applications of character recognition are numerous: postal address interpretation, bank check processing, use of touch screens, car license plate recognition, etc. <i>Ad hoc</i> data pre&#x2010;processing can provide a significant gain by taking advantage of features present at several points in a person's handwriting. The error rate of the quadratic discriminant analysis model obtained, 20.63%, is much higher than that of the linear discriminant analysis, which can be explained by the complexity of the model to be fitted, with many more coefficients to determine. Extremely randomized Trees, or Extra&#x2010;Trees, are a variant of random forests. XGBoost is one of the most used machine learning algorithms in Kaggle competitions.},
  keywords={Character recognition;Handwriting recognition;NIST;Training;Touch sensitive screens;Surface treatment;Standards},
  doi={10.1002/9781119845041.ch6},
  ISSN={},
  publisher={Wiley},
  isbn={9781119845027},
  url={https://ieeexplore.ieee.org/document/9968187},}@INPROCEEDINGS{7000093,
  author={Ouhbi, Sofia and Idri, Ali and Fernandez Aleman, Jose Luis and Toval, Ambrosio},
  booktitle={2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement},
  title={Evaluating Software Product Quality: A Systematic Mapping Study},
  year={2014},
  volume={},
  number={},
  pages={141-151},
  abstract={Evaluating software product quality (SPQ) is an important task to ensure the quality of software products. In this paper a systematic mapping study was performed to summarize the existing SPQ evaluation (SPQE) approaches in literature and to classify the selected studies according to seven classification criteria: SPQE approaches, research types, empirical types, data sets used in the empirical evaluation of these studies, artifacts, SQ models, and SQ characteristics. Publication channels and trends were also identified. 57 papers were selected. The results show that the main publication sources of the papers identified were journals. Data mining techniques are the most frequently approaches reported in literature. Solution proposals were the main research type identified. The majority of the selected papers were history-based evaluations using existing data, which were mainly obtained from open source software projects and domain specific projects. Source code was the main artifacts used by SPQE approaches. Well-known SQ models were mentioned by half of the selected papers and reliability is the SQ characteristic through which SPQE was mainly achieved. SPQE-related subjects seem to attract more interest from researchers since the past years.},
  keywords={ISO standards;IEC standards;Conferences;Erbium;Digital signal processing;Software;Data mining},
  doi={10.1109/IWSM.Mensura.2014.30},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{5222903,
  author={Wu, Hao and Pei, Yijian and Yu, Jiang},
  booktitle={2009 Eighth IEEE/ACIS International Conference on Computer and Information Science},
  title={Hidden Topic Analysis Based Formal Framework for Finding Experts in Metadata Corpus},
  year={2009},
  volume={},
  number={},
  pages={369-374},
  abstract={As a retrieval task, expert finding has recently attracted much attention. And various methods have been proposed to rank expert candidates against topical query. The most efficient approach is document-based method that treats supporting documents as a ldquobridgerdquo and ranks the candidates based on the co-occurrences of topic and candidate mentions in the supporting documents.However, such kind of methods models relevance between query and candidates on the much lower and hence less ambiguous level. It lacks of the capability to capture the hidden semantic association between queries and candidates. In this paper, we propose a hidden topic analysis based approach to estimate the relevance between query and candidates. It models query and supporting document as a word-topic-document association instead of the word-document association in language model. In addition, the prior knowledge of supporting document is considered to favor expert ranking. The empirical results on metadata corpus have demonstrated the model can effectively catch the semantic association between queries and candidates, thus improves the performance of expert finding.},
  keywords={Information science;Bridges;Information analysis;Lakes;Information retrieval;Resource description framework;Computer science;Software libraries;Internet;Linear discriminant analysis},
  doi={10.1109/ICIS.2009.131},
  ISSN={},
  month={June},}@INPROCEEDINGS{8931826,
  author={Al-Jarrah, Heba and Al-Asa'd, Muntaha and Al-Zboon, Sa'ad A. and Tawalbeh, Saja Khaled and Hammad, Mahmoud M. and AL-Smadi, Mohammad},
  booktitle={2019 Sixth International Conference on Social Networks Analysis, Management and Security (SNAMS)},
  title={Resolving Conflict of Interests and Recommending Expert Reviewers for Academic Publications Using Linked Open Data},
  year={2019},
  volume={},
  number={},
  pages={91-98},
  abstract={Scholarly peer review is a process of evaluating the suitability of a research work for publication judged by qualified researchers. A professional peer review process ensures the quality of the produced scientific research work. However, there are two main challenges to achieve professional peer review: (1) selecting reviewers with similar competences as the authors (peers) of a submitted research work and (2) resolving any Conflict of Interest (Col) between reviewers and authors. Currently, to solve the first challenge, editors and conferences organizers select reviewers manually. Similarly, the current solution of the second challenge is that authors and reviewers are asked to manually declare any CoI. Such a manual solution is error-prone, waste time, and tedious for reviewers, authors, editors, and organizers. To address the aforementioned two challenges, we have developed a novel recommender system that (1) suggests expert reviewers and (2) resolves any CoI between the recommended reviewers and the author(s) of a submitted paper. To develop our recommender system, we utilized the DBLP citation network database represented as Linked Open Data. To select candidate reviewers who are expert in the topic of a submitted paper without CoIs, we use Latent Dirichlet Allocation (LDA) topic modeling to extract the topics researchers are working on and the topics of a submitted paper, then our system executes a SPARQL query that returns the best candidate reviewers. Finally, our system executes another SPARQL query that detects any CoIs between the candidate reviewers and the authors of a submitted paper and hence excludes them. Our experimental evaluations corroborate the ability or our system to recommend expert reviewers without CoIs.},
  keywords={Recommender systems;Resource description framework;Semantics;Social networking (online);Feature extraction;Collaboration;Conflict of Interests (CoIs);Knowledge Representation;Linked Open Data (LOD);SPARQL;DBLP;OWL;RDF;Latent Dirichlet Allocation (LDA);Apache Jena Fuseki},
  doi={10.1109/SNAMS.2019.8931826},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{5607427,
  author={Tang, Xiaoyu and Zeng, Qingtian and Cui, Tingting and Wu, Zeze},
  booktitle={2010 IEEE 2nd Symposium on Web Society},
  title={Regular expression-based reference metadata extraction from the web},
  year={2010},
  volume={},
  number={},
  pages={346-350},
  abstract={Accurate reference metadata extraction becomes an intriguing task to researchers who want to collect data of scientific publications. In this paper, we introduce an approach to extracting the reference metadata based on regular expressions. A prototype system named “Goldrusher” is created which automatically extracts data from the website of Association for Computing Machinery (ACM). The experimental results show that, by using our regular expression-based method, we can effectively extract author names, article titles, journal titles, DIOs, etc.},
  keywords={Web pages;Data mining;Books;HTML;Crawlers;Machinery;Libraries},
  doi={10.1109/SWS.2010.5607427},
  ISSN={2158-6993},
  month={Aug},}@INPROCEEDINGS{10505528,
  author={Ning, Dong},
  booktitle={2023 13th International Conference on Information Technology in Medicine and Education (ITME)},
  title={Research on Library Smart Reference Services Model Driven by Associated Data},
  year={2023},
  volume={},
  number={},
  pages={20-26},
  abstract={The widespread application of data and information technology has prompted the transformation and upgrading of library reference service models, therefore to construct a smart reference service model from associated data-driven perspective helps promote the implementation of smart services in libraries. On the basis of elaborating the correlation of associated data technology and smart reference services, the study constructs a smart reference services model that includes intelligent perception of user demand, smart Q&A services, and user intelligent push services. Finally, the study proposes implementation strategies and suggestions for the smart reference service model in order to provide inspiration and reference for the transformation and upgrading of library reference services in big data environment.},
  keywords={Analytical models;Correlation;Education;Big Data;Data models;Libraries;Information technology;associated data;smart reference service;service model},
  doi={10.1109/ITME60234.2023.00016},
  ISSN={2474-3828},
  month={Nov},}@ARTICLE{745723,
  author={Witten, I.H. and McNab, R.J. and Jones, S. and Apperley, M. and Bainbridge, D. and Cunningham, S.J.},
  journal={Computer},
  title={Managing complexity in a distributed digital library},
  year={1999},
  volume={32},
  number={2},
  pages={74-79},
  abstract={As the capabilities of distributed digital libraries increase, managing organizational and software complexity becomes a key issue. How can collections and indexes be updated without impacting queries currently in progress? How can the system handle several user interface clients for the same collections? Computer science professors from the University of Waikato have developed a software structure that successfully manages this complexity in the New Zealand Digital Library. This digital library has been a success in managing organizational and software complexity, The researchers' primary goal has been to minimize the effort required to keep the system operational and yet continue to expand its offering.},
  keywords={Software libraries;Natural languages;Displays;Search engines;Optical character recognition software;Communication system software;Software development management;User interfaces;Protocols;World Wide Web},
  doi={10.1109/2.745723},
  ISSN={1558-0814},
  month={Feb},}@INPROCEEDINGS{9022826,
  author={Aung, Thet Thet and Nyunt, Thi Thi Soe},
  booktitle={2020 IEEE Conference on Computer Applications(ICCA)},
  title={Community Detection in Scientific Co-Authorship Networks using Neo4j},
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Community structure in scientific collaboration network has become an important research area. Co-author of a paper can be thought of as a collaborative document between more than one authors. Community detection in co-authorship network reveals characteristic patterns of scientific collaboration in computer science research and help to understand the identity-organization of the author community. Louvain algorithm is a simple, easy to implement and efficient to recognize community in huge networks. In this paper, it is used to examine the structure of community in Computer University's coauthor network in Myanmar. Neo4j is also used to visualize the co-authorship network analysis results. Modularity is used to measure the quality of the cluster structure found by community discovery algorithms. In experiment, Louvain algorithm gives more effective qualitative community structures than other algorithms in co-authorship network.},
  keywords={Clustering algorithms;Detection algorithms;Collaboration;Image edge detection;Partitioning algorithms;Libraries;Software algorithms;co-authorship network;community detection;modularity;Neo4j},
  doi={10.1109/ICCA49400.2020.9022826},
  ISSN={},
  month={Feb},}@ARTICLE{10273422,
  author={Sood, Sandeep Kumar and Monika},
  journal={IEEE Transactions on Engineering Management},
  title={Bibliometric Analysis and Visualization of Quantum Engineering Technology},
  year={2024},
  volume={71},
  number={},
  pages={10217-10231},
  abstract={Quantum computing is a revolutionary field of research in science and technology, which provides sophisticated processing capacity and computation in a finite time frame based on quantum mechanics. It has the great potential to offer more effective and faster solutions to overcome current processing constraints. Therefore, to address the global significance, relevance, and exponential growth, it is imperative to conduct a thorough investigation and promote advancements through rigorous scientometric analysis. The current article presents the knowledge mapping of rapidly expanding quantum computing research using the CiteSpace visualization tool. The foundation of research is the scientific literature on quantum computing extracted from the Scopus database, covering the period of 2008–2022. It provides publication patterns, citation structure, and in-depth keyword co-occurrence network analysis insights to extract emerging research areas. It illustrates a variety of empirical approaches to discover the status quo and evolution. The findings reveal that quantum algorithms serve as the fundamental basis for all research fields, and quantum machine learning is the most active research area nowadays.},
  keywords={Quantum computing;Computers;Bibliometrics;Computer science;Qubit;Network analyzers;Databases;CiteSpace;quantum algorithms (QAs);quantum computing (QC);quantum machine learning (QML);scientometrics},
  doi={10.1109/TEM.2023.3313984},
  ISSN={1558-0040},
  month={},}@INPROCEEDINGS{6102477,
  author={Lemieux, Victoria and Endicott-Popovsky, Barbara and Eckler, Karl and Dang, Thomas and Jansen, Adam},
  booktitle={2011 IEEE Conference on Visual Analytics Science and Technology (VAST)},
  title={Visualizing an information assurance risk taxonomy},
  year={2011},
  volume={},
  number={},
  pages={287-288},
  abstract={The researchers explore the intersections between Information Assurance and Risk using visual analysis of text mining operations. The methodological approach involves searching for and extracting for analysis those abstracts and keywords groupings that relate to risk within a defined subset of scientific research journals. This analysis is conducted through a triangulated study incorporating visualizations produced using both Starlight and In-Spire visual analysis software. The results are definitional, showing current attitudes within the Information Assurance research community towards risk management strategies, while simultaneously demonstrating the value of visual analysis processes when engaging in sense making of a large body of knowledge.},
  keywords={Visualization;Educational institutions;Databases;Abstracts;Availability;Security;Taxonomy;Risk;Risk Management;Information Assurance;Information Security;Visual Analysis},
  doi={10.1109/VAST.2011.6102477},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10609054,
  author={Wang, Xianghan and Long, Sheng and Zeng, Li and Chen, Chao and Yishan, L.},
  booktitle={2024 International Symposium on Power Electronics, Electrical Drives, Automation and Motion (SPEEDAM)},
  title={Mapping the Evolution and Future Trajectories of Network Mining: A Scientometric Analysis (2004–2023)},
  year={2024},
  volume={},
  number={},
  pages={468-473},
  abstract={A Network Mining, a pivotal element of business intelligence, has been the subject of extensive study by both academic institutions and industry for an extended period. This paper presents a scientometric analysis spanning two decades to shed light on the evolving landscapes, focal points of research, and novel trends within this domain. The analysis utilized a dataset curated from the Web of Science, encompassing the years 2004 to 2023. This study goes beyond elementary scientific output assessments, employing advanced scientometric tools like CiteSpace, VOSViewer, and Bibliometrix to dissect the intellectual structure of Network Mining. The findings indicate a substantial increase in Network Mining research over the past twenty years, with a corpus of 2,110 papers emanating from 77 countries/territories. The United States, China, India, Japan, and France emerge as the top five most prolific contributors. The research domain encompasses 2,028 institutes, with the University of Illinois, Carnegie Mellon University, Chinese Academy of Sciences, Tsinghua University, and Arizona State University being the most influential. Furthermore, keywords exhibiting the most significant citation bursts, such as Social Network Mining, Anomaly Detection, Task Analysis, Network Embedding, Network Representation Learning, and Graph Neural Networks, were identified, signifying the cutting-edge directions in Network Mining research. The insights provided in this paper aim to bolster ongoing research endeavors in Network Mining, serving as a beacon for future scholarly exploration.},
  keywords={Representation learning;Industries;Social networking (online);Bibliometrics;Dynamics;Market research;Power electronics;Scientometric;Citespace;Network Mining},
  doi={10.1109/SPEEDAM61530.2024.10609054},
  ISSN={2835-8457},
  month={June},}@ARTICLE{9627684,
  author={Frisoni, Giacomo and Moro, Gianluca and Carbonaro, Antonella},
  journal={IEEE Access},
  title={A Survey on Event Extraction for Natural Language Understanding: Riding the Biomedical Literature Wave},
  year={2021},
  volume={9},
  number={},
  pages={160721-160757},
  abstract={Motivation: The scientific literature embeds an enormous amount of relational knowledge, encompassing interactions between biomedical entities, like proteins, drugs, and symptoms. To cope with the ever-increasing number of publications, researchers are experiencing a surge of interest in extracting valuable, structured, concise, and unambiguous information from plain texts. With the development of deep learning, the granularity of information extraction is evolving from entities and pairwise relations to events. Events can model complex interactions involving multiple participants having a specific semantic role, also handling nested and overlapping definitions. After being studied for years, automatic event extraction is on the road to significantly impact biology in a wide range of applications, from knowledge base enrichment to the formulation of new research hypotheses. Results: This paper provides a comprehensive and up-to-date survey on the link between event extraction and natural language understanding, focusing on the biomedical domain. First, we establish a flexible event definition, summarizing the terminological efforts conducted in various areas. Second, we present the event extraction task, the related challenges, and the available annotated corpora. Third, we deeply explore the most representative methods and present an analysis of the current state-of-the-art, accompanied by performance discussion. To help researchers navigate the avalanche of event extraction works, we provide a detailed taxonomy for classifying the contributions proposed by the community. Fourth, we compare solutions applied in biomedicine with those evaluated in other domains, identifying research opportunities and providing insights for strategies not yet explored. Finally, we discuss applications and our envisions about future perspectives, moving the needle on explainability and knowledge injection.},
  keywords={Proteins;Semantics;Biology;Task analysis;Protein engineering;Biological system modeling;Taxonomy;Biomedical text mining;event extraction;natural language understanding;semantic parsing},
  doi={10.1109/ACCESS.2021.3130956},
  ISSN={2169-3536},
  month={},}@ARTICLE{8492342,
  author={Barmpatsalou, Konstantia and Cruz, Tiago and Monteiro, Edmundo and Simoes, Paulo},
  journal={IEEE Access},
  title={Mobile Forensic Data Analysis: Suspicious Pattern Detection in Mobile Evidence},
  year={2018},
  volume={6},
  number={},
  pages={59705-59727},
  abstract={Culprits' identification by the means of suspicious pattern detection techniques from mobile device data is one of the most important aims of the mobile forensic data analysis. When criminal activities are related to entirely automated procedures such as malware propagation, predicting the corresponding behavior is a rather achievable task. However, when human behavior is involved, such as in cases of traditional crimes, prediction and detection become more compelling. This paper introduces a combined criminal profiling and suspicious pattern detection methodology for two criminal activities with moderate to the heavy involvement of mobile devices, cyberbullying and low-level drug dealing. Neural and Neurofuzzy techniques are applied on a hybrid original and simulated dataset. The respective performance results are measured and presented, the optimal technique is selected, and the scenarios are re-run on an actual dataset for additional testing and verification.},
  keywords={Forensics;Artificial neural networks;Data analysis;Mobile handsets;Performance evaluation;Fuzzy logic;Tools;Mobile forensics;evidence data analysis;criminal profiling;behavioral evidence analysis;neural networks;ANFIS},
  doi={10.1109/ACCESS.2018.2875068},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{1204839,
  author={Cunningham, S.J. and Reeves, N. and Britland, M.},
  booktitle={2003 Joint Conference on Digital Libraries, 2003. Proceedings.},
  title={An ethnographic study of music information seeking: implications for the design of a music digital library},
  year={2003},
  volume={},
  number={},
  pages={5-17},
  abstract={At present, music digital library systems are being developed based on anecdotal evidence of user needs, intuitive feelings for user information seeking behavior, and a priori assumptions of typical usage scenarios. Emphasis has been placed on basic research into music document representation, efficient searching, and audio-based searching, rather than on exploring the music information needs or information behavior of a target user group. We focus on eliciting the 'native' music information strategies employed by people searching for popular music (that is, music sought for recreational or enjoyment purposes rather than to support a 'serious' or scientific exploration of some aspect of music). To this end, we conducted an ethnographic study of the searching/browsing techniques employed by people in the researchers' local communities, as they use two common sources of music: the public library and music stores. We argue that the insights provided by this type of study can inform the development of searching/browsing support for music digital libraries.},
  keywords={Multiple signal classification;Software libraries;Music information retrieval;Computer science;Multimedia systems;Multimedia computing;Indexing;Usability;Grounding},
  doi={10.1109/JCDL.2003.1204839},
  ISSN={},
  month={May},}@BOOK{10614669,
  author={Ding, Jeffrey},
  title={Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={A novel theory of how technological revolutions affect the rise and fall of great powersWhen scholars and policymakers consider how technological advances affect the rise and fall of great powers, they draw on theories that center the moment of innovation—the eureka moment that sparks astonishing technological feats. In this book, Jeffrey Ding offers a different explanation of how technological revolutions affect competition among great powers. Rather than focusing on which state first introduced major innovations, he investigates why some states were more successful than others at adapting and embracing new technologies at scale. Drawing on historical case studies of past industrial revolutions as well as statistical analysis, Ding develops a theory that emphasizes institutional adaptations oriented around diffusing technological advances throughout the entire economy.Examining Britain’s rise to preeminence in the First Industrial Revolution, America and Germany’s overtaking of Britain in the Second Industrial Revolution, and Japan’s challenge to America’s technological dominance in the Third Industrial Revolution (also known as the “information revolution”), Ding illuminates the pathway by which these technological revolutions influenced the global distribution of power and explores the generalizability of his theory beyond the given set of great powers. His findings bear directly on current concerns about how emerging technologies such as AI could influence the US-China power balance.},
  keywords={artificial intelligence;China;economic productivity;engineering;general-purpose technology;great powers;human capital;industrial revolutions;Jeffrey Ding;leading sectors;political economy;rising powers;Technological;technological competition;Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition;United States;Power;Economic;Sectors;Industrial;Growth;Innovation;Diffusion;Productivity;Mechanism;Industries;Technologies;Gpts;Economy;Revolution;GPT diffusion;Transition;Period;Trajectory;Technology;Power transition;Infrastructure;Institutional;Chemical;Skill infrastructure;Leadership;Economic power;Machine;Technological change;Century;Data;Country;Steam;GPT skill infrastructure;LS mechanism;Analysis;Education;Iron;Computerization;Industrial revolution;Breakthroughs;Technical;Military;Economic power transition;Production;Software;Productivity growth;Machine tools;Steam engine;Scholars;GPT mechanism;Engineers;Cotton;Software engineering;Research},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691260372},
  url={https://ieeexplore.ieee.org/document/10614669},}@INPROCEEDINGS{762607,
  author={Kambayashi, Y. and Nakatsu, N. and Yajima, S.},
  booktitle={COMPSAC 79. Proceedings. Computer Software and The IEEE Computer Society's Third International Applications Conference, 1979.},
  title={Hierarchical string pattern matching using dynamic pattern matching machines},
  year={1979},
  volume={},
  number={},
  pages={813-818},
  abstract={},
  keywords={Pattern matching;Dictionaries;Space technology;Information science;Sufficient conditions;Testing;Computer science;Educational technology;Computer science education},
  doi={10.1109/CMPSAC.1979.762607},
  ISSN={},
  month={Nov},}@ARTICLE{976919,
  author={Schatz, B.R.},
  journal={Computer},
  title={The Interspace: concept navigation across distributed communities},
  year={2002},
  volume={35},
  number={1},
  pages={54-62},
  abstract={Within the next decade, computing technology will transform the Internet into the Interspace, an information infrastructure that supports semantic indexing and concept navigation across widely distributed community repositories. With the Interspace, the global information infrastructure will, for the first time, directly support interaction with abstraction. This infrastructure uses technologies that go beyond searching individual repositories to analyze and correlate knowledge across multiple sources and subjects. The Interspace will offer distributed services to transfer concepts across domains, just as Arpanet used distributed services to transfer files across machines and the Internet uses distributed services to transfer objects across repositories. The Community Architectures for Network Information Systems Laboratory has developed a working Interspace prototype that uses scalable technologies for concept extraction and navigation. They have successfully tested these technologies, which compute contextual frequency of document phrases within a community repository, on discipline-scale, real-world collections.},
  keywords={Navigation;Indexing;Web and internet services;Computer architecture;Information systems;Laboratories;Prototypes;Data mining;Testing;Frequency},
  doi={10.1109/2.976919},
  ISSN={1558-0814},
  month={Jan},}@ARTICLE{6796168,
  author={Heinz, Jeffrey},
  journal={Linguistic Inquiry},
  title={Learning Long-Distance Phonotactics},
  year={2010},
  volume={41},
  number={4},
  pages={623-661},
  abstract={This article shows that specific properties of long-distance phonotactic patterns derived from consonantal harmony patterns (Hansson 2001, Rose and Walker 2004) follow from a learner that generalizes only on the basis of the order of sounds, not the distance between them. The proposed learner is simple, efficient, and provably correct, and does not require an a priori notion of tier or projection (contra the model in Hayes and Wilson 2008); nor does it rely on the additional structure provided by Optimality Theory grammars (Prince and Smolensky 1993, 2004) or grammars in the principles-and-parameters framework (Chomsky 1981, Dresher and Kaye 1990, Gibson and Wexler 1994). Not only does the noncounting nature of nonlocal dependencies automatically follow from the way the learner generalizes, it also explains the absence of blocking patterns from the typology. Finally, the learner lends support to the idea that long-distance phonotactic patterns are phenomenologically distinct from spreading patterns, contra the hypothesis of Strict Locality (Gafos 1999, et seq.).},
  keywords={phonotactic learning;long-distance agreement;precedence;grammatical inference;formal language theory},
  doi={10.1162/LING_a_00015},
  ISSN={0024-3892},
  month={Oct},}@BOOK{8187070,
  author={Pang, Bo and Lee, Lillian},
  title={Opinion Mining and Sentiment Analysis},
  year={2008},
  volume={},
  number={},
  pages={},
  abstract={An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object. Opinion Mining and Sentiment Analysis covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. The focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. The survey includes an enumeration of the various applications, a look at general challenges and discusses categorization, extraction and summarization. Finally, it moves beyond just the technical issues, devoting significant attention to the broader implications that the development of opinion-oriented information-access services have: questions of privacy, vulnerability to manipulation, and whether or not reviews can have measurable economic impact. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided. Opinion Mining and Sentiment Analysis is the first such comprehensive survey of this vibrant and important research area and will be of interest to anyone with an interest in opinion-oriented information-seeking systems.},
  keywords={ARTIFICIAL INTELLIGENCE / MACHINE LEARNING},
  doi={10.1561/1500000011},
  ISSN={},
  publisher={now},
  isbn={9781601981516},
  url={https://ieeexplore.ieee.org/document/8187070},}@ARTICLE{9732914,
  author={},
  journal={IEEE P1547.2/D6.2, March 2022},
  title={IEEE Draft Application Guide for IEEE Std 1547(TM), IEEE Standard for Interconnecting Distributed Resources with Electric Power Systems},
  year={2022},
  volume={},
  number={},
  pages={1-316},
  abstract={Technical background and application details to support understanding of IEEE Std 1547™- 2018 are provided. The guide facilitates the use of IEEE Std 1547-2018 by characterizing various forms of distributed energy resource (DER) technologies and their associated interconnection issues. It provides background and rationale of the technical requirements of IEEE Std 1547-2018. It also provides tips, techniques, and common practices to address issues related to DER project implementation. This guide is intended for use by engineers, engineering consultants, and knowledgeable individuals in the field of DERs. The IEEE 1547 series of standards is cited in the Federal Energy Policy Act of 2005, and this guide is one document in the IEEE 1547 series.},
  keywords={IEEE Standards;Power distribution;Distributed power generation;Fault diagnosis;Storage management;Flicker;Energy storage;Resource management;amendment;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547™;IEEE 1547.2™;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={March},}@ARTICLE{10227850,
  author={},
  journal={IEEE P1547.2/D6.5, August 2023},
  title={IEEE Approved Draft Application Guide for IEEE Std 1547™, IEEE Standard for Interconnecting Distributed Resources with Electric Power Systems},
  year={2023},
  volume={},
  number={},
  pages={1-322},
  abstract={Technical background and application details to support understanding of IEEE Std 1547™- 2018 are provided. The guide facilitates the use of IEEE Std 1547-2018 by characterizing various forms of distributed energy resource (DER) technologies and their associated interconnection issues. It provides background and rationale of the technical requirements of IEEE Std 1547-2018. It also provides tips, techniques, and common practices to address issues related to DER project implementation. This guide is intended for use by engineers, engineering consultants, and knowledgeable individuals in the field of DERs. The IEEE 1547 series of standards is cited in the Federal Energy Policy Act of 2005, and this guide is one document in the IEEE 1547 series.},
  keywords={IEEE Standards;Distributed power generation;Interconnected systems;Power electronics;amendment;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547™;IEEE 1547.2™;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={Sep.},}@ARTICLE{9984148,
  author={},
  journal={IEEE P1547.2/D6.3, November 2022},
  title={IEEE Draft Application Guide for IEEE Std 1547(TM), IEEE Standard for Interconnecting Distributed Resources with Electric Power Systems},
  year={2022},
  volume={},
  number={},
  pages={1-353},
  abstract={Technical background and application details to support understanding of IEEE Std 1547™- 2018 are provided. The guide facilitates the use of IEEE Std 1547-2018 by characterizing various forms of distributed energy resource (DER) technologies and their associated interconnection issues. It provides background and rationale of the technical requirements of IEEE Std 1547-2018. It also provides tips, techniques, and common practices to address issues related to DER project implementation. This guide is intended for use by engineers, engineering consultants, and knowledgeable individuals in the field of DERs. The IEEE 1547 series of standards is cited in the Federal Energy Policy Act of 2005, and this guide is one document in the IEEE 1547 series.},
  keywords={IEEE Standards;Power systems;Power grids;Energy consumption;Power distribution;Interoperability;Fuel cells;Flicker;Distributed power generation;amendment;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547™;IEEE 1547.2™;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={Dec},}@ARTICLE{10186328,
  author={},
  journal={IEEE P1547.2/D6.4, June 2023},
  title={IEEE Draft Application Guide for IEEE Std 1547(TM), IEEE Standard for Interconnecting Distributed Resources with Electric Power Systems},
  year={2023},
  volume={},
  number={},
  pages={1-324},
  abstract={Technical background and application details to support understanding of IEEE Std 1547™- 2018 are provided. The guide facilitates the use of IEEE Std 1547-2018 by characterizing various forms of distributed energy resource (DER) technologies and their associated interconnection issues. It provides background and rationale of the technical requirements of IEEE Std 1547-2018. It also provides tips, techniques, and common practices to address issues related to DER project implementation. This guide is intended for use by engineers, engineering consultants, and knowledgeable individuals in the field of DERs. The IEEE 1547 series of standards is cited in the Federal Energy Policy Act of 2005, and this guide is one document in the IEEE 1547 series.},
  keywords={IEEE Standards;Certification;Distributed power generation;Energy management;Power electronics;Interconnected systems;amendment;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547™;IEEE 1547.2™;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={July},}@INPROCEEDINGS{9457403,
  author={Hongqiu, Liu},
  booktitle={2021 2nd International Conference on Big Data and Informatization Education (ICBDIE)},
  title={Research on Library Personalized Service Mode Based on Big Data Application},
  year={2021},
  volume={},
  number={},
  pages={143-146},
  abstract={This paper explores the concept and technology of the big data, analyzes the big data environment in the library, and analyzes the source, classification and characteristics of library big data from the relationship between readers, collections and librarians. On this basis, this article constructs a library personalized information service model based on big data applications, and analyzes the current problems and countermeasures faced by libraries.},
  keywords={Technological innovation;Analytical models;Education;Big Data;Big Data applications;Libraries;Data models;big data;digital library;personalized service;information service},
  doi={10.1109/ICBDIE52740.2021.00040},
  ISSN={},
  month={April},}@ARTICLE{9137295,
  author={Aliagas, Carles and García-Famoso, Montse and Meseguer, Roc and Millán, Pere and Molina, Carlos},
  journal={IEEE Revista Iberoamericana de Tecnologias del Aprendizaje},
  title={A Low-Cost Multicomputer for Teaching Environments},
  year={2020},
  volume={15},
  number={3},
  pages={171-182},
  abstract={We propose a teaching resource that uses HardKernel boards to build an MPI server with 256 cores. Although this system has a relatively low performance, the aim is to provide access to hundreds of cores for carrying out scalability analyses, while obtaining a good trade-off between performance, price, and energy consumption. Here, we give details about the implementation of this system at both the hardware and software levels. We also explain how it was used to teach parallel programming in a university degree course, and discuss the teachers' and students' comments about using this new system.},
  keywords={Parallel programming;Graphics processing units;Education;Servers;Scalability;Hardware;Parallel machines;multicore processing;multiprocessor interconnection;low-power systems;educational activities},
  doi={10.1109/RITA.2020.3008098},
  ISSN={1932-8540},
  month={Aug},}@ARTICLE{10669285,
  author={},
  journal={IEEE P1012/D21, August 2024},
  title={IEEE Draft Standard for System, Software, and Hardware Verification and Validation},
  year={2024},
  volume={},
  number={},
  pages={1-324},
  abstract={Verification and validation (V&V) processes are used to determine whether the development products of a given activity conform to the requirements of that activity and whether the product satisfies its intended use and user needs. V&V life cycle process requirements are specified for different integrity levels. The scope of V&V processes encompasses systems, software, and hardware, and it includes their interfaces. This standard applies to systems, software, and hardware being developed, maintained, or reused [legacy, commercial off-the-shelf (COTS), non-developmental items]. The term software also includes firmware and microcode, and each of the terms system, software, and hardware includes related information or documentation. V&V processes include the analysis, evaluation, review, inspection, assessment, and testing of product},
  keywords={IEEE Standards;Artificial intelligence;Testing;Performance evaluation;Hazards;Formal verification;Field programmable gate arrays;Microprogramming;Hardware;Life cycle assessment;acceptance testing;architecture evaluation;adaptive;Agile;AI;artificial intelligence;component testing;concept documentation evaluation;COTS;criticality;criticality analysis;design evaluation;disposal plan evaluation;environmental verification and validation (V&V) factors;field programmable gate array;firmware;FPGA;hardware life cycle;hardware V&V;hardware verification and validation;hazard analysis;IEEE 1012;implementation evaluation;independent V&V;integration testing;integrity level;interface analysis;IV&V;machine learning;microcode;minimum V&V tasks;ML;nth of a kind;objective evidence;operating procedure evaluation;qualification testing;quality assurance;regression analysis;regression testing;requirements allocation analysis;requirements evaluation;reuse software;risk analysis;security analysis;software as a service;SaaS software life cycle;software quality assurance;software V&V;software verification and validation;source code documentation evaluation;source code evaluation;SQA;stakeholder needs and requirements evaluation;system element interaction analysis;system life cycle;system maintenance strategy assessment;system of interest;system requirements evaluation;system V&V;system verification and validation;testing;traceability analysis;V&V;V&V measures;validation;verification;vignette},
  doi={},
  ISSN={},
  month={Sep.},}@ARTICLE{10539079,
  author={},
  journal={IEEE P1012/D20, May 2024},
  title={IEEE Draft Standard for System, Software, and Hardware Verification and Validation},
  year={2024},
  volume={},
  number={},
  pages={1-327},
  abstract={Verification and validation (V&V) processes are used to determine whether the development products of a given activity conform to the requirements of that activity and whether the product satisfies its intended use and user needs. V&V life cycle process requirements are specified for different integrity levels. The scope of V&V processes encompasses systems, software, and hardware, and it includes their interfaces. This standard applies to systems, software, and hardware being developed, maintained, or reused [legacy, commercial off-the-shelf (COTS), non-developmental items]. The term software also includes firmware and microcode, and each of the terms system, software, and hardware includes related information or documentation. V&V processes include the analysis, evaluation, review, inspection, assessment, and testing of product},
  keywords={IEEE Standards;Hardware design languages;Software testing;Artificial intelligence;Software performance;Formal verification;Software reliability;Performance evaluation;acceptance testing;architecture evaluation;adaptive;Agile;AI;artificial intelligence;component testing;concept documentation evaluation;COTS;criticality;criticality analysis;design evaluation;disposal plan evaluation;environmental verification and validation (V&V) factors;field programmable gate array;firmware;FPGA;hardware life cycle;hardware V&V;hardware verification and validation;hazard analysis;IEEE 1012;implementation evaluation;independent V&V;integration testing;integrity level;interface analysis;IV&V;machine learning;microcode;minimum V&V tasks;ML;nth of a kind;objective evidence;operating procedure evaluation;qualification testing;quality assurance;regression analysis;regression testing;requirements allocation analysis;requirements evaluation;reuse software;risk analysis;security analysis;software as a service;SaaS software life cycle;software quality assurance;software V&V;software verification and validation;source code documentation evaluation;source code evaluation;SQA;stakeholder needs and requirements evaluation;system element interaction analysis;system life cycle;system maintenance strategy assessment;system of interest;system requirements evaluation;system V&V;system verification and validation;testing;traceability analysis;V&V;V&V measures;validation;verification;vignette},
  doi={},
  ISSN={},
  month={May},}@ARTICLE{10352402,
  author={},
  journal={IEEE Std 1547.3-2023 (Revision of IEEE Std 1547.3-2007)},
  title={IEEE Guide for Cybersecurity of Distributed Energy Resources Interconnected with Electric Power Systems},
  year={2023},
  volume={},
  number={},
  pages={1-183},
  abstract={Guidelines for cybersecurity of distributed energy resources (DER) interconnection with electric power systems (EPS) are provided in this guide.},
  keywords={IEEE Standards;Power systems;Distributed power generation;Computer security;Resource management;Energy management;Energy storage;Power distribution;Electrical products;Power grids;cybersecurity;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={10.1109/IEEESTD.2023.10352402},
  ISSN={},
  month={Dec},}@ARTICLE{9927298,
  author={},
  journal={IEEE P1547.3/3.08, October 2022},
  title={IEEE Draft Guide for Cybersecurity of Distributed Energy Resources Interconnected with Electric Power Systems},
  year={2022},
  volume={},
  number={},
  pages={1-175},
  abstract={Guidelines for cybersecurity of distributed energy resources (DER) interconnection with electric power systems (EPS) are provided in this guide.},
  keywords={IEEE Standards;Power systems;Distributed power generation;Computer security;Energy management;cybersecurity;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={Oct},}@ARTICLE{10068352,
  author={},
  journal={IEEE P1547.3/D3.12, March 2023},
  title={IEEE Approved Draft Guide for Cybersecurity of Distributed Energy Resources Interconnected with Electric Power Systems},
  year={2023},
  volume={},
  number={},
  pages={1-158},
  abstract={Guidelines for cybersecurity of distributed energy resources (DER) interconnection with electric power systems (EPS) are provided in this guide.},
  keywords={IEEE Standards;Computer security;Distributed power generation;Energy storage;Energy management;Interoperability;Power grids;cybersecurity;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={June},}@ARTICLE{9751215,
  author={},
  journal={P1547.3/D3.08, March 2022},
  title={IEEE Draft Guide for Cybersecurity of Distributed Energy Resources Interconnected with Electric Power Systems},
  year={2022},
  volume={},
  number={},
  pages={1-155},
  abstract={Guidelines for cybersecurity of distributed energy resources (DER) interconnection with electric power systems (EPS) are provided in this guide.},
  keywords={IEEE Standards;Distributed power generation;Energy management;Power systems;Computer security;Information exchange;Information security;cybersecurity;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={March},}@ARTICLE{10044655,
  author={},
  journal={IEEE P1547.3/D3.11, January 2023},
  title={IEEE Draft Guide for Cybersecurity of Distributed Energy Resources Interconnected with Electric Power Systems},
  year={2023},
  volume={},
  number={},
  pages={1-158},
  abstract={Guidelines for cybersecurity of distributed energy resources (DER) interconnection with electric power systems (EPS) are provided in this guide.},
  keywords={IEEE Standards;Computer security;Codes;Power systems;Energy storage;Diesel engines;Interoperability;Turbine engines;Power distribution;Fuel cells;Generators;cybersecurity;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={Feb},}@ARTICLE{9991109,
  author={},
  journal={IEEE P1547.3/D3.10, December 2022},
  title={IEEE Draft Guide for Cybersecurity of Distributed Energy Resources Interconnected with Electric Power Systems},
  year={2022},
  volume={},
  number={},
  pages={1-158},
  abstract={Guidelines for cybersecurity of distributed energy resources (DER) interconnection with electric power systems (EPS) are provided in this guide.},
  keywords={IEEE Standards;Power systems;Distributed power generation;Computer security;Safety;Power distribution;cybersecurity;certification;clearing time;codes;commissioning;communications;dc injection;design;diesel generators;dispersed generation;distributed generation;electric distribution systems;electric power systems;energy resources;energy storage;faults;field;flicker;frequency support;fuel cells;generators;grid;grid support;harmonics;IEEE 1547;induction machines;installation;interconnection requirements and specifications;interoperability;inverters;islanding;microturbines;monitoring and control;networks;paralleling;performance;photovoltaic power systems;point of common coupling;power converters;production tests;quality;power;protection functions;public utility commissions;reclosing coordination;regulations;ride through;rule-making;standards;storage;synchronous machines;testing;trip setting;utilities;voltage regulation;wind energy systems},
  doi={},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{7113314,
  author={Chakraborty, Tanmoy and Modani, Natwar and Narayanam, Ramasuri and Nagar, Seema},
  booktitle={2015 IEEE 31st International Conference on Data Engineering},
  title={DiSCern: A diversified citation recommendation system for scientific queries},
  year={2015},
  volume={},
  number={},
  pages={555-566},
  abstract={Performing literature survey for scholarly activities has become a challenging and time consuming task due to the rapid growth in the number of scientific articles. Thus, automatic recommendation of high quality citations for a given scientific query topic is immensely valuable. The state-of-the-art on the problem of citation recommendation suffers with the following three limitations. First, most of the existing approaches for citation recommendation require input in the form of either the full article or a seed set of citations, or both. Nevertheless, obtaining the recommendation for citations given a set of keywords is extremely useful for many scientific purposes. Second, the existing techniques for citation recommendation aim at suggesting prestigious and well-cited articles. However, we often need recommendation of diversified citations of the given query topic for many scientific purposes; for instance, it helps authors to write survey papers on a topic and it helps scholars to get a broad view of key problems on a topic. Third, one of the problems in the keyword based citation recommendation is that the search results typically would not include the semantically correlated articles if these articles do not use exactly the same keywords. To the best of our knowledge, there is no known citation recommendation system in the literature that addresses the above three limitations simultaneously. In this paper, we propose a novel citation recommendation system called DiSCern to precisely address the above research gap. DiSCern finds relevant and diversified citations in response to a search query, in terms of keyword(s) to describe the query topic, while using only the citation graph and the keywords associated with the articles, and no latent information. We use a novel keyword expansion step, inspired by community finding in social network analysis, in DiSCern to ensure that the semantically correlated articles are also included in the results. Our proposed approach primarily builds on the Vertex Reinforced Random Walk (VRRW) to balance prestige and diversity in the recommended citations. We demonstrate the efficacy of DiSCern empirically on two datasets: a large publication dataset of more than 1.7 million articles in computer science domain and a dataset of more than 29,000 articles in theoretical high-energy physics domain. The experimental results show that our proposed approach is quite efficient and it outperforms the state-of-the-art algorithms in terms of both relevance and diversity.},
  keywords={Communities;Approximation methods;Mathematical model;Markov processes;Context;Clustering algorithms;Computer science},
  doi={10.1109/ICDE.2015.7113314},
  ISSN={2375-026X},
  month={April},}@ARTICLE{10649569,
  author={Dwivedi, Rahul and Elluri, Lavanya},
  journal={IEEE Access},
  title={Exploring Generative Artificial Intelligence Research: A Bibliometric Analysis Approach},
  year={2024},
  volume={12},
  number={},
  pages={119884-119902},
  abstract={Artificial Intelligence (AI) and its many applications are changing our lives in ways we could not have imagined a decade ago. Generative artificial intelligence is an artificial intelligence system capable of generating texts, images, and other media based on the input training data. Although still in their early stages, numerous examples of such systems in different domains have gained widespread attention from the public, media, policymakers, and researchers. This study aims to explore the generative AI academic research in the past decade using bibliometrics, text analysis, and social network analysis. Specifically, research themes and their relationships, the evolution of research themes over time, and prominent authors, articles, journals, institutions, and countries publishing in generative AI are identified. The data was further found to partially support the classical bibliometrics laws of Zipf, and Bradford’s. The two overarching research themes identified using knowledge synthesis from most cited articles and journals are technical advancements and developments in generative AI systems; and their applications to image processing, pattern recognition, and computer vision. ChatGPT, large language models, and the application of generative AI to healthcare and education are emerging research topics. Additionally, generative AI’s usefulness to geoscience, remote sensing, Internet of Things (IoT), and cybersecurity are discussed.},
  keywords={Bibliometrics;Generative AI;Artificial intelligence;Databases;Market research;Indexes;Internet;Generative artificial intelligence;bibliometric analysis},
  doi={10.1109/ACCESS.2024.3450629},
  ISSN={2169-3536},
  month={},}@ARTICLE{7328760,
  author={Zhang, Haijun and Chow, Tommy W. S. and Wu, Q. M. Jonathan},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  title={Organizing Books and Authors by Multilayer SOM},
  year={2016},
  volume={27},
  number={12},
  pages={2537-2550},
  abstract={This paper introduces a new framework for the organization of electronic books (e-books) and their corresponding authors using a multilayer self-organizing map (MLSOM). An author is modeled by a rich tree-structured representation, and an MLSOM-based system is used as an efficient solution to the organizational problem of structured data. The tree-structured representation formulates author features in a hierarchy of author biography, books, pages, and paragraphs. To efficiently tackle the tree-structured representation, we used an MLSOM algorithm that serves as a clustering technique to handle e-books and their corresponding authors. A book and author recommender system is then implemented using the proposed framework. The effectiveness of our approach was examined in a large-scale data set containing 3868 authors along with the 10500 e-books that they wrote. We also provided visualization results of MLSOM for revealing the relevance patterns hidden from presented author clusters. The experimental results corroborate that the proposed method outperforms other content-based models (e.g., rate adapting poisson, latent Dirichlet allocation, probabilistic latent semantic indexing, and so on) and offers a promising solution to book recommendation, author recommendation, and visualization.},
  keywords={Electronic publishing;Feature extraction;Collaboration;Recommender systems;Biographies;Adaptation models;Nonhomogeneous media;Author recommendation;book recommendation;content-based recommendation;self-organizing map (SOM);tree structure},
  doi={10.1109/TNNLS.2015.2496281},
  ISSN={2162-2388},
  month={Dec},}@ARTICLE{9106193,
  author={Zanella Gomes, Joneval and Victória Barbosa, Jorge Luis and Resin Geyer, Claudio Fernando and Santos dos Anjos, Julio Cesar and Vicente Canto, José and Pessin, Gustavo},
  journal={Interacting with Computers},
  title={Ubiquitous Intelligent Services for Vehicular Users: A Systematic Mapping},
  year={2019},
  volume={31},
  number={3},
  pages={465-479},
  abstract={The space inside a vehicle, which people can enjoy while travelling, is becoming more intelligent due to the technological developments within the automotive industry. This has influenced the increase in new services that are intended to fulfil the needs of vehicle users. This article presents a survey on the provisioning of ubiquitous intelligent services for vehicular users. It also identifies clusters of research interest on this subject. To this end, an evaluation of results from six scientific research databases supports these goals. This process initially identified 37 328 publications; after filtering and clustering, the scope of analysis became 39 articles. The main contributions points to (i) the existence of five active research clusters, as a result of the inclusion of a visual clustering step in the systematic mapping protocol; (ii) an indication of a string of studies starting in the late ’90s from traffic issues, intelligent transport systems and profiling, to smart cities, the Internet of Things, big data, fog computing and internet of vehicles in recent years; and (iii) that the main challenges associated with the implementation of ubiquitous intelligent services for vehicular users are related to data security, infrastructure, connectivity and high mobility.},
  keywords={ubiquity;vehicle occupants;intelligent services;research clusters;systematic mapping},
  doi={10.1093/iwcomp/iwz030},
  ISSN={1873-7951},
  month={May},}@INPROCEEDINGS{4178388,
  author={Zhong, Ping and Chen, Jinlin and Cook, Terry},
  booktitle={2006 1st IEEE Workshop on Hot Topics in Web Systems and Technologies},
  title={Web Information Extraction Using Generalized Hidden Markov Model},
  year={2006},
  volume={},
  number={},
  pages={1-8},
  abstract={Hidden Markov model (HMM) is an important approach for information extraction (IE). When applied to Web IE, several problems exist with HMM based approaches due to the lack of consideration on Web-specific features. In this paper we present a generalized hidden Markov model (GHMM) that extends traditional HMMs by making use of Web-specific information for Web IE. In our approach we use Web content block instead of term as basic extraction unit. Besides, instead of using the traditional sequential state transition order, we detect the state transition order of GHMM based on layout structure of the corresponding Web page. Furthermore, we use multiple emission features instead of single emission feature. In this way GHMM can better accommodate Web IE. Experiments show promising results comparing to traditional HMM based Web IE},
  keywords={Data mining;Hidden Markov models;Web pages;Computer science;Cities and towns;Electronic mail;Information analysis;Internet;Web sites;Humans;Hidden Markov Model;Information extraction;Layout Analysis;Web},
  doi={10.1109/HOTWEB.2006.355271},
  ISSN={},
  month={Nov},}@ARTICLE{10070493,
  author={Amon, A and Robertson, N C and Miyatake, H and Heymans, C and White, M and DeRose, J and Yuan, S and Wechsler, R H and Varga, T N and Bocquet, S and Dvornik, A and More, S and Ross, A J and Hoekstra, H and Alarcon, A and Asgari, M and Blazek, J and Campos, A and Chen, R and Choi, A and Crocce, M and Diehl, H T and Doux, C and Eckert, K and Elvin-Poole, J and Everett, S and Ferté, A and Gatti, M and Giannini, G and Gruen, D and Gruendl, R A and Hartley, W G and Herner, K and Hildebrandt, H and Huang, S and Huff, E M and Joachimi, B and Lee, S and MacCrann, N and Myles, J and Navarro-Alsina, A and Nishimichi, T and Prat, J and Secco, L F and Sevilla-Noarbe, I and Sheldon, E and Shin, T and Tröster, T and Troxel, M A and Tutusaus, I and Wright, A H and Yin, B and Aguena, M and Allam, S and Annis, J and Bacon, D and Bilicki, M and Brooks, D and Burke, D L and Carnero Rosell, A and Carretero, J and Castander, F J and Cawthon, R and Costanzi, M and da Costa, L N and Pereira, M E S and de Jong, J and De Vicente, J and Desai, S and Dietrich, J P and Doel, P and Ferrero, I and Frieman, J and García-Bellido, J and Gerdes, D W and Gschwend, J and Gutierrez, G and Hinton, S R and Hollowood, D L and Honscheid, K and Huterer, D and Kannawadi, A and Kuehn, K and Kuropatkin, N and Lahav, O and Lima, M and Maia, M A G and Marshall, J L and Menanteau, F and Miquel, R and Mohr, J J and Morgan, R and Muir, J and Paz-Chinchón, F and Pieres, A and Plazas Malagón, A A and Porredon, A and Rodriguez-Monroy, M and Roodman, A and Sanchez, E and Serrano, S and Shan, H and Suchyta, E and Swanson, M E C and Tarle, G and Thomas, D and To, C and Zhang, Y},
  journal={Monthly Notices of the Royal Astronomical Society},
  title={Consistent lensing and clustering in a low-S8 Universe with BOSS, DES Year 3, HSC Year 1, and KiDS-1000},
  year={2022},
  volume={518},
  number={1},
  pages={477-503},
  abstract={We evaluate the consistency between lensing and clustering based on measurements from Baryon Oscillation Spectroscopic Survey combined with galaxy–galaxy lensing from Dark Energy Survey (DES) Year 3, Hyper Suprime-Cam Subaru Strategic Program (HSC) Year 1, and Kilo-Degree Survey (KiDS)-1000. We find good agreement between these lensing data sets. We model the observations using the DARK EMULATOR and fit the data at two fixed cosmologies: Planck (S8 = 0.83), and a Lensing cosmology (S8 = 0.76). For a joint analysis limited to large scales, we find that both cosmologies provide an acceptable fit to the data. Full utilization of the higher signal-to-noise small-scale measurements is hindered by uncertainty in the impact of baryon feedback and assembly bias, which we account for with a reasoned theoretical error budget. We incorporate a systematic inconsistency parameter for each redshift bin, A, that decouples the lensing and clustering. With a wide range of scales, we find different results for the consistency between the two cosmologies. Limiting the analysis to the bins for which the impact of the lens sample selection is expected to be minimal, for the Lensing cosmology, the measurements are consistent with A = 1; A = 0.91 ± 0.04 (A = 0.97 ± 0.06) using DES+KiDS (HSC). For the Planck case, we find a discrepancy: A = 0.79 ± 0.03 (A = 0.84 ± 0.05) using DES+KiDS (HSC). We demonstrate that a kinematic Sunyaev–Zeldovich-based estimate for baryonic effects alleviates some of the discrepancy in the Planck cosmology. This analysis demonstrates the statistical power of small-scale measurements; however, caution is still warranted given modelling uncertainties and foreground sample selection effects.},
  keywords={gravitational lensing: weak;large-scale structure of Universe;cosmology: observations},
  doi={10.1093/mnras/stac2938},
  ISSN={1365-2966},
  month={June},}@INPROCEEDINGS{10223544,
  author={Mitsa, Oleksandr and Sharkan, Vasyl and Maksymchuk, Vitalii and Varha, Sabolch and Shkurko, Halyna},
  booktitle={2023 IEEE International Conference on Smart Information Systems and Technologies (SIST)},
  title={Ethnocultural, Educational and Scientific Potential of the Interactive Dialects Map},
  year={2023},
  volume={},
  number={},
  pages={226-231},
  abstract={The article analyzes the features of an interactive map of dialects developed by Ukrainian scientists and available on the Internet at dialectmap.org. The client part of the interactive map was created using the React.js library of the JavaScript programming language, and the MOBX state management library was also used. The server side of the information system was written in Ruby programming language using the Ruby on Rails framework. The relational database management system Postgresql was used, and Redis cache was used for caching some frequently used data. Ukrainian universities successfully use the interactive map of dialects to conduct dialectological and ethnolinguistic practices, teach linguistic disciplines, and organize students' research work within linguistic clubs. The information system is a hub that more widely reflects local language features; the map contains materials from various studies created in different periods. The interactive map of dialects has more features than traditional (paper) dialectological atlases, makes the results of dialectological surveys accessible to many people, can hold a large amount of dialect data, and thus contributes to the preservation of linguistic diversity. The interactive map of dialects is developed to represent data from different languages and territories; thus, the project can be expanded to other countries in Europe, Asia, and other continents.},
  keywords={Rails;Surveys;Computer languages;Linguistics;Libraries;Servers;Speech processing;interactive map of dialects;dialectological research;language preservation;React.js;MOBX;Postgresql;Redis cache;Jest;Enzyme;RSpec},
  doi={10.1109/SIST58284.2023.10223544},
  ISSN={},
  month={May},}@INPROCEEDINGS{6927531,
  author={Dey, Lipika and Mahajan, Diwakar and Gupta, Hemant},
  booktitle={2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)},
  title={Obtaining Technology Insights from Large and Heterogeneous Document Collections},
  year={2014},
  volume={1},
  number={},
  pages={102-109},
  abstract={Keeping up with rapid advances in research in various fields of Engineering and Technology is a challenging task. Decision makers including academics, program managers, venture capital investors, industry leaders and funding agencies not only need to be abreast of latest developments but also be able to assess the effect of growth in certain areas on their core business. Though analyst agencies like Gartner, McKinsey etc. Provide such reports for some areas, thought leaders of all organisations still need to amass data from heterogeneous collections like research publications, analyst reports, patent applications, competitor information etc. To help them finalize their own strategies. Text mining and data analytics researchers have been looking at integrating statistics, text analytics and information visualization to aid the process of retrieval and analytics. In this paper, we present our work on automated topical analysis and insight generation from large heterogeneous text collections of publications and patents. While most of the earlier work in this area provides search-based platforms, ours is an integrated platform for search and analysis. We have presented several methods and techniques that help in analysis and better comprehension of search results. We have also presented methods for generating insights about emerging and popular trends in research along with contextual differences between academic research and patenting profiles. We also present novel techniques to present topic evolution that helps users understand how a particular area has evolved over time.},
  keywords={Patents;Context;Market research;Data mining;Data visualization;Hidden Markov models;Indexing;mining publications;mining patent databases;analyzing research trends},
  doi={10.1109/WI-IAT.2014.22},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{848377,
  author={Wolff, J.E. and Florke, H. and Cremers, A.B.},
  booktitle={Proceedings IEEE Advances in Digital Libraries 2000},
  title={Searching and browsing collections of structural information},
  year={2000},
  volume={},
  number={},
  pages={141-150},
  abstract={This paper proposes a new approach to querying collections of structured textual information such as SGML/XML documents. Knowledge about the structure of documents is an additional resource that should be exploited during retrieval since the semantics of the different textual objects can be used to specify an information need much more precisely. However the traditional probabilistic retrieval model lacks the ability to handle structural information. We define a new retrieval function based on the probabilistic model which overcomes this drawback. The presented query language allows the assignment of structural roles to individual terms. The efficient evaluation of queries in this framework requires appropriate index structures. We design text and structure indexes and show how their information is combined during evaluation. The implementation supports additional functionalities such as a table of contents for browsing. First evaluation results show the feasibility of the approach on collections of unstructured documents.},
  keywords={Cost accounting;Database languages;Information retrieval;SGML;XML;Internet;Publishing;World Wide Web;Markup languages;Usability},
  doi={10.1109/ADL.2000.848377},
  ISSN={},
  month={May},}@INPROCEEDINGS{8107145,
  author={Pănescu, Adrian-Tudor and Manta, Vasile},
  booktitle={2017 21st International Conference on System Theory, Control and Computing (ICSTCC)},
  title={RDF-based workflows for the figshare research data repository},
  year={2017},
  volume={},
  number={},
  pages={860-865},
  abstract={This paper describes an RDF data model for repositories concerned with managing various aspects of the life cycle of research outputs, such as articles, data sets or learning objects. Using the figshare application as a use-case and starting from its relational model, it proposes a new mechanism for storing, enhancing and disseminating metadata, using workflows specific to the linked data ecosystem, such as RDF, XML, XSLT, triple and graph databases; the viability of the mechanism is demonstrated by a developed proof-of-concept.},
  keywords={Resource description framework;Metadata;Ontologies;XML;Standards;Vocabulary;Web Services;Software Engineering;Other Topics},
  doi={10.1109/ICSTCC.2017.8107145},
  ISSN={},
  month={Oct},}@BOOK{8187446,
  author={Gupta, Manish and Bendersky, Michael},
  title={Information Retrieval with Verbose Queries},
  year={2015},
  volume={},
  number={},
  pages={},
  abstract={Information retrieval with verbose natural language queries has been generating a lot of interest in recent years. The focus of many novel search applications has shifted from short keyword queries to verbose queries. Examples include question answering systems and dialogue systems, voice search on mobile devices, and entity search engines like Facebook's Graph Search or Google's Knowledge Graph. However, the performance of textbook information retrieval techniques for such verbose queries is not as good as that for their shorter counterparts. Thus, effective handling of verbose queries has become a critical factor for adoption of information retrieval techniques in this new breed of search applications. Over the past decade, the information retrieval community has deeply explored the problem of transforming natural language verbose queries using operations like reduction, weighting, expansion, reformulation and segmentation into more effective structural representations. Information Retrieval with Verbose Queries is the first monograph to provide a coherent and organized survey on this topic. It puts together the various research pieces of the puzzle, provides a comprehensive and structured overview of diverse proposed methods, and lists several application scenarios where effective verbose query processing can make a significant difference. Information Retrieval with Verbose Queries is a very timely reference on this important topic. It is entirely based on previously published research and publicly available datasets and as such, it should prove useful for both practitioners and academic researchers interested in reproducing the reported results.},
  keywords={Information Retrieval},
  doi={10.1561/1500000050},
  ISSN={},
  publisher={now},
  isbn={9781680830453},
  url={https://ieeexplore.ieee.org/document/8187446},}@INPROCEEDINGS{9653592,
  author={Li, Feng and Liao, Kai},
  booktitle={2021 International Conference on Intelligent Computing, Automation and Applications (ICAA)},
  title={Construction of Mobile Service Platform in University Library},
  year={2021},
  volume={},
  number={},
  pages={320-324},
  abstract={In recent years, with the popularity of wireless networks, mobile library as a quick and convenient service is used in the construction of university library, but its service mode is single and hard to better play the advantages of mobile service. In this paper, a kind of based on the integration of functions, basic services, extended services and personalized service composition of mobile service platform in university libraries. At last, the case of mobile service platform construction of Huazhong Agricultural University Library is introduced.},
  keywords={Automation;Wireless networks;Libraries},
  doi={10.1109/ICAA53760.2021.00065},
  ISSN={},
  month={June},}@INBOOK{6277891,
  author={Mehlenbacher, Brad},
  booktitle={Instruction and Technology: Designs for Everyday Learning},
  title={A Framework for Everyday Instructional Situations},
  year={2010},
  volume={},
  number={},
  pages={193-330},
  abstract={This chapter contains sections titled: Learner Background and Knowledge, Learner Tasks and Activities, Social Dynamics, Instructor Activities, Learning Environment and Artifacts},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262289634},
  url={https://ieeexplore.ieee.org/document/6277891},}@INPROCEEDINGS{10151369,
  author={Shafiya, Soofi and Jabin, Suraiya},
  booktitle={2023 International Conference on Recent Advances in Electrical, Electronics & Digital Healthcare Technologies (REEDCON)},
  title={Current Trends in Social-Media based Disease Outbreak Prediction & Surveillance Systems},
  year={2023},
  volume={},
  number={},
  pages={205-210},
  abstract={The purpose of disease surveillance is to monitor and predict public health emergencies or to predict outbreak of any disease. It can be achieved by analyzing disease related data being collected by various government agencies/hospitals. Public health officials keep track of information, predict possible outbreaks, and keep an eye on emergency situations using this data. In the recent past, social media has grown enough to have deep impact on people’s life specially during time of pandemic. Analysis of social media data offers meaningful insights towards disease surveillance. It helps in figuring out how much people know about diseases and how they feel about official/government health related communications and responses. In this study, we present review of most recent works which used social media as a data source for the prevention & prediction of outbreaks. Additionally, we discuss methods that are being implemented on social media data towards designing robust disease outbreak prediction models. This study emphasizes how social media has helped during various outbreaks in the past. A bibliometric analysis has also been done over the most recent & relevant literature collected from source SCOPUS.},
  keywords={Social networking (online);Pandemics;Surveillance;Soft sensors;Government;Predictive models;Market research;disease outbreak;machine learning;social media;prediction;health care},
  doi={10.1109/REEDCON57544.2023.10151369},
  ISSN={},
  month={May},}@BOOK{10460886,
  author={Sherrington, Malcolm},
  title={Mastering Julia: Enhance your analytical and programming skills for data modeling and processing with Julia},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={A hands-on, code-based guide to leveraging Julia in a variety of scientific and data-driven scenariosKey FeaturesAugment your basic computing skills with an in-depth introduction to JuliaFocus on topic-based approaches to scientific problems and visualisationBuild on prior knowledge of programming languages such as Python, R, or C/C++Purchase of the print or Kindle book includes a free PDF eBookBook DescriptionJulia is a well-constructed programming language which was designed for fast execution speed by using just-in-time LLVM compilation techniques, thus eliminating the classic problem of performing analysis in one language and translating it for performance in a second. This book is a primer on Julia’s approach to a wide variety of topics such as scientific computing, statistics, machine learning, simulation, graphics, and distributed computing. Starting off with a refresher on installing and running Julia on different platforms, you’ll quickly get to grips with the core concepts and delve into a discussion on how to use Julia with various code editors and interactive development environments (IDEs). As you progress, you’ll see how data works through simple statistics and analytics and discover Julia's speed, its real strength, which makes it particularly useful in highly intensive computing tasks. You’ll also and observe how Julia can cooperate with external processes to enhance graphics and data visualization. Finally, you will explore metaprogramming and learn how it adds great power to the language and establish networking and distributed computing with Julia. By the end of this book, you’ll be confident in using Julia as part of your existing skill set.What you will learnDevelop simple scripts in Julia using the REPL, code editors, and web-based IDEsGet to grips Julia’s type system, multiple dispatch, metaprogramming, and macro developmentInteract with data files, tables, data frames, SQL, and NoSQL databasesDelve into statistical analytics, linear programming, and optimization problemsCreate graphics and visualizations to enhance modeling and simulation in JuliaUnderstand Julia's main approaches to machine learning, Bayesian analysis, and AIWho this book is forThis book is not an introduction to computer programming, but a practical guide for developers who want to enhance their basic knowledge of Julia, or those wishing to augment their skill set by adding Julia to their existing roster of programming languages. Familiarity with a scripting language such as Python or R, or a compiled language such as C/C++, C# or Java, is a prerequisite.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805128236},
  url={https://ieeexplore.ieee.org/document/10460886},}@INPROCEEDINGS{10559292,
  author={Soni, Tanishq and Gupta, Deepali and Uppal, Mudita},
  booktitle={2024 International Conference on E-mobility, Power Control and Smart Systems (ICEMPS)},
  title={Mapping AI's Influence on Cardiac Healthcare: A Bibliometric Analysis (2014-2023)},
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Artificial intelligence in one of the fast-growing fields. In medical and smart healthcare field artificial intelligence has gained considerable momentum. The utilization of technological advances in cardiology has gained significance as a study topic recently. Smart healthcare seeks to increase the health and well-being of people throughout their lives by utilizing cutting-edge technology like AI. The use of artificial intelligence during medical procedures will enhance illness diagnosis, categorization, and prediction, which is advantageous to both patients and healthcare professionals. One of the primary objectives of bibliometric analysis is to assess and comprehend the academic significance of research publications, authors, journals, and academic institutions. The goal of the study was to carry out a bibliometric analysis on healthcare-related, heart disease, or cardiac disease research with AI predication and deep learning from the years 2014 to 2023.},
  keywords={Heart;Bibliometrics;Power control;Medical services;Transforms;Predictive models;Market research;healthcare;artificial intelligence;heart disease;bibliometric},
  doi={10.1109/ICEMPS60684.2024.10559292},
  ISSN={},
  month={April},}@INPROCEEDINGS{9971449,
  author={Ramadhani, Erika and Hariyadi, Dedy and Nastiti, Faulinda Ely},
  booktitle={2022 IEEE 7th International Conference on Information Technology and Digital Applications (ICITDA)},
  title={A Bibliometrics Analysis of Digital Forensics Research in Indonesia Based on Scopus Index: 2012-2021},
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={This study aims to demonstrate step-by-step bibliometric data analysis through VOSViewer. Through mapping tools in the VOSViewer, this study provides insight into digital forensics over ten years (2012–2022). In order to describe the bibliometric analysis performance of the selected topic, we created a network visualization using qualitative descriptive methods. This study found 117 journal articles published in digital forensics between 2012 and 2021, and then the data was cleaned to produce 103 articles for analysis. The study concludes that Indonesian digital forensics research is still relatively incipient. This bibliometric analysis study will support discovering new things on similar topics. In addition, research in the field of study is essential because it contributes to understanding the validity of digital evidence, which can support decision-making in court.},
  keywords={Data analysis;Databases;Soft sensors;Digital forensics;Bibliometrics;Ecosystems;Decision making;digital forensics;bibliometrics analysis;scopus;research;publication},
  doi={10.1109/ICITDA55840.2022.9971449},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{5715564,
  author={Zhou, Rurui},
  booktitle={2010 Fourth International Conference on Genetic and Evolutionary Computing},
  title={Data-Intensive Scientific Workflows for Grid Computing with CSCL},
  year={2010},
  volume={},
  number={},
  pages={845-848},
  abstract={In terms of several new technologies, such as ubiquitous computing, ontology engineering, semantic web and grid computing, this paper proposes a kind of flexible educational platform architecture for Computer-Supported Collaborative Learning (CSCL). With data-intensive scientific workflows, it is promising to gain concept reusability, device and user adaptability, automatic composition, function and performance scalability. In the meantime, the grid-based system design of workflow creation simulation is also proposed. Experiments indicate that this architecture is viable and flexible. At last, the author discusses some problems of CSCL and the next work.},
  keywords={Education;Object oriented modeling;Computational modeling;Biological system modeling;Web services;Collaborative work;Software;CSCL systems;Grid Computing;workflow;Web Service;Simulation},
  doi={10.1109/ICGEC.2010.214},
  ISSN={},
  month={Dec},}@ARTICLE{1032652,
  author={King, P.H.},
  journal={IEEE Engineering in Medicine and Biology Magazine},
  title={Noninvasive instrumentation and measurement medical diagnosis},
  year={2002},
  volume={21},
  number={4},
  pages={131-133},
  abstract={},
  keywords={Instrumentation and measurement;Medical diagnosis;Magnetic field measurement;Biomedical imaging;Biomedical measurements;Pressure measurement;Ultrasonic variables measurement;Electric variables measurement;Anthropometry;Humans},
  doi={10.1109/MEMB.2002.1032652},
  ISSN={1937-4186},
  month={July},}@INPROCEEDINGS{10621499,
  author={Pop, Mădălin-Dorin and Micea, Mihai V.},
  booktitle={2024 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)},
  title={Visual Analysis of the Bibliometric Data Associated with the Calibration of Car-Following Models},
  year={2024},
  volume={},
  number={},
  pages={647-652},
  abstract={Intelligent transportation systems (ITS) use the latest technologies for real-time traffic control and monitoring to ensure efficient traffic management and reduce the risks of traffic accidents. The microscopic level of traffic modeling is the most appropriate level for controlling and monitoring the interaction between vehicles based on car-following scenarios. However, the data retrieved from sensor networks can be affected by measurement errors, and consequently the implementation of appropriate mechanisms to overcome their propagation to the control system is mandatory. This paper aims to analyse the current research in the calibration of car-following models and provide valuable insights of recent developments in this field. To achieve this goal, VOSviewer has been chosen as a visualisation tool to create bibliographic maps based on the output from the well-known scientific database Clarivate Analytics Web of Science (WoS). The maps obtained provide a visual representation of the main institutions involved in this field of research and identify the research interests based on author and indexing keywords. Furthermore, this paper analyses the top five clusters identified based on the analysis of co-occurrence keywords, presenting discussions about the connections existing within these clusters.},
  keywords={Analytical models;Visualization;Computational modeling;Bibliometrics;Transportation;Traffic control;Data models;bibliometric analysis;calibration;car-following model;intelligent transportation systems;VOSviewer},
  doi={10.1109/DCOSS-IoT61029.2024.00101},
  ISSN={2325-2944},
  month={April},}@INPROCEEDINGS{10090145,
  author={Assiya, Bakass and Agouti, Tarik and Zahir, Jihad and Ait-Mlouk, A. and Adnani, Mohammed El},
  booktitle={2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)},
  title={Big data and Intelligent decision Making: Approaches and Applications},
  year={2022},
  volume={},
  number={},
  pages={i-viii},
  abstract={Big data technology is the next frontier for innovation, competition, and productivity. Big data analytics in the manufacturing industry is mainly used to process massive data in various manufacturing activities. Today, many industrial companies are starting discussions on the importance of DSS (Decision Support Systems) and its cloud processing capability. In recent years different approaches have been developed to provide appropriate solutions for big data and DSS. In this work, we will carry out detailed bibliographical research on the methods and applications of big data for decision making and provide a classification for them. Even if this bibliographic review does not call on all the studies carried out in this field, we believe that it can constitute a valuable source of information, particularly for practitioners in big data and Intelligent Decision-Making.},
  keywords={Decision support systems;Productivity;Manufacturing industries;Technological innovation;Decision making;Companies;Big Data;Decision Support Systems;Big Data;Manufacturing industry;Intelligent Decision-Making},
  doi={10.1109/SITIS57111.2022.00074},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9891941,
  author={Przybyła, Piotr and Borkowski, Piotr and Kaczyński, Konrad},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  title={Countering Disinformation by Finding Reliable Sources: a Citation-Based Approach},
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={We propose a new task aimed at countering dis- and misinformation, called Finding Reliable Sources. Given a one-sentence claim, the challenge is to automatically find a knowledge source (e.g. a book, a research article, a web page) that could support or refute the claim. We show that this capability could be learnt by observing associations between sentences in English Wikipedia and citations provided for them. Thus, we collect a corpus of over 50 million references to 24 million identified sources with the citation context from Wikipedia, and build search indices using several meaning representation methods. For evaluation, apart from the Wikipedia corpus, we prepare another test set based on the FEVER fact-checking dataset.},
  keywords={Knowledge engineering;Neural networks;Web pages;Encyclopedias;Internet;Reliability;Online services;finding reliable sources;misinformation;fact-checking;fake news},
  doi={10.1109/IJCNN55064.2022.9891941},
  ISSN={2161-4407},
  month={July},}@ARTICLE{9690118,
  author={},
  journal={P1484.20.2/D2, December 2021},
  title={IEEE Draft Recommended Practices for Defining Competencies},
  year={2022},
  volume={},
  number={},
  pages={1-88},
  abstract={Recommended practices for defining competencies are provided. IEEE P1484.20.3/D1 (February 2022), a technical standard for competency definitions data, is supported. Recommended practices are given for defining the kinds of information that IEEE P1484.20.3/D1 (February 2022) encodes as interoperable data, i.e., data that defines a skill, knowledge, ability, attitude, habit of practice, behavior, or learning outcome.},
  keywords={IEEE Standards;Industry applications;Learning systems;Terminology;Performance evaluation;competency;competency definition;competency framework;IEEE 1484.20.2},
  doi={},
  ISSN={},
  month={Jan},}@BOOK{8759000,
  author={Harman, Donna},
  title={Information Retrieval: The Early Years},
  year={2019},
  volume={},
  number={},
  pages={},
  abstract={Information Retrieval is at the core of our daily lives. Modern search, ranking and indexing systems underpinned by enhanced computing power, fast network speeds and near unlimited data storage capacity mean we have easy access to all the information we need, when we need it. Yet the principles upon which this modern technology based date back to before the 1960s. In this concise history of the early years of Information Retrieval, Donna Harman, one of the pioneers of the field, provides the reader with a plethora of insights into the important work that led us to where we are today. Written in a chronological order and in a manner that presents the technical context, the research and the early commercialization efforts, it lays out how each contribution built on what went before. The reader is offered a text that is not only a delight to read, but is also insightful in the way the technologies evolve as computing power increases. Information Retrieval: The Early Years will be of interest to everyone with an interest in understanding the foundations of the science behind search engines.},
  keywords={},
  doi={10.1561/1500000065},
  ISSN={},
  publisher={now},
  isbn={9781680835854},
  url={https://ieeexplore.ieee.org/document/8759000},}@ARTICLE{9775951,
  author={Burhanudin, Kharismi and Jusoh, Mohamad Huzaimy and Latiff, Zatul Iffah Abdul and Hashim, Mohd Helmy and Ashar, Nur Dalila Khirul},
  journal={IEEE Access},
  title={The Estimation of the Geomagnetically Induced Current Based on Simulation and Measurement at the Power Network: A Bibliometric Analysis of 42 Years (1979–2021)},
  year={2022},
  volume={10},
  number={},
  pages={56525-56549},
  abstract={GIC (geomagnetic induced current) is a natural current that flows through a conductive substance. The purpose of this study is to provide bibliometric analysis on the computation of the GIC at the Power Network, since determining the backflow current’s threshold limit is crucial to avoid electrical equipment failure. The methodology of the study includes topics, scope, and eligibility, as well as screening and an analytical screen paper. From 1979 to 2021, we investigate the evolution of bibliometric studies on the assessment of the GIC at the power network. According to the statistics, there are 601 Scopus articles and 357 Web of Science (WoS) papers in the study on GIC at the power network that focus on estimation from 1979 to 2021. According to the data, the Engineering and Energy disciplines contribute the most to research on predicting the GIC at the Power Network. The words “geomagnetically induced current,” “reactive power,” and “geomagnetism” are commonly used instead of “magnetic storm,” “power grids,” and “geoelectric fields.” The bibliometric method encompasses themes, scope and eligibility, screening, and screen paper for all publications in a search for developing subjects based on Scopus and WoS to map the time-trend, disciplinary distribution, and high-frequency keywords.},
  keywords={Bibliometrics;Computational modeling;Estimation;Circuit faults;Power grids;Data models;Current measurement;GIC;bibliometric;scopus;WoS},
  doi={10.1109/ACCESS.2022.3175882},
  ISSN={2169-3536},
  month={},}@ARTICLE{531798,
  author={Hsinchun Chen and Schatz, B. and Ng, T. and Martinez, J. and Kirchhoff, A. and Chienting Lin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={A parallel computing approach to creating engineering concept spaces for semantic retrieval: the Illinois Digital Library Initiative project},
  year={1996},
  volume={18},
  number={8},
  pages={771-782},
  abstract={This research presents preliminary results generated from the semantic retrieval research component of the Illinois Digital Library Initiative (DLI) project. Using a variation of the automatic thesaurus generation techniques, to which we refer to as the concept space approach, we aimed to create graphs of domain-specific concepts (terms) and their weighted co-occurrence relationships for all major engineering domains. Merging these concept spaces and providing traversal paths across different concept spaces could potentially help alleviate the vocabulary (difference) problem evident in large-scale information retrieval. In order to address the scalability issue related to large-scale information retrieval and analysis for the current Illinois DLI project, we conducted experiments using the concept space approach on parallel supercomputers. Our test collection included computer science and electrical engineering abstracts extracted from the INSPEC database. The concept space approach called for extensive textual and statistical analysis (a form of knowledge discovery) based on automatic indexing and co-occurrence analysis algorithms, both previously tested in the biology domain. Initial testing results using a 512-node CM-5 and a 16-processor SGI Power Challenge were promising.},
  keywords={Parallel processing;Large-scale systems;Information retrieval;Testing;Software libraries;Thesauri;Merging;Vocabulary;Scalability;Information analysis},
  doi={10.1109/34.531798},
  ISSN={1939-3539},
  month={Aug},}@INPROCEEDINGS{7876350,
  author={Skulimowski, Andrzej M. J.},
  booktitle={2016 IEEE International Conference on Computer and Information Technology (CIT)},
  title={Impact of Future Intelligent Information Technologies on the Methodology of Scientific Research},
  year={2016},
  volume={},
  number={},
  pages={289-298},
  abstract={The impact of future Information and Communication Technologies on different spheres of human private, professional and social life attracts a growing attention of a broad public. In particular, the question whether the deployment of intelligent systems will be a chance or a threat to mankind has been considered by many researchers recently. This paper is devoted to an estimation of advanced ICT tools impact on the modes and progress of scientific research in various areas. We will present and discuss the development prospects of selected intelligent technologies such as Global Expert Systems (GES), bi-directional Brain-Computer Interfaces (BCI), or Creativity Support Systems (CSS). The forecasts of their development until 2025 has been presented in a series of papers resulting from a recent ICT foresight project. The above mentioned technologies are supposed to allow scientists to compete with the growing capabilities of intelligent systems to make autonomous decisions in dynamically changing environment. Such systems are referred to as Artificial Autonomous Decision Systems (AADS). We claim that a future scientist endowed with joint capacities of human brain and new technologies will be able to cope with big scientific data growth which exhausts the capacities of any individual researcher. We will present and discuss scenarios that may dominate the development of future scientific methodology. The first one will rely on automation of research, i.e. letting automated expert systems select and process knowledge up to the stage of an edited scientific paper, with relatively minor and ever-decreasing human intervention. The other assumes an intensive development of BCI as an enabling technology for future advanced hybrid intelligent systems. All mitigating approaches are expected to merge, but other optimistic or pessimistic scenarios are also possible. These will be discussed in the Conclusions section of this paper together with recommendations to researchers and R&D policy makers.},
  keywords={Expert systems;Market research;Knowledge engineering;Brain-computer interfaces;Search engines;Google;AI Foresight;E-Science Scenarios;Global Expert Systems;Brain-Computer Interfaces;Anticipatory Networks},
  doi={10.1109/CIT.2016.118},
  ISSN={},
  month={Dec},}@ARTICLE{9007723,
  author={Al Qassem, Lamees M. and Stouraitis, Thanos and Damiani, Ernesto and Elfadel, Ibrahim M.},
  journal={IEEE Transactions on Services Computing},
  title={FPGAaaS: A Survey of Infrastructures and Systems},
  year={2022},
  volume={15},
  number={2},
  pages={1143-1156},
  abstract={The popularity of cloud computing services for delivering and accessing infrastructure on demand has significantly increased over the last few years. Concurrently, the usage of FPGAs to accelerate compute-intensive applications has become more widespread in different computational domains due to their ability to achieve high throughput and predictable latency while providing programmability and improved energy efficiency. Computationally intensive applications such as big data analytics, machine learning, and video processing have been accelerated by FPGAs. With the exponential workload increase in data centers, major cloud service providers have made FPGAs and their capabilities available as cloud services. However, enabling FPGAs in the cloud is not a trivial task due to incompatibilities with existing cloud infrastructure and operational challenges related to abstraction, virtualization, partitioning, and security. In this article, we survey recent frameworks for offering FPGA hardware acceleration as a cloud service, classify them based on their virtualization mode, tenancy model, communication interface, software stack, and hardware infrastructure. We further highlight current FPGAaaS trends and identify FPGA resource sharing, security, and microservicing as important areas for future research.},
  keywords={Field programmable gate arrays;Cloud computing;Acceleration;Virtualization;Task analysis;Web services;microservices;FPGA;virtualization;orchestration;hardware acceleration},
  doi={10.1109/TSC.2020.2976012},
  ISSN={1939-1374},
  month={March},}@ARTICLE{7809016,
  author={Xia, Feng and Wang, Wei and Bekele, Teshome Megersa and Liu, Huan},
  journal={IEEE Transactions on Big Data},
  title={Big Scholarly Data: A Survey},
  year={2017},
  volume={3},
  number={1},
  pages={18-35},
  abstract={With the rapid growth of digital publishing, harvesting, managing, and analyzing scholarly information have become increasingly challenging. The term Big Scholarly Data is coined for the rapidly growing scholarly data, which contains information including millions of authors, papers, citations, figures, tables, as well as scholarly networks and digital libraries. Nowadays, various scholarly data can be easily accessed and powerful data analysis technologies are being developed, which enable us to look into science itself with a new perspective. In this paper, we examine the background and state of the art of big scholarly data. We first introduce the background of scholarly data management and relevant technologies. Second, we review data analysis methods, such as statistical analysis, social network analysis, and content analysis for dealing with big scholarly data. Finally, we look into representative research issues in this area, including scientific impact evaluation, academic recommendation, and expert finding. For each issue, the background, main challenges, and latest research are covered. These discussions aim to provide a comprehensive review of this emerging area. This survey paper concludes with a discussion of open issues and promising future directions.},
  keywords={Data mining;Big data;Metadata;Libraries;Social network services;Crawlers;Data analysis;Bibliographies;Scholarly data;data analysis;academic social networks;science of science},
  doi={10.1109/TBDATA.2016.2641460},
  ISSN={2332-7790},
  month={March},}@ARTICLE{9714822,
  author={Ma, Yang and Zhang, Chaoyi and Cabezas, Mariano and Song, Yang and Tang, Zihao and Liu, Dongnan and Cai, Weidong and Barnett, Michael and Wang, Chenyu},
  journal={IEEE Journal of Biomedical and Health Informatics},
  title={Multiple Sclerosis Lesion Analysis in Brain Magnetic Resonance Images: Techniques and Clinical Applications},
  year={2022},
  volume={26},
  number={6},
  pages={2680-2692},
  abstract={Multiple sclerosis (MS) is a chronic inflammatory and degenerative disease of the central nervous system, characterized by the appearance of focal lesions in the white and gray matter that topographically correlate with an individual patient’s neurological symptoms and signs. Magnetic resonance imaging (MRI) provides detailed in-vivo structural information, permitting the quantification and categorization of MS lesions that critically inform disease management. Traditionally, MS lesions have been manually annotated on 2D MRI slices, a process that is inefficient and prone to inter-/intra-observer errors. Recently, automated statistical imaging analysis techniques have been proposed to detect and segment MS lesions based on MRI voxel intensity. However, their effectiveness is limited by the heterogeneity of both MRI data acquisition techniques and the appearance of MS lesions. By learning complex lesion representations directly from images, deep learning techniques have achieved remarkable breakthroughs in the MS lesion segmentation task. Here, we provide a comprehensive review of state-of-the-art automatic statistical and deep-learning MS segmentation methods and discuss current and future clinical applications. Further, we review technical strategies, such as domain adaptation, to enhance MS lesion segmentation in real-world clinical settings.},
  keywords={Lesions;Image segmentation;Imaging;Magnetic resonance imaging;White matter;Training;Three-dimensional displays;Brain;image segmentation;lesion analysis;multiple sclerosis;magnetic resonance imaging;review},
  doi={10.1109/JBHI.2022.3151741},
  ISSN={2168-2208},
  month={June},}@ARTICLE{9274368,
  author={Zhang, Yidan and Duan, Lei and Zheng, Huiru and Li-Ling, Jesse and Qin, Ruiqi and Chen, Zihao and He, Chengxin and Wang, Tingting},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  title={Mining Similar Aspects for Gene Similarity Explanation Based on Gene Information Network},
  year={2022},
  volume={19},
  number={3},
  pages={1734-1746},
  abstract={Analysis of gene similarity not only can provide information on the understanding of the biological roles and functions of a gene, but may also reveal the relationships among various genes. In this paper, we introduce a novel idea of mining similar aspects from a gene information network, i.e., for a given gene pair, we want to know in which aspects (meta paths) they are most similar from the perspective of the gene information network. We defined a similarity metric based on the set of meta paths connecting the query genes in the gene information network and used the rank of similarity of a gene pair in a meta path set to measure the similarity significance in that aspect. A minimal set of gene meta paths where the query gene pair ranks the highest is a similar aspect, and the similar aspect of a query gene pair is far from trivial. We proposed a novel method, SCENARIO, to investigate minimal similar aspects. Our empirical study on the gene information network, constructed from six public gene-related databases, verified that our proposed method is effective, efficient, and useful.},
  keywords={Biology;Semantics;Diseases;Proteins;Ontologies;Search problems;Peer-to-peer computing;Similar aspect;gene information network;gene meta path},
  doi={10.1109/TCBB.2020.3041559},
  ISSN={1557-9964},
  month={May},}@INPROCEEDINGS{5489234,
  author={Boddu, Sekhar Babu and Anne, V.P Krishna and Kurra, Rajesekhara Rao and Mishra, Durgesh Kumar},
  booktitle={2010 Fourth Asia International Conference on Mathematical/Analytical Modelling and Computer Simulation},
  title={Knowledge Discovery and Retrieval on World Wide Web Using Web Structure Mining},
  year={2010},
  volume={},
  number={},
  pages={532-537},
  abstract={The World Wide Web is nearing omnipresence. The explosively growing number of Web contents including Digitalized manuals, emails pictures, multimedia, and Web services require a distinct and elaborate structural framework that can provide a navigational surrogate for clients as well as for servers. Due to the increasing amount of data Available online, the World Wide Web has becoming one of the most valuable resources for information retrievals and knowledge discoveries. Web mining technologies are the right solutions for knowledge discovery on the Web. The knowledge extracted from the Web can be used to raise the performances for Web information retrievals, question answering, and Web based data warehousing. In this paper, we provide an introduction of Web mining as well as a review of the Web mining categories. Then we focus on one of these categories: the Web structure mining. Within this category, we introduce link mining and review two popular methods applied in Web structure mining: HITS and Page Rank.},
  keywords={Web sites;Web mining;Information retrieval;Data mining;Web pages;Databases;Asia;Analytical models;Mathematical model;Computer simulation;Web structure mining;Web mining;link mining},
  doi={10.1109/AMS.2010.108},
  ISSN={2376-1172},
  month={May},}@INPROCEEDINGS{8070862,
  author={Anupkant, S. and Kumar, P.V.M. Seravana and Sateesh, Nayani and Mahesh, D. Bhanu},
  booktitle={2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)},
  title={Opinion mining on author's citation characteristics of scientific publications},
  year={2017},
  volume={},
  number={},
  pages={348-351},
  abstract={Opinion mining of authors opinions on scientific papers in citations is an important feature of scientific publications. Opinion mining aims to determine the defiance of a topic with respect to the overall polarity of a document. The main engine that drives opinion mining is the processing of subjective information. A dataset in the form of sentence-based collection of over 785 citations were collected. After excluding neutral citations, the dataset of 234 opinion citations were analyzed for the presence of positive and negative features. It was observed that majority (91.6%) were found to be positive, the fraction of citations with negative orientation amounted to 8.4% and nearly 6% of the citations contained opinion terms of both positive and negative polarity. Logistic regression applied on training set resulted in 0.98 precision obtained on a classification model; hence the obtained regression model was applied on test set data to reveal positive and negative opinions.},
  keywords={Data mining;Training;Logistics;Data models;Bibliometrics;Sentiment analysis;Measurement;Opinion mining;citation;publications;regression;sentiment analysis},
  doi={10.1109/ICBDACI.2017.8070862},
  ISSN={},
  month={March},}@BOOK{9106085,
  author={Leon, Alexis},
  title={A Guide to Software Configuration Management},
  year={2000},
  volume={},
  number={},
  pages={},
  abstract={Take control of the software development process with this new book, a comprehensive explanation of Software Configuration Management (SCM). It provides everything you need to know - from the basic definition of SCM as a scientific tool that brings control to the development process, to the procedures for SCM implementation in your organization. You review each phase in the software development lifecycle, and learn how SCM can help you avoid pitfalls at every step. A Guide to Software Configuration Management explains the essential SCM lessons that you need to understand the mechanics of this development tool and how to implement it completely to save your organization time, money, and headaches. Using this guide, you learn how to create an SCM implementation plan, decide what additional SCM tools you may need, and understand how to evaluate SCM tools currently on the market. You also learn how to deal with SCM vendors and consultants, gain acceptance for SCM among your staff, and maintain an effective SCM plan over the long haul. SCM is explained in jargon-free language, allowing you to easily comprehend every step in the process, while gaining the in-depth knowledge you need to discern what specific SCM elements you need most. Without sermonizing, this book allows you to gain the confidence you need to understand, plan, implement, and manage SCM in your organization. Plus, the book includes a comprehensive list of SCM resources useful to all software development managers.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580534208},
  url={https://ieeexplore.ieee.org/document/9106085},}@ARTICLE{9363693,
  author={},
  journal={IEEE Std 802.11-2020 (Revision of IEEE Std 802.11-2016)},
  title={IEEE Standard for Information Technology--Telecommunications and Information Exchange between Systems - Local and Metropolitan Area Networks--Specific Requirements - Part 11: Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specifications},
  year={2021},
  volume={},
  number={},
  pages={1-4379},
  abstract={Technical corrections and clarifications to IEEE Std 802.11 for wireless local area networks (WLANs) as well as enhancements to the existing medium access control (MAC) and physical layer (PHY) functions are specified in this revision. Amendments 1 to 5 published in 2016 and 2018 have also been incorporated into this revision. (The PDF of this standard is available at no cost to you compliments of the IEEE GET program https://ieeexplore.ieee.org/browse/standards/get-program/page/series?id=68)},
  keywords={IEEE Standards;Information technology;Information exchange;Media Access Protocol;Encryption;Authentication;Message services;2.4 GHz;256-QAM;3650 MHz;4.9 GHz;5 GHz;5.9 GHz;60 GHz;advanced encryption standard;AES;audio;beamforming;carrier sense multiple access/collision avoidance;CCMP;channel switching;clustering;contention based access period;Counter mode with Cipher-block chaining Message authentication code Protocol;confidentiality;CSMA/CA;DFS;direct link;directional multi-gigabit;dynamic allocation of service period;dynamic extension of service period;dynamic frequency selection;dynamic truncation of service period;E911;EDCA;emergency alert system;emergency services;fast session transfer;forwarding;GCMP;generic advertisement service;high throughput;IEEE 802.11;international roaming;interworking;interworking with external networks;LAN;local area network;MAC;management;measurement;medium access control;media-independent handover;medium access controller;mesh;MIS;millimeter-wave;MIMO;MIMO-OFDM;multi-band operation;multi-hop;multi-user MIMO;multiple input multiple output;network advertisement;network discovery;network management;network selection;noncontiguous frequency segments;OCB;path-selection;personal basic service set;PHY;physical layer;power saving;QoS;quality of service;quality-of-service management frame;radio;radio frequency;RF;radio resource;radio management;relay operation;spatial sharing;SSPN;subscriber service provider;television white spaces;TPC;transmit power control;video;wireless access in vehicular environments;wireless LAN;wireless local area network;WLAN;wireless network management;zero-knowledge proof},
  doi={10.1109/IEEESTD.2021.9363693},
  ISSN={},
  month={Feb},}@ARTICLE{4479458,
  author={Kang, Hyunmo and Getoor, Lise and Shneiderman, Ben and Bilgic, Mustafa and Licamele, Louis},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  title={Interactive Entity Resolution in Relational Data: A Visual Analytic Tool and Its Evaluation},
  year={2008},
  volume={14},
  number={5},
  pages={999-1014},
  abstract={Databases often contain uncertain and imprecise references to real-world entities. Entity resolution, the process of reconciling multiple references to underlying real-world entities, is an important data cleaning process required before accurate visualization or analysis of the data is possible. In many cases, in addition to noisy data describing entities, there is data describing the relationships among the entities. This relational data is important during the entity resolution process; it is useful both for the algorithms which determine likely database references to be resolved and for visual analytic tools which support the entity resolution process. In this paper, we introduce a novel user interface, D-Dupe, for interactive entity resolution in relational data. D-Dupe effectively combines relational entity resolution algorithms with a novel network visualization that enables users to make use of an entity's relational context for making resolution decisions. Since resolution decisions often are interdependent, D-Dupe facilitates understanding this complex process through animations which highlight combined inferences and a history mechanism which allows users to inspect chains of resolution decisions. An empirical study with 12 users confirmed the benefits of the relational context visualization on the performance of entity resolution tasks in relational data in terms of time as well as users' confidence and satisfaction.},
  keywords={Visual analytics;Data visualization;Relational databases;Visual databases;Cleaning;Data analysis;User interfaces;Inference algorithms;Animation;History;User interfaces;Human-centered computing;Graphical user interfaces;User-centered design;Information visualization;User interfaces;Human-centered computing;Graphical user interfaces;User-centered design;Information visualization},
  doi={10.1109/TVCG.2008.55},
  ISSN={1941-0506},
  month={Sep.},}@ARTICLE{8360794,
  author={},
  journal={ISO/IEC/IEEE 8802-11:2018(E)},
  title={ISO/IEC/IEEE - International Standard - Information technology--Telecommunications and information exchange between systems--Local and metropolitan area networks--Specific requirements--Part 11: Wireless LAN medium access control (MAC) and physical layer (PHY) specifications},
  year={2018},
  volume={},
  number={},
  pages={1-3538},
  abstract={Technical corrections and clarifications to IEEE Std 802.11 for wireless local area networks (WLANs) as well as enhancements to the existing medium access control (MAC) and physical layer (PHY) functions are specified in this revision. Amendments 1 to 5 published in 2012 and 2013 have also been incorporated into this revision.},
  keywords={IEEE Standards;ISO Standards;IEC Standards;Information technologyu;Encryption;Wireless LAN;Media Access Protocol;Physical layer;Quality of service;2.4 GHz;256-QAM;3650 MHz;4.9 GHz;5 GHz;5.9 GHz;60 GHz;advanced encryption standard;AES;audio;beamforming;carrier sense multiple access/collision avoidance;CCMP;channel switching;clustering;contention based access period;Counter mode with Cipherblock chaining Message authentication code Protocol;confidentiality;CSMA/CA;DFS;direct link;directional multi-gigabit;dynamic allocation of service period;dynamic extension of service period;dynamic frequency selection;dynamic truncation of service period;E911;EDCA;emergency alert system;emergency services;fast session transfer;forwarding;GCMP;generic advertisement service;high throughput;IEEE 802.11™;international roaming;interworking;interworking with external networks;LAN;local area network;MAC;management;measurement;medium access control;media-independent handover;medium access controller;mesh;MIH;millimeter-wave;MIMO;MIMO-OFDM;multi-band operation;multi-hop;multi-user MIMO;multiple input multiple output;network advertisement;network discovery;network management;network selection;noncontiguous frequency segments;OCB;path-selection;personal basic service set;PHY;physical layer;power saving;QoS;quality of service;quality-of-service management frame;radio;radio frequency;RF;radio resource;radio management;relay operation;spatial sharing;SSPN;subscriber service provider;television white spaces;TPC;transmit power control;video;wireless access in vehicular environments;wireless LAN;wireless local area network;WLAN;wireless network management;zero-knowledge proof},
  doi={10.1109/IEEESTD.2018.8360794},
  ISSN={},
  month={May},}@ARTICLE{7383204,
  author={},
  journal={IEEE P802.11-REVmc/D5.0, January 2016 (Revision of IEEE Std 802.11-2012 as amended by IEEE Std 802.11ae-2012, IEEE Std 802.11aa-2012, IEEE Std 802.11ad-2012, IEEE Std 802.11ac-2013, and IEEE Std 802.11af-2013)},
  title={IEEE Draft Standard for Information technology--Telecommunications and information exchange between systems Local and metropolitan area networks--Specific requirements Part 11: Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specifications},
  year={2016},
  volume={},
  number={},
  pages={1-3766},
  abstract={This revision specifies technical corrections and clarifications to IEEE Std 802.11 for wireless local area networks (WLANS) as well as enhancements to the existing medium access control (MAC) and physical layer (PHY) functions. TBD: It also incorporates Amendments 1 and 2 published in 2012.},
  keywords={IEEE Standards;Information exchange;Encryption;Array signal processing;Collision avoidance;Transmission intervals;2.4 GHz;3650 MHz;4.9 GHz;5 GHz;5.9 GHz;60 GHz;A-BFT;advanced encryption standard;AES;announcement transmission interval;association beamforming training time;ATI;audio;beacon transmission interval;beamforming;BTI;carrier sense multiple access/collision avoidance;CBAP;CCMP;channel switching;clustering;contention-based access period;Countermode with Cipher-block chaining Message authentication code Protocol;confidentiality;CSMA/CA;DFS;direct link;directional multi-gigabit;DMG;dynamic allocation of service period;MIH;millimeter-wave;MIMO;MIMO-OFDM;multi-band operation;multi-hop;temporal key integrity protocol;TKIP;TPC},
  doi={},
  ISSN={},
  month={Jan},}@ARTICLE{7489061,
  author={},
  journal={IEEE P802.11-REVmc/D6.0, June 2016},
  title={IEEE Draft Standard for Information technology--Telecommunications and information exchange between systems - Local and metropolitan area networks--Specific requirements Part 11: Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specifications},
  year={2016},
  volume={},
  number={},
  pages={1-3774},
  abstract={This revision specifies technical corrections and clarifications to IEEE Std 802.11 for wireless local area networks (WLANS) as well as enhancements to the existing medium access control (MAC) and physical layer (PHY) functions. TBD: It also incorporates Amendments 1 and 2 published in 2012.},
  keywords={IEEE Standards;Encryption;Array signal processing;Ciphers;Collision avoidance;Training;Information exchange;Wireless LAN;Local area networks;Metropolitan area networks;Authentication;Messaging systems;2.4 GHz;3650 MHz;4.9 GHz;5 GHz;5.9 GHz;60 GHz;A-BFT;advanced encryption standard;AES;announcement transmission interval;association beamforming training time;ATI;audio;beacon transmission interval;beamforming;BTI;carrier sense multiple access/collision avoidance;CBAP;CCMP;channel switching;clustering;contention-based access period;Countermode with Cipher-block chaining Message authentication code Protocol;confidentiality;CSMA/CA;DFS;direct link;directional multi-gigabit;DMG;dynamic allocation of service period;MIH;millimeter-wave;MIMO;MIMO-OFDM;multi-band operation;multi-hop;temporal key integrity protocol;TKIP;TPC},
  doi={},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10488171,
  author={Vallejo-Huanga, Diego and Morocho, Jair and Salgado, Juan},
  booktitle={2023 15th International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter)},
  title={SimilaCode: Programming Source Code Similarity Detection System Based on NLP},
  year={2023},
  volume={},
  number={},
  pages={171-178},
  abstract={Some tools have been developed in the scientific field to detect similarities in texts; however, some software is not very efficient in detecting plagiarism in programming source codes. In computing, it is expected to find cases of plagiarism in the source code, and there are currently tools that measure the degree of similarity, but they require paid licenses. This scientific article proposes constructing a system that uses Natural Language Processing (NLP), vector space models, and similarity metrics to identify the degree of divergence between pairs of source codes in the Python programming language, with the possibility of extrapolating its applicability to other programming languages. The proposed system is structured in several modules, each with a specific function for both the back-end and front-end of the prototype deployed on the web. The experimentation was carried out using pairs of source codes subjected to modifications at a linguistic and structural level. The results show that our system, Similacode, can detect 100% similarities between source code pairs that have changed their comments. It was observed that the system could identify similarities, even when modifications have been made to the names of variables and functions, reaching levels of similarity higher than 88%. In addition, comparisons were made with two other plagiarism detection tools to assess the degree of similarity, obtaining results with less than 1% differences between the different software. The experiments in Similacode have yielded satisfactory results, demonstrating the system's efficiency in detecting similarities in the analyzed source codes.},
  keywords={Source coding;Plagiarism;Prototypes;Programming;Linguistics;Licenses;Natural language processing;Code Plagiarism;Programming Languages;Python;Code Clone;Vector Cosine Model},
  doi={10.1109/IIAI-AAI-Winter61682.2023.00040},
  ISSN={},
  month={Dec},}@BOOK{8187424,
  author={Dwork, Cynthia and Roth, Aaron},
  title={The Algorithmic Foundations of Differential Privacy},
  year={2014},
  volume={},
  number={},
  pages={},
  abstract={The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition. The Algorithmic Foundations of Differential Privacy starts out by motivating and discussing the meaning of differential privacy, and proceeds to explore the fundamental techniques for achieving differential privacy, and the application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some powerful computational results, there are still fundamental limitations. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power — certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed. The monograph then turns from fundamentals to applications other than query-release, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams, is discussed. The Algorithmic Foundations of Differential Privacy is meant as a thorough introduction to the problems and techniques of differential privacy, and is an invaluable reference for anyone with an interest in the topic.},
  keywords={Algorithmic game theory;Cryptography and information security;Database theory;Design and analysis of algorithms},
  doi={10.1561/0400000042},
  ISSN={},
  publisher={now},
  isbn={9781601988195},
  url={https://ieeexplore.ieee.org/document/8187424},}@ARTICLE{9005214,
  author={Basumatary, Hirakjyoti and Hazarika, Shyamanta M.},
  journal={IEEE Transactions on Human-Machine Systems},
  title={State of the Art in Bionic Hands},
  year={2020},
  volume={50},
  number={2},
  pages={116-130},
  abstract={Prosthetic hands have made a significant influence on the quality of life of people with upper arm amputation. Research on prosthetic hands today is focused on replicating the functionalities of the biological hands. The present article provides a bibliometric survey on bionic hands, done through a compilation of a scientific publications database on the field of prosthetic hands spanning the last two decades. Through network-based information analysis, meaningful patterns are inferred, and several key questions significant to bionic prosthetic hands are answered. The article gives an insight into the growth, progress, and future trend in bionic hand prostheses, including identifying subject areas and discovering distinct research communities within the subject. The analysis clearly reveals the rapid expansion of bionic hand research over the last two decades, which is expected to rise considerably in near future.},
  keywords={Prosthetic hand;Feature extraction;Muscles;Surgery;Bones;Visualization;Bibliometric survey;bionic hand;hand prosthesis;information analysis;prosthetic hand},
  doi={10.1109/THMS.2020.2970740},
  ISSN={2168-2305},
  month={April},}@ARTICLE{9480518,
  author={Malanchev, K L and Pruzhinskaya, M V and Korolev, V S and Aleo, P D and Kornilov, M V and Ishida, E E O and Krushinsky, V V and Mondon, F and Sreejith, S and Volnova, A A and Belinski, A A and Dodin, A V and Tatarnikov, A M and Zheltoukhov, S G and The SNAD Team},
  journal={Monthly Notices of the Royal Astronomical Society},
  title={Anomaly detection in the Zwicky Transient Facility DR3},
  year={2021},
  volume={502},
  number={4},
  pages={5147-5175},
  abstract={We present results from applying the SNAD anomaly detection pipeline to the third public data release of the Zwicky Transient Facility (ZTF DR3). The pipeline is composed of three stages: feature extraction, search of outliers with machine learning algorithms, and anomaly identification with followup by human experts. Our analysis concentrates in three ZTF fields, comprising more than 2.25 million objects. A set of four automatic learning algorithms was used to identify 277 outliers, which were subsequently scrutinized by an expert. From these, 188 (68 per cent) were found to be bogus light curves – including effects from the image subtraction pipeline as well as overlapping between a star and a known asteroid, 66 (24 per cent) were previously reported sources whereas 23 (8 per cent) correspond to non-catalogued objects, with the two latter cases of potential scientific interest (e.g. one spectroscopically confirmed RS Canum Venaticorum star, four supernovae candidates, one red dwarf flare). Moreover, using results from the expert analysis, we were able to identify a simple bi-dimensional relation that can be used to aid filtering potentially bogus light curves in future studies. We provide a complete list of objects with potential scientific application so they can be further scrutinised by the community. These results confirm the importance of combining automatic machine learning algorithms with domain knowledge in the construction of recommendation systems for astronomy. Our code is publicly available.1},
  keywords={methods: data analysis;astronomical data bases: miscellaneous;stars: variables: general},
  doi={10.1093/mnras/stab316},
  ISSN={1365-2966},
  month={Feb},}@ARTICLE{8776665,
  author={},
  journal={IEEE P2410-D4, July 2019},
  title={IEEE Draft Standard Format for LSI-Package-Board Interoperable Design},
  year={2019},
  volume={},
  number={},
  pages={1-28},
  abstract={A method is provided for specifying a common interoperable format for electronic systems design. The format provides a common way to specify information/data about the project management, netlists, components, design rules, and geometries used in the large-scale integration-package-board designs. The method provides the ability to make electronic systems a key consideration early in the design process; design tools can use it to seamlessly exchange information/data.},
  keywords={IEEE Standards;Servers;Access control;Relational databases;Cluster approximation;Open systems;common interoperable format;components;design analysis;design rules;geometries;IEEE 2401;large-scale integration (LSI);netlists;packages for LSI circuits;printed circuit board;project management;Verilog-HDL},
  doi={},
  ISSN={},
  month={July},}@BOOK{9647706,
  author={Portwood-Stacer, Laura},
  title={The Book Proposal Book: A Guide for Scholarly Authors},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={A step-by-step guide to crafting a compelling scholarly book proposal—and seeing your book through to successful publicationThe scholarly book proposal may be academia’s most mysterious genre. You have to write one to get published, but most scholars receive no training on how to do so—and you may have never even seen a proposal before you’re expected to produce your own. The Book Proposal Book cuts through the mystery and guides prospective authors step by step through the process of crafting a compelling proposal and pitching it to university presses and other academic publishers.Laura Portwood-Stacer, an experienced developmental editor and publishing consultant for academic authors, shows how to select the right presses to target, identify audiences and competing titles, and write a project description that will grab the attention of editors—breaking the entire process into discrete, manageable tasks. The book features over fifty time-tested tips to make your proposal stand out; sample prospectuses, a letter of inquiry, and a response to reader reports from real authors; optional worksheets and checklists; answers to dozens of the most common questions about the scholarly publishing process; and much, much more.Whether you’re hoping to publish your first book or you’re a seasoned author with an unfinished proposal languishing on your hard drive, The Book Proposal Book provides honest, empathetic, and invaluable advice on how to overcome common sticking points and get your book published. It also shows why, far from being merely a hurdle to clear, a well-conceived proposal can help lead to an outstanding book.},
  keywords={Publishing;Publication;Author;Writing;Academic publishing;Paragraph;Copy editing;Princeton University Press;Suggestion;Guideline;Manuscript;Target audience;Table of contents;Narrative;Marketing;Credential;Handbook;Career;Bibliography;Website;Word count;Finding;Writing style;Understanding;Email;Publicist;Editorial board;Literature review;Article (publishing);Proofreading;Editorial;Case study;Literary agent;Peer review;Copyright;Gaze;Quality assurance;Illustration;First Book;Editing;Rhetorical question;Phenomenon;Passive voice;Writing process;Wildlife conservation;Funding;Academic journal;Citizen science;Technology;Book;Recommendation (European Union);Edition (book);Designer;Developmental editing;Writer;Rhetoric;Environmental protection;Search engine optimization;Ecosystem;Institution;Monograph;Calculation;Marketing plan;Price point;Academic writing;Social media;Publicity;Consideration;Documents (magazine);Sociology;Brand management;Femininity;Librarian;Style guide;Symbolic capital;On Royalty;Neoliberalism;Qualitative research;Verb;Infrastructure;Thesis statement;Routledge;Criticism;Decision-making;Book design;Textbook;Creative director;Methodology;Sensibility;Brand culture;University of Illinois Press;Sentence (linguistics);Newspaper;Hardcover;Description;Public talks;Empowerment;Sexism;Globalization;One Laptop per Child},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691216621},
  url={https://ieeexplore.ieee.org/document/9647706},}@ARTICLE{7094263,
  author={Alcalá-Fdez, Jesús and Alonso, José M.},
  journal={IEEE Transactions on Fuzzy Systems},
  title={A Survey of Fuzzy Systems Software: Taxonomy, Current Research Trends, and Prospects},
  year={2016},
  volume={24},
  number={1},
  pages={40-56},
  abstract={Fuzzy systems have been used widely thanks to their ability to successfully solve a wide range of problems in different application fields. However, their replication and application require a high level of knowledge and experience. Furthermore, few researchers publish the software and/or source code associated with their proposals, which is a major obstacle to scientific progress in other disciplines and in industry. In recent years, most fuzzy system software has been developed in order to facilitate the use of fuzzy systems. Some software is commercially distributed, but most software is available as free and open-source software, reducing such obstacles and providing many advantages: quicker detection of errors, innovative applications, faster adoption of fuzzy systems, etc. In this paper, we present an overview of freely available and open-source fuzzy systems software in order to provide a well-established framework that helps researchers to find existing proposals easily and to develop well-founded future work. To accomplish this, we propose a two-level taxonomy, and we describe the main contributions related to each field. Moreover, we provide a snapshot of the status of the publications in this field according to the ISI Web of Knowledge. Finally, some considerations regarding recent trends and potential research directions are presented.},
  keywords={Fuzzy systems;Frequency selective surfaces;Software;Libraries;Proposals;Taxonomy;Communities;Fuzzy logic;fuzzy systems;fuzzy systems software;software for applications;software engineering;educational software;open source software;Educational software;fuzzy logic;fuzzy systems;fuzzy systems software (FSS);open-source software;software engineering;software for applications},
  doi={10.1109/TFUZZ.2015.2426212},
  ISSN={1941-0034},
  month={Feb},}@BOOK{9100760,
  author={Popescu, Mihail and Xu, Dong},
  title={Data Mining Applications Using Ontologies in Biomedicine},
  year={2009},
  volume={},
  number={},
  pages={},
  abstract={Presently, a growing number of ontologies are being built and used for annotating data in biomedical research. Thanks to the tremendous amount of data being generated, ontologies are now being used in numerous ways, including connecting different databases, refining search capabilities, interpreting experimental/clinical data, and inferring knowledge. This cutting-edge resource introduces you to latest developments in bio-ontologies. The book provides you with the theoretical foundations and examples of ontologies, as well as applications of ontologies in biomedicine, from molecular levels to clinical levels. You also find details on technological infrastructure for bio-ontologies. This comprehensive, one-stop volume presents a wide range of practical bio-ontology information, offering you detailed guidance in the clustering of biological data, protein classification, gene and pathway prediction, and text mining. More than 160 illustrations support key topics throughout the book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781596933712},
  url={https://ieeexplore.ieee.org/document/9100760},}@ARTICLE{10177771,
  author={Martínez-Gárate, Ángel Antonio and Aguilar-Calderón, José Alfonso and Tripp-Barba, Carolina and Zaldívar-Colado, Aníbal},
  journal={IEEE Access},
  title={Model-Driven Approaches for Conversational Agents Development: A Systematic Mapping Study},
  year={2023},
  volume={11},
  number={},
  pages={73088-73103},
  abstract={Conversational agents are a piece of software widely used nowadays in several domains, such as e-commerce, customer support, education, among others. To build one, proficiency in distinct areas of knowledge is necessary. Moreover, its development demands the availability of frameworks, tools, and proposals to facilitate its application in several domain areas. Recently, the adoption of models in a software development process has increased through the application of Model-Driven Development in several areas to improve and increase productivity in software development. This work aims to provide state of the art through a Systematic Mapping Study regarding to Model-Driven Development approaches to automate or semi-automate the development of Chatbots. For this study, 429 scientific articles were analyzed, of which, after the inclusion and exclusion criteria, only 20 were considered primary studies.},
  keywords={Software;Object oriented modeling;Computational modeling;Chatbots;Natural languages;Model-driven development;DSL;Chatbot;model-driven engineering;model-driven development;systematic mapping study;review;conversational agent;bot;development frameworks;chatbot development methods;MDD;MDD chatbot development},
  doi={10.1109/ACCESS.2023.3293849},
  ISSN={2169-3536},
  month={},}@BOOK{9969980,
  author={Maor, Eli and Jost, Eugen},
  title={Pentagons and Pentagrams: An Illustrated History},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={A fascinating exploration of the pentagon and its role in various culturesThe pentagon and its close cousin, the pentagram, have inspired individuals for the last two and half millennia, from mathematicians and philosophers to artists and naturalists. Despite the pentagon’s wide-ranging history, no single book has explored the important role of this shape in various cultures, until now. Richly illustrated, Pentagons and Pentagrams offers a sweeping view of the five-sided polygon, revealing its intriguing geometric properties and its essential influence on a variety of fields.Traversing time, Eli Maor narrates vivid stories, both celebrated and unknown, about the pentagon and pentagram. He discusses the early Pythagoreans, who ascribed to the pentagon mythical attributes, adopted it as their emblem, and figured out its construction with a straightedge and compass. Maor looks at how a San Diego housewife uncovered four previously unknown types of pentagonal tilings, and how in 1982 a scientist’s discovery of fivefold symmetries in certain alloys caused an uproar in crystallography and led to a Nobel Prize. Maor also discusses the pentagon’s impact on many buildings, from medieval fortresses to the Pentagon in Washington, D.C. Eugen Jost’s superb illustrations provide sumptuous visual context, and the book’s puzzles and mazes offer fun challenges for readers, with solutions given in an appendix.},
  keywords={Golden ratio;Tessellation;Irrational number;Supermassive black hole;Dodecahedron;Geology of the Moon;Geometry;Photon;Diffraction;Duodecimal;Mathematician;Meteorite;Rare earth element;Black hole;Gravity wave;Rectangle;Spacecraft;Stellar classification;Quantity;Wavelength;Regular polygon;X-ray;Radio wave;Pentagonal tiling;Neutrino;Pentagon;Detection;Radio telescope;Polyhedron;Deuterium;Quasar;Baltimore World Trade Center;Architectural firm;Rotational symmetry;Line segment;Notation;Electron rest mass;Calculation;Approximation;Star of David;Infrastructure;Light-year;Cosmic ray;Jeff Bezos;Ray (optics);Solar mass;Tourism;Planetary system;Australian Astronomical Observatory;Chapter 9;Temperature;Gravitational wave;Emblem;Potomac River;Equation;Star formation;Bending;Fractal;Cosmic microwave background;Kip Thorne;Jaca;Length;Accelerating expansion of the universe;Defensive wall;Compact star;Metal;Muon;Accretion disk;International Space Station;Industry;Lunar south pole;Gravitational field;Black-body radiation;Statistic;Logarithmic spiral;To the Moon;Dwarf galaxy;Tetrahedron;Fort McHenry;Integer;Search algorithm;Prediction;Minute and second of arc;Supply (economics);Laser;Alloy;Astronaut;Fusion power;HTTPS;Suleiman the Magnificent;Equilateral triangle;Baryon;Uranus;Anti-gravity;American Airlines Flight 77;Solar neutrino;Millisecond;Fine art;Transcendental number;Centroid},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691238555},
  url={https://ieeexplore.ieee.org/document/9969980},}@BOOK{9969978,
  author={Silk, Joseph},
  title={Back to the Moon: The Next Giant Leap for Humankind},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={A scientist’s inspiring vision of our return to the Moon as humanity’s next thrilling step in space explorationJust over half a century since Neil Armstrong first stepped foot on the lunar surface, a new space race to the Moon is well underway and rapidly gaining momentum. Laying out a vision for the next fifty years, Back to the Moon is astrophysicist Joseph Silk’s persuasive and impassioned case for putting scientific discovery at the forefront of lunar exploration.The Moon offers opportunities beyond our wildest imaginings, and plans to return are rapidly gaining momentum around the world. NASA aims to build a habitable orbiting space station to coordinate lunar development and exploration, while European and Chinese space agencies are planning lunar villages and the mining of precious resources dwindling here on Earth. Powerful international and commercial interests are driving the race to revisit the Moon, but lunar infrastructures could also open breathtaking vistas onto the cosmos. Silk describes how the colonization of the Moon could usher in a thrilling new age of scientific exploration, and lays out what the next fifty years of lunar science might look like. With lunar telescopes of unprecedented size situated in permanently dark polar craters and on the far side of the Moon, we could finally be poised to answer some of the most profound questions confronting humankind, including whether we are alone in the Universe and what our cosmic origins are.Addressing both the daunting challenges and the immense promise of lunar exploration and exploitation, Back to the Moon reveals how prioritizing science, and in particular lunar astronomy, will enable us to address the deepest cosmic mysteries.},
  keywords={Geology of the Moon;Spacecraft;Exploration of the Moon;Exoplanet;Colonization of the Moon;Cosmic microwave background;Lunar soil;Gravity wave;Far side of the Moon;Astronomer;Space telescope;Origin of the Moon;Astronaut;Space exploration;Meteorite;Space station;Milky Way;Radio wave;International Space Station;Spaceport;Jupiter;Radio telescope;Impact event;Extraterrestrial life;Chronology of the universe;Apollo program;Lunar orbit;Astronomy;Lunar lander;To the Moon;SpaceX;Lunar rover;Atmosphere of Earth;Saturn V;Stellar classification;We choose to go to the Moon;Moon rock;Lunar water;Moon;Sub-orbital spaceflight;Space Shuttle;Cosmic ray;Spaceflight;Lunar outpost (NASA);Neutron star;Moons of Saturn;Icy moon;Apollo (spacecraft);Space colonization;Outer Space Treaty;Solar mass;Lunar south pole;Around the Moon;Space tourism;Orbital spaceflight;Supermassive black hole;Heliocentric orbit;Apollo 15;Uranus;Inflation (cosmology);Starship;Planetary system;Dwarf galaxy;Low Earth orbit;Space probe;Asteroid;Helium-3;Universe;Space-based solar power;Soviet space program;Cosmic background radiation;Pluto;Asteroid mining;Skylab;Accretion (astrophysics);NASA Earth Observatory;Terrestrial planet;Private spaceflight;Space Shuttle program;Formation and evolution of the Solar System;Mercury (planet);Comet;Lava tube;Interstellar communication;Orbital elements;Orbit;Geostationary orbit;Asteroid belt;Wavelength;Planck (spacecraft);NASA;Detection;Apollo Command/Service Module;Cosmogony;Proxima Centauri;Star formation;Elliptical galaxy;Fusion power;Galaxy cluster;Indian Space Research Organisation},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691242880},
  url={https://ieeexplore.ieee.org/document/9969978},}@INPROCEEDINGS{9476248,
  author={Ferreira, Leonardo and de Oliveira, Felipe Schiavon and da Rocha, Ovídio Pires and Holanda, Maristela and de Carvalho Victorino, Marcio and Ribeiro, Edward},
  booktitle={2021 16th Iberian Conference on Information Systems and Technologies (CISTI)},
  title={MongoDB: Analysis of Performance with Data from the National High School Exam (Enem)},
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={It is increasingly common for public and private organizations to deal with the challenges of data management in Big Data environments, tipycally characterized by high volume, great variety, fast generation velocity, in addition to intrinsic issues of data veracity and value. This context gave rise to NoSQL database technologies, with MongoDB as one of its prominent representatives. This paper aims to evaluate the performance of MongoDB in its cluster architecture against the standalone mode, with massive data from the National High School Exam (Enem), in addition to performing a brief interpretation of the results from the perspective of the educational domain. The initial results of query, update, and delete operations demonstrate a set of advantages and disadvantages of the experiments' architectures. The Information Technology (IT) professional's role in defining the scope of the database project for a more bold choice of the NoSQL architecture is highlighted in a cost-benefit perspective for organizations.},
  keywords={Databases;Scalability;NoSQL databases;Organizations;Big Data;Data models;Indexes;Big Data;NoSQL;MongoDB;MongoDB Atlas;Performance;Standalone;Cluster;DbaaS;AWS;Enem},
  doi={10.23919/CISTI52073.2021.9476248},
  ISSN={2166-0727},
  month={June},}@ARTICLE{9099866,
  author={Rafique, Wajid and Qi, Lianyong and Yaqoob, Ibrar and Imran, Muhammad and Rasool, Raihan Ur and Dou, Wanchun},
  journal={IEEE Communications Surveys & Tutorials},
  title={Complementing IoT Services Through Software Defined Networking and Edge Computing: A Comprehensive Survey},
  year={2020},
  volume={22},
  number={3},
  pages={1761-1804},
  abstract={Millions of sensors continuously produce and transmit data to control real-world infrastructures using complex networks in the Internet of Things (IoT). However, IoT devices are limited in computational power, including storage, processing, and communication resources, to effectively perform compute-intensive tasks locally. Edge computing resolves the resource limitation problems by bringing computation closer to the edge of IoT devices. Providing distributed edge nodes across the network reduces the stress of centralized computation and overcomes latency challenges in the IoT. Therefore, edge computing presents low-cost solutions for compute-intensive tasks. Software-Defined Networking (SDN) enables effective network management by presenting a global perspective of the network. While SDN was not explicitly developed for IoT challenges, it can, however, provide impetus to solve the complexity issues and help in efficient IoT service orchestration. The current IoT paradigm of massive data generation, complex infrastructures, security vulnerabilities, and requirements from the newly developed technologies make IoT realization a challenging issue. In this research, we provide an extensive survey on SDN and the edge computing ecosystem to solve the challenge of complex IoT management. We present the latest research on Software-Defined Internet of Things orchestration using Edge (SDIoT-Edge) and highlight key requirements and standardization efforts in integrating these diverse architectures. An extensive discussion on different case studies using SDIoT-Edge computing is presented to envision the underlying concept. Furthermore, we classify state-of-the-art research in the SDIoT-Edge ecosystem based on multiple performance parameters. We comprehensively present security and privacy vulnerabilities in the SDIoT-Edge computing and provide detailed taxonomies of multiple attack possibilities in this paradigm. We highlight the lessons learned based on our findings at the end of each section. Finally, we discuss critical insights toward current research issues, challenges, and further research directions to efficiently provide IoT services in the SDIoT-Edge paradigm.},
  keywords={Edge computing;Internet of Things;Security;Computer architecture;Virtualization;Software;Ecosystems;Edge computing;Internet of Things;software-defined networking;software-defined IoT;network virtualization;IoT service orchestration},
  doi={10.1109/COMST.2020.2997475},
  ISSN={1553-877X},
  month={thirdquarter},}

  @ARTICLE{ClusterPub,
  title={ClusterPub: A system to cluster scientific papers},
  keywords={Bibliographic Search;Bibliographic File;Command-Line Interface;Machine Learning;Clustering},
  abstract={Bibliographic research processes are extremely common in academic life since, for the preparation of some works, such as course completion projects, master's dissertations, and doctoral theses, a fundamental part of the elaboration process is the search for bibliographic reference. However, this process is currently quite laborious because when performing a search in academic repositories, such as IEEE Xplore, Google Scholar, and Pubmed, thousands of results are obtained, which makes the researcher need to perform a manual task of organizing, classifying, and filtering the returned articles. This task requires a lot of time and effort. This work proposes the development of a command line application that is capable of processing bibliographic files, resulting in the generation of dendrograms that reflect the similarities between the works contained in the processed file in order to speed up the process of bibliographic survey. During the preparation of the current work, the proposed tool was developed using the Python programming language, together with the framework for the development of command line applications, Typer, in addition to using several libraries, such as Scikit-Learn and Scipy, to make the grouping algorithm. For the tests, we used a bibliographic file in BibTex format containing 25 articles for each of the following topics: Artificial Intelligence, Biotechnology, Circular Economy, Climate Change, Complex Systems, Genetics, Mental Health, and Neuroscience. The combination of the method of linking the weighted average of distances and the distance metric of cosine similarity can be considered as the one that obtained the best results in general since it resulted in the values of 0.8545, 118.0987, and 0.5394 for the Davies-Bouldin Calinski-Harabasz and silhouette indices, respectively. By analyzing the values listed in the current text in relation to the results obtained by other studies also related to textual clustering models, it can be concluded that the results measured by the current work are satisfactory since numerically better values are sometimes observed. However, it is not plausible to use these comparisons to affirm that the ClusterPub tool is superior to the works used in the comparisons cited since this work did not perform tests with the databases used by the other tools. In this work, the reference articles were used to obtain reference values for the metrics analyzed. The developed tool was made available for installation in the public repository of Python packages PyPi.},
}
